<!DOCTYPE html> <html lang="pt-br"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="g-X-aUUud1gtIVUgKEAx9T7VKpcQw1Ymd8aQ-hlIIY8"> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Analizando o histórico do CVPR | George C. de Araújo </title> <meta name="author" content="George C. de Araújo"> <meta name="description" content="Uma análise do histórico do CVPR e seus workshops de 2017 a 2024."> <meta name="keywords" content="machine learning, artificial intelligence, deep learning, computer vision, natural language processing"> <meta property="og:site_name" content="George C. de Araújo"> <meta property="og:type" content="article"> <meta property="og:title" content="George C. de Araújo | Analizando o histórico do CVPR"> <meta property="og:url" content="https://george-gca.github.io/blog/2025/cvpr-history-analysis/"> <meta property="og:description" content="Uma análise do histórico do CVPR e seus workshops de 2017 a 2024."> <meta property="og:image" content="assets/img/profile_pic.png"> <meta property="og:image:alt" content="Foto de perfil"> <meta property="og:locale" content="pt-br"> <meta name="twitter:card" content="summary"> <meta name="twitter:title" content="Analizando o histórico do CVPR"> <meta name="twitter:description" content="Uma análise do histórico do CVPR e seus workshops de 2017 a 2024."> <meta name="twitter:image" content="assets/img/profile_pic.png"> <script type="application/ld+json">
    {"author":{"@type":"Person","name":"George C. de Araújo"},"url":"https://george-gca.github.io/blog/2025/cvpr-history-analysis/","@type":"BlogPosting","description":"Uma análise do histórico do CVPR e seus workshops de 2017 a 2024.","headline":"Analizando o histórico do CVPR","name":"George C. de Araújo","@context":"https://schema.org"}
  </script> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%91%A8%F0%9F%8F%BD%E2%80%8D%F0%9F%92%BB&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://george-gca.github.io/pt-br/blog/2025/cvpr-history-analysis/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/pt-br/"> <span class="font-weight-bold">George</span> C. de Araújo </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap" style="--stagger: 1;" data-animate=""> <li class="nav-item "> <a class="nav-link" href="/pt-br/">Bio </a> </li> <li class="nav-item active"> <a class="nav-link" href="/pt-br/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/pt-br/publications/">Publicações </a> </li> <li class="nav-item "> <a class="nav-link" href="/pt-br/repositories/">Repositórios </a> </li> <li class="nav-item "> <a class="nav-link" href="/pt-br/cv/">CV </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/2025/cvpr-history-analysis/"> EN-US</a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link"><i class="ti ti-search"></i> ctrl k</span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main" style="--stagger: 0;" data-animate> <div class="post"> <header class="post-header"> <h1 class="post-title">Analizando o histórico do CVPR</h1> <p class="post-meta" style="--stagger: 2;" data-animate=""> Criado em 25 de Abril, 2025 </p> <p class="post-tags" style="--stagger: 3;" data-animate=""> <a href="/pt-br/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/pt-br/blog/tag/ai"> <i class="fa-solid fa-hashtag fa-sm"></i> ai</a>   <a href="/pt-br/blog/tag/conference"> <i class="fa-solid fa-hashtag fa-sm"></i> conference</a>   <a href="/pt-br/blog/tag/analysis"> <i class="fa-solid fa-hashtag fa-sm"></i> analysis</a>   ·   <a href="/pt-br/blog/category/data-science"> <i class="fa-solid fa-tag fa-sm"></i> data-science</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p style="--stagger: 4;" data-animate="">Enquanto a <a href="https://cvpr.thecvf.com/" rel="external nofollow noopener" target="_blank">IEEE/CVF Conference on Computer Vision and Pattern Recognition</a> (CVPR) 2025 se aproxima, vamos dar uma olhada no histórico da conferência e de seus workshops de 2017 a 2024. O objetivo desta análise é proporcionar uma compreensão sobre a evolução dos tópicos e tendências na pesquisa de inteligência artificial ao longo dos anos. Tenha em mente que essas informações devem ser analisadas com cautela, pois alguns dados que poderiam ser relevantes para as análises são descartados durante o processo de limpeza. Parte da análise baseia-se em palavras-chave, e fazemos algumas suposições sobre como os autores as utilizam (por exemplo, é bastante improvável que um artigo sobre dados de imagem tenha a palavra-chave <code class="language-plaintext highlighter-rouge">audio</code> em seu título ou resumo), mas essa não é uma solução perfeita. O objetivo desta postagem é fornecer uma visão sobre a história da conferência, e não uma análise definitiva.</p> <p style="--stagger: 5;" data-animate="">Observe que alguns dos gráficos utilizam percentis do número total de artigos publicados a cada ano. Como há quantidades diferentes de artigos publicados a cada ano, os números não podem ser comparados diretamente de um ano para o outro. O objetivo desses gráficos é mostrar a distribuição dos artigos publicados durante o período e quaisquer mudanças no foco da comunidade acadêmica. Você também pode interagir com as visualizações. É possível dar zoom em partes específicas, habilitar ou desabilitar linhas clicando em seus nomes na legenda, e passar o mouse sobre os pontos para ver mais informações.</p> <h2 id="estatísticas-gerais" style="--stagger: 6;" data-animate="">Estatísticas Gerais</h2> <p style="--stagger: 7;" data-animate="">Aqui você pode ver o número de artigos publicados. A cada ano, há mais e mais artigos publicados em comparação com o ano anterior, exceto em 2023. Foram publicados mais de três vezes mais artigos em 2024 do que em 2017.</p> <pre><code class="language-plotly">{"data": [{"hovertemplate": "ano=%{x}&lt;br&gt;artigos=%{y}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "", "orientation": "v", "showlegend": false, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "i2", "bdata": "KAQuBVQHwQd+CEgKNgmgDQ=="}, "yaxis": "y", "type": "scatter"}], "layout": {"xaxis": {"anchor": "y", "domain": [0.0, 1.0], "title": {"text": "ano"}}, "yaxis": {"anchor": "x", "domain": [0.0, 1.0], "title": {"text": "artigos"}}, "legend": {"tracegroupgap": 0}}}
</code></pre> <p style="--stagger: 8;" data-animate="">Em relação às modalidades utilizadas nos artigos, podemos ver que a modalidade de imagem continua sendo a mais comum, mas o uso de texto e de múltiplas modalidades aumentou significativamente. A aplicação de fluxo óptico, grafos e informações de profundidade diminuiu nos últimos anos, enquanto o uso de partículas permaneceu relativamente estável.</p> <pre><code class="language-plotly">{"data": [{"customdata": [["audio"], ["audio"], ["audio"], ["audio"], ["audio"], ["audio"], ["audio"], ["audio"]], "hovertemplate": "modalidade=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "audio", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "audio", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "2FBeQ3kN5T8dk/Uck/XsP30vhdy8DuE/g0lGn20u6D/N5tSIhffrP/WdjfrORvU/WASE4EIk+z+P78JB9PgAQA=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["depth"], ["depth"], ["depth"], ["depth"], ["depth"], ["depth"], ["depth"], ["depth"]], "hovertemplate": "modalidade=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "depth", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "depth", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "zB4we8DsE0Ak1egj1egTQER0/3wvhRhAQj1lr7AmFEDHDwNNB98TQMTkCmJyBRFAzGK7c/F4EUDBpv1kCWwPQA=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["graph"], ["graph"], ["graph"], ["graph"], ["graph"], ["graph"], ["graph"], ["graph"]], "hovertemplate": "modalidade=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "graph", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "graph", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "HX1z9M3RC0A+eSo+eSoOQHumE2xSRRFAg0lGn20uGEDucuibFmAQQL+zUd/ZqAtALkSykbMfCkCWwKb9ZAkDQA=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["image"], ["image"], ["image"], ["image"], ["image"], ["image"], ["image"], ["image"]], "hovertemplate": "modalidade=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "image", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "image", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "eQ3lNZQXQkBUyixUyixCQAEGofVGhkBAMA4SdHz7PkB3eggk2Ro8QPeySqCiNz1AeiSI92T9O0B8Fw6ixydAQA=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["mesh"], ["mesh"], ["mesh"], ["mesh"], ["mesh"], ["mesh"], ["mesh"], ["mesh"]], "hovertemplate": "modalidade=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "mesh", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "mesh", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "9oDZA2YP6D/bxovaxovqP0kPVM5u4fc/kOUMnMb8+D9dS0BRmUsBQCZXEJMriPk/XoOZB+cIBUCUJbBpP1kCQA=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["multi modal"], ["multi modal"], ["multi modal"], ["multi modal"], ["multi modal"], ["multi modal"], ["multi modal"], ["multi modal"]], "hovertemplate": "modalidade=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "multi modal", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "multi modal", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "FbFUxFIRC0A1SIM0SIMEQFJF/XDtmQZAw1Unjyo2DEDlDfuqVnANQCdeT8ocAxhA1CNBvCfrF0DtJ0tg034jQA=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["optical flow"], ["optical flow"], ["optical flow"], ["optical flow"], ["optical flow"], ["optical flow"], ["optical flow"], ["optical flow"]], "hovertemplate": "modalidade=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "optical flow", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "optical flow", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "yLgg44KMA0A+eSo+eSr+Pw7LXP52jv8/98VBgo5v/z+EcWIiEo33P1osjwhNtfc/JoMsSX2t8z8Vc6szUjHvPw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["particle"], ["particle"], ["particle"], ["particle"], ["particle"], ["particle"], ["particle"], ["particle"]], "hovertemplate": "modalidade=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "particle", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "particle", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "9oDZA2YPyD8TYk4TYk6zPy5/O8fHSrs/AjGEv/Me0D8j1cmZzanRPylzDHDwc9M/mwIc7fRIwD84iB7fhYPQPw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["path"], ["path"], ["path"], ["path"], ["path"], ["path"], ["path"], ["path"]], "hovertemplate": "modalidade=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "path", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "path", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "NOHPhD8T7j+Y+iGY+iH4P4RTS55mNABAg0lGn20u+D+cmIhE4wX5P1osjwhNtfc/QgNjKDJb9D9GKuZWZ6T0Pw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["point cloud"], ["point cloud"], ["point cloud"], ["point cloud"], ["point cloud"], ["point cloud"], ["point cloud"], ["point cloud"]], "hovertemplate": "modalidade=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "point cloud", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "point cloud", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "FbFUxFIR6z8D77MC77MCQFlpwzKXvwVAw1Unjyo2DED9NCHNJ+kOQCVQ0Vs6DQtAA2MoMlvUEkCpmFudkYoIQA=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["text"], ["text"], ["text"], ["text"], ["text"], ["text"], ["text"], ["text"]], "hovertemplate": "modalidade=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "text", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "text", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "9oDZA2YPGEB3FO12FO0WQD/ZqivwKBlAU4Bd658oFUBUIxbeb5sUQChljgEOfhZAQgNjKDJbFEAJbNpPlsAbQA=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["video"], ["video"], ["video"], ["video"], ["video"], ["video"], ["video"], ["video"]], "hovertemplate": "modalidade=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "video", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "video", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "NpTXUF5DLkCE5g2E5g0sQMnTjAYkxidAjT7bXDDJKEDsq1rBdfQqQL+zUd/ZqCtANeR/ySBLKkDYtJ8sgc0pQA=="}, "yaxis": "y", "type": "scatter"}], "layout": {"legend": {"title": {"text": "modalidade"}, "tracegroupgap": 0}, "xaxis": {"anchor": "y", "domain": [0.0, 1.0], "title": {"text": "ano"}}, "yaxis": {"anchor": "x", "domain": [0.0, 1.0], "title": {"text": "ocorrências (%)"}}}}
</code></pre> <p style="--stagger: 9;" data-animate="">É bastante comum que os artigos introduzam novos conceitos, seja um novo método, um novo conjunto de dados ou uma nova arquitetura. O gráfico a seguir mostra os conceitos mais comuns apresentados nos artigos. Não é surpreendente que algoritmos sejam o conceito mais frequente. Algoritmos também envolvem novos métodos ou abordagens. Novas tarefas foram introduzidas ao longo dos anos, o que está altamente correlacionado com a criação de novos conjuntos de dados. A introdução de novas arquiteturas também aumentou no último ano, incluindo novos modelos, módulos e redes. A criação de diferentes funções de perda e métricas tem se mantido bastante estável ao longo dos anos, com pouquíssimos artigos introduzindo novidades.</p> <pre><code class="language-plotly">{"data": [{"customdata": [["algorithms"], ["algorithms"], ["algorithms"], ["algorithms"], ["algorithms"], ["algorithms"], ["algorithms"], ["algorithms"]], "hovertemplate": "conceito=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "algorithms", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "algorithms", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "qYilIpaKAEA+eSo+eSr+P1x7phNsUgVA3Y20iNzS/T9UIxbeb5v0P1osjwhNtfc/GoUB+zTk/z+P78JB9PgQQA=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["architectures"], ["architectures"], ["architectures"], ["architectures"], ["architectures"], ["architectures"], ["architectures"], ["architectures"]], "hovertemplate": "conceito=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "architectures", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "architectures", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "2FBeQ3kN5T9fX19fX1/vP4RTS55mNPA/0PHti4ME7T+XwbYIZe3wPylzDHDwc/M//gTLG4A27z8H0eO7cBD7Pw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["datasets"], ["datasets"], ["datasets"], ["datasets"], ["datasets"], ["datasets"], ["datasets"], ["datasets"]], "hovertemplate": "conceito=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "datasets", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "datasets", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "5+ibo2+O9j/8rMD7rMD7PyE3r0N0//w/g0lGn20u+D87/O+7niLzP4rXkzLHAP8/44SUPMuI/j9s2k+WwKb/Pw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["losses"], ["losses"], ["losses"], ["losses"], ["losses"], ["losses"], ["losses"], ["losses"]], "hovertemplate": "conceito=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "losses", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "losses", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "9oDZA2YPuD8dk/Uck/XcP30vhdy8DtE/aRG5pbuR1j+1v65mtH7aP76sEqjoLc0/WASE4EIkyz9LYNN+sgTGPw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["metrics"], ["metrics"], ["metrics"], ["metrics"], ["metrics"], ["metrics"], ["metrics"], ["metrics"]], "hovertemplate": "conceito=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "metrics", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "metrics", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "AAAAAAAAAAATYk4TYk6zPy5/O8fHSqs/nYHTmB/LuT/lDfuqVnDNPwAAAAAAAAAAeQPQ5pu2pT+61RmpmFu9Pw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["tasks"], ["tasks"], ["tasks"], ["tasks"], ["tasks"], ["tasks"], ["tasks"], ["tasks"]], "hovertemplate": "conceito=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "tasks", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "tasks", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "9oDZA2YP2D8TYk4TYk7TPxTvIsAgtO4/0PHti4ME7T9sSjwAQRTmP1w6DXcvq/Q/XoOZB+cI9T9GKuZWZ6T0Pw=="}, "yaxis": "y", "type": "scatter"}], "layout": {"xaxis": {"anchor": "y", "domain": [0.0, 1.0], "title": {"text": "ano"}}, "yaxis": {"anchor": "x", "domain": [0.0, 1.0], "title": {"text": "ocorrências (%)"}}, "legend": {"title": {"text": "conceito"}, "tracegroupgap": 0}}}
</code></pre> <p style="--stagger: 10;" data-animate="">Em relação às tarefas comuns nos artigos, podemos observar um grande aumento nas tarefas de geração, especialmente após 2022. Isso pode estar relacionado aos avanços em grandes modelos de linguagem, como <a href="https://openreview.net/forum?id=TG8KACxEON" rel="external nofollow noopener" target="_blank">InstructGPT</a> e <a href="https://openai.com/index/chatgpt/" rel="external nofollow noopener" target="_blank">ChatGPT</a> no final de 2022, e ao lançamento das primeiras coleções de modelos fundamentais de linguagem, como o <a href="https://arxiv.org/abs/2302.13971" rel="external nofollow noopener" target="_blank">LLaMA</a> no início de 2023. Classificação, detecção, estimação e reconhecimento têm apresentado uma queda de interesse ao longo dos anos, enquanto previsão só apresentou diminuição recentemente. Tarefas como segmentação permaneceram relativamente estáveis. O uso de tarefas de raciocínio também aumentou significativamente no último ano, mas ainda corresponde a uma pequena porcentagem do total de artigos publicados (cerca de 3%).</p> <pre><code class="language-plotly">{"data": [{"customdata": [["captioning"], ["captioning"], ["captioning"], ["captioning"], ["captioning"], ["captioning"], ["captioning"], ["captioning"]], "hovertemplate": "tarefa=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "captioning", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "captioning", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "JUmSJEmS/D/yexnyexnyP3YLvxoT6fE/T9krrAn15D9UIxbeb5vkP76sEqjoLe0/mwIc7fRI8D/5LhxEj+/2Pw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["classification"], ["classification"], ["classification"], ["classification"], ["classification"], ["classification"], ["classification"], ["classification"]], "hovertemplate": "tarefa=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "classification", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "classification", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "zB4we8DsI0CmpaWlpaUhQPI0owGJcSJAlEcVG646IUCng+85dnYfQL+zUd/ZqBtAzYNzhLq/F0DMrc5IxVwWQA=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["clustering"], ["clustering"], ["clustering"], ["clustering"], ["clustering"], ["clustering"], ["clustering"], ["clustering"]], "hovertemplate": "tarefa=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "clustering", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "clustering", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "qYilIpaKAED8rMD7rMD7P4RTS55mNABAIrd0N9IiAkDRNy3AMI8AQF1Ii+URoQFA/gTLG4A2/z9VzK3OSMX4Pw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["counting"], ["counting"], ["counting"], ["counting"], ["counting"], ["counting"], ["counting"], ["counting"]], "hovertemplate": "tarefa=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "counting", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "counting", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "2FBeQ3kN5T9WLrhVLrjlP1x7phNsUvU/0PHti4ME7T9sSjwAQRTmP44BDn5u4uU/6QOqY29t2D8Vc6szUjHfPw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["detection"], ["detection"], ["detection"], ["detection"], ["detection"], ["detection"], ["detection"], ["detection"]], "hovertemplate": "tarefa=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "detection", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "detection", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "DDIuyLggKkCIh4eHh4cnQMOvxkR6oChAFWDX+idKKUCJhffbJuUlQNuvLqTF8iZAA2MoMlvUIkB/sgQ27WciQA=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["estimation"], ["estimation"], ["estimation"], ["estimation"], ["estimation"], ["estimation"], ["estimation"], ["estimation"]], "hovertemplate": "tarefa=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "estimation", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "estimation", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "vzn65uibIkA/Uo0+Uo0iQBycWvI0ox1AEnR8++IgIUAfhHFiIhIdQCZXEJMriBlAPIRNAY52GkD7yRLYtJ8XQA=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["forecasting"], ["forecasting"], ["forecasting"], ["forecasting"], ["forecasting"], ["forecasting"], ["forecasting"], ["forecasting"]], "hovertemplate": "tarefa=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "forecasting", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "forecasting", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "9oDZA2YP2D8TYk4TYk7TP0kPVM5u4dc/g0lGn20u6D87/O+7niLjP/BzE68nZe4/0wKJq16k4T8Cm/aTJbDpPw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["generation"], ["generation"], ["generation"], ["generation"], ["generation"], ["generation"], ["generation"], ["generation"]], "hovertemplate": "tarefa=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "generation", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "generation", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "+ubom6NvGECEv3CEv3AgQHumE2xSRSFAzgWTjD7bJEBNhbbHUBcnQPSPD4zsUChAtRpffs7rMECuzkjF3Mo2QA=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["identification"], ["identification"], ["identification"], ["identification"], ["identification"], ["identification"], ["identification"], ["identification"]], "hovertemplate": "tarefa=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "identification", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "identification", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "wOwBswfMAkAMIFsMIFsMQDWjAYlxcApAQj1lr7AmBEA7/O+7niIDQPKBkR0KW/s/WASE4EIk6z9LYNN+sgT2Pw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["navigation"], ["navigation"], ["navigation"], ["navigation"], ["navigation"], ["navigation"], ["navigation"], ["navigation"]], "hovertemplate": "tarefa=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "navigation", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "navigation", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "9oDZA2YP6D93FO12FO32P0kPVM5u4fc/trlgktFn6z/HDwNNB9/zPylzDHDwc/M/7oK/ihNS8j+ix3fhIHr2Pw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["prediction"], ["prediction"], ["prediction"], ["prediction"], ["prediction"], ["prediction"], ["prediction"], ["prediction"]], "hovertemplate": "tarefa=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "prediction", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "prediction", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "tbrT6k6rIUAdbFgdbFghQOBRwiz2ySRA6+RRxYarJkA7/O+7niIjQEFMriAmVyZA76N3m9yYKEDDQfT4LpwjQA=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["reasoning"], ["reasoning"], ["reasoning"], ["reasoning"], ["reasoning"], ["reasoning"], ["reasoning"], ["reasoning"]], "hovertemplate": "tarefa=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "reasoning", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "reasoning", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "2FBeQ3kNBUAk1egj1egDQFJF/XDtmQZAfPviIEHHB0DWDv/7rqcIQMPWjPOPDwRAJoMsSX2tA0AJbNpPlsALQA=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["recognition"], ["recognition"], ["recognition"], ["recognition"], ["recognition"], ["recognition"], ["recognition"], ["recognition"]], "hovertemplate": "tarefa=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "recognition", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "recognition", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "2FBeQ3kNJUCmpaWlpaUhQCbSA5WzWxxAumCS0WebG0D4XU+RqdAWQPSPD4zsUBhAQgNjKDJbFEBlCWzaTxYRQA=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["regression"], ["regression"], ["regression"], ["regression"], ["regression"], ["regression"], ["regression"], ["regression"]], "hovertemplate": "tarefa=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "regression", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "regression", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "LBWxVMRSDUAk1egj1egDQHDn+FhpwwJAKQXYtf6JAkAj1cmZzakBQPakzDHAwQNAQgNjKDJb9D+n/WQJbNr3Pw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["retrieval"], ["retrieval"], ["retrieval"], ["retrieval"], ["retrieval"], ["retrieval"], ["retrieval"], ["retrieval"]], "hovertemplate": "tarefa=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "retrieval", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "retrieval", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "DeU1lNdQCkBWLrhVLrgFQFJF/XDtmQZAL1M7NCvxAkAQhXWzekkIQFszzj8+MAZASsTocGjNCkDyXTiIHt8EQA=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["segmentation"], ["segmentation"], ["segmentation"], ["segmentation"], ["segmentation"], ["segmentation"], ["segmentation"], ["segmentation"]], "hovertemplate": "tarefa=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "segmentation", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "segmentation", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "MHvA7AGzHUDRleTQleQgQAOPEmaxTyBAgF3rnygFIECrl4Tzis4dQF5PyhwDHCBAmwIc7fRIIEDPSMXc6swgQA=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["tracking"], ["tracking"], ["tracking"], ["tracking"], ["tracking"], ["tracking"], ["tracking"], ["tracking"]], "hovertemplate": "tarefa=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "tracking", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "tracking", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "0IQ/E/5MFEBJXJdIXJcQQChbdQUeJQxAtrlgktFnC0DucuibFmAQQCVQ0Vs6DQtAQgNjKDJbBEAFNu0nS2AKQA=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["translation"], ["translation"], ["translation"], ["translation"], ["translation"], ["translation"], ["translation"], ["translation"]], "hovertemplate": "tarefa=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "translation", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "translation", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "FbFUxFIR6z8TYk4TYk4DQFJF/XDtmQZAPO8BMYS/A0BUIxbeb5sEQJEWyyNCUwFAlYMGxlBk9j+ZW52RirnzPw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["verification"], ["verification"], ["verification"], ["verification"], ["verification"], ["verification"], ["verification"], ["verification"]], "hovertemplate": "tarefa=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "verification", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "verification", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "FbFUxFIR6z8dk/Uck/XsPxTvIsAgtO4/g0lGn20u6D+EcWIiEo3XP/SPD4zsUNg/eQPQ5pu21T84iB7fhYPQPw=="}, "yaxis": "y", "type": "scatter"}], "layout": {"xaxis": {"anchor": "y", "domain": [0.0, 1.0], "title": {"text": "ano"}}, "yaxis": {"anchor": "x", "domain": [0.0, 1.0], "title": {"text": "ocorrências (%)"}}, "legend": {"title": {"text": "tarefa"}, "tracegroupgap": 0}}}
</code></pre> <p style="--stagger: 11;" data-animate="">Vamos nos aprofundar um pouco mais nas tarefas.</p> <p style="--stagger: 12;" data-animate="">Algoritmos focados em <strong>segurança e privacidade</strong> existem há algum tempo, mas o número de artigos publicados sobre eles aumentou significativamente no último ano. A detecção de spoofing é crucial para aplicações como o reconhecimento de identidade, onde atacantes podem tentar utilizar fotos ou vídeos para se fazer passar por outra pessoa, e parece ter ganhado urgência desde que as tecnologias deepfake se tornaram mais prevalentes.</p> <pre><code class="language-plotly">{"data": [{"customdata": [["adversarial attack"], ["adversarial attack"], ["adversarial attack"], ["adversarial attack"], ["adversarial attack"], ["adversarial attack"], ["adversarial attack"], ["adversarial attack"]], "hovertemplate": "tarefa=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "adversarial attack", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "adversarial attack", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "9oDZA2YPuD8dk/Uck/XcP1ZX4FHCLPY/D81KvEztAEBUIxbeb5v0P8HIDoWtGfc/BITgQiQb+T/5LhxEj+/2Pw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["anomaly detection"], ["anomaly detection"], ["anomaly detection"], ["anomaly detection"], ["anomaly detection"], ["anomaly detection"], ["anomaly detection"], ["anomaly detection"]], "hovertemplate": "tarefa=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "anomaly detection", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "anomaly detection", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "9oDZA2YPyD8TYk4TYk7jP3Dn+Fhpw+I/aRG5pbuR5j8j1cmZzanhPylzDHDwc+M/mwIc7fRI4D8Vc6szUjHvPw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["disambiguation"], ["disambiguation"], ["disambiguation"], ["disambiguation"], ["disambiguation"], ["disambiguation"], ["disambiguation"], ["disambiguation"]], "hovertemplate": "tarefa=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "disambiguation", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "disambiguation", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "9oDZA2YPuD8AAAAAAAAAAC5/O8fHSrs/AAAAAAAAAACEcWIiEo2nPylzDHDwc6M/mwIc7fRIwD+UJbBpP1nCPw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["face verification"], ["face verification"], ["face verification"], ["face verification"], ["face verification"], ["face verification"], ["face verification"], ["face verification"]], "hovertemplate": "tarefa=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "face verification", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "face verification", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "9oDZA2YP2D9WLrhVLrjlPy5/O8fHSts/NqGesldYwz+EcWIiEo23P76sEqjoLb0/eQPQ5pu2pT+61RmpmFudPw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["fact checking"], ["fact checking"], ["fact checking"], ["fact checking"], ["fact checking"], ["fact checking"], ["fact checking"], ["fact checking"]], "hovertemplate": "tarefa=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "fact checking", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "fact checking", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAClzDHDwc6M/AAAAAAAAAAAAAAAAAAAAAA=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["forensics"], ["forensics"], ["forensics"], ["forensics"], ["forensics"], ["forensics"], ["forensics"], ["forensics"]], "hovertemplate": "tarefa=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "forensics", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "forensics", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "FbFUxFIR6z+Y+iGY+iHYP2OfbNUVeOQ/T9krrAn15D87/O+7niLjP/SPD4zsUNg/WASE4EIkyz8Cm/aTJbDZPw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["fraud detection"], ["fraud detection"], ["fraud detection"], ["fraud detection"], ["fraud detection"], ["fraud detection"], ["fraud detection"], ["fraud detection"]], "hovertemplate": "tarefa=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "fraud detection", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "fraud detection", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAnYHTmB/LqT8AAAAAAAAAAClzDHDwc7M/eQPQ5pu2pT+61RmpmFu9Pw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["privacy"], ["privacy"], ["privacy"], ["privacy"], ["privacy"], ["privacy"], ["privacy"], ["privacy"]], "hovertemplate": "tarefa=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "privacy", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "privacy", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "9oDZA2YP6D8dk/Uck/XcPxTvIsAgtO4/0PHti4ME7T+XwbYIZe3wP8HIDoWtGfc/IAQXItnI+T+waT9ZApv6Pw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["safety"], ["safety"], ["safety"], ["safety"], ["safety"], ["safety"], ["safety"], ["safety"]], "hovertemplate": "tarefa=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "safety", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "safety", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "2FBeQ3kN9T8TYk4TYk7zP3Dn+FhpwwJAaRG5pbuR9j+cmIhE4wX5P11Ii+URoQFAjwTxnqx//D/yXTiIHt8EQA=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["spamming"], ["spamming"], ["spamming"], ["spamming"], ["spamming"], ["spamming"], ["spamming"], ["spamming"]], "hovertemplate": "tarefa=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "spamming", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "spamming", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAClzDHDwc6M/AAAAAAAAAAC61RmpmFudPw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["spoofing"], ["spoofing"], ["spoofing"], ["spoofing"], ["spoofing"], ["spoofing"], ["spoofing"], ["spoofing"]], "hovertemplate": "tarefa=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "spoofing", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "spoofing", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "9oDZA2YPuD8TYk4TYk7TP30vhdy8DuE/nYHTmB/L2T8j1cmZzanBP/SPD4zsUMg/eQPQ5pu2xT84iB7fhYPgPw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["spotting"], ["spotting"], ["spotting"], ["spotting"], ["spotting"], ["spotting"], ["spotting"], ["spotting"]], "hovertemplate": "tarefa=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "spotting", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "spotting", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "9oDZA2YP2D8dk/Uck/XcP2OfbNUVeMQ/0PHti4ME3T8j1cmZzanRP/SPD4zsUMg/eQPQ5pu2xT+UJbBpP1nCPw=="}, "yaxis": "y", "type": "scatter"}], "layout": {"xaxis": {"anchor": "y", "domain": [0.0, 1.0], "title": {"text": "ano"}}, "yaxis": {"anchor": "x", "domain": [0.0, 1.0], "title": {"text": "ocorrências (%)"}}, "legend": {"title": {"text": "tarefa"}, "tracegroupgap": 0}}}
</code></pre> <p style="--stagger: 13;" data-animate=""><strong>Interpretabilidade e explicabilidade</strong> ganharam destaque nos últimos anos, com um aumento significativo no número de artigos publicados sobre o tema por volta de 2019, após a criação de algumas conferências e workshops específicos sobre transparência de modelos, interpretabilidade e equidade, como o <a href="https://facctconference.org/" rel="external nofollow noopener" target="_blank">ACM FaccT</a> e o <a href="https://visxai.io/" rel="external nofollow noopener" target="_blank">VISxAI</a>. A explicabilidade é crucial para construir confiança em sistemas de IA e garantir que suas decisões sejam baseadas em raciocínio válido. Uma das áreas que mais recebeu investimentos nos últimos anos é a fundamentação do modelo, ou seja, o processo de relacionar as previsões do modelo a características específicas dos dados de entrada. Isso é particularmente importante em aplicações como classificação de imagens e resposta a perguntas, onde é essencial compreender quais partes de uma entrada (texto, imagem) estão impulsionando as previsões do modelo.</p> <pre><code class="language-plotly">{"data": [{"customdata": [["explainability"], ["explainability"], ["explainability"], ["explainability"], ["explainability"], ["explainability"], ["explainability"], ["explainability"]], "hovertemplate": "termo=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "explainability", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "explainability", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "9oDZA2YPuD8TYk4TYk6zPy5/O8fHSts/AjGEv/Me0D8LrqN3/DDgP76sEqjoLd0/CgP2acj/0j+61RmpmFvdPw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["grounding"], ["grounding"], ["grounding"], ["grounding"], ["grounding"], ["grounding"], ["grounding"], ["grounding"]], "hovertemplate": "termo=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "grounding", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "grounding", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "uSDjgowL0j9WLrhVLrjlP0kPVM5u4ec/NqGesldY4z+1v65mtH7qP11Ii+URofE/lYMGxlBk9j9ZApv2kyX6Pw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["interpretability"], ["interpretability"], ["interpretability"], ["interpretability"], ["interpretability"], ["interpretability"], ["interpretability"], ["interpretability"]], "hovertemplate": "termo=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "interpretability", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "interpretability", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "uSDjgowL4j+Y+iGY+iHYP3Dn+Fhpw/I/KQXYtf6J8j+XwbYIZe3wPyM7FLZmnO8/t4JSzKn28D9C9PguHETzPw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["traceability"], ["traceability"], ["traceability"], ["traceability"], ["traceability"], ["traceability"], ["traceability"], ["traceability"]], "hovertemplate": "termo=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "traceability", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "traceability", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAC61RmpmFudPw=="}, "yaxis": "y", "type": "scatter"}], "layout": {"legend": {"title": {"text": "termo"}, "tracegroupgap": 0}, "xaxis": {"anchor": "y", "domain": [0.0, 1.0], "title": {"text": "ano"}}, "yaxis": {"anchor": "x", "domain": [0.0, 1.0], "title": {"text": "ocorrências (%)"}}}}
</code></pre> <p style="--stagger: 14;" data-animate=""><strong>Tarefas visuais</strong> como a remoção de ruído em imagens têm recebido muita atenção nos últimos anos, com muitos artigos publicados sobre o tema. Isso pode ser devido à crescente importância da qualidade das imagens em aplicações de visão computacional, ao desenvolvimento de novas técnicas para aprimorar essa qualidade e à maior capacidade dos modelos visuais de lidar com entradas mais robustas. Essa categoria de tarefas também inclui desembaçamento, remoção de névoa, remoção de moiré, remoção de chuva, entre outras. As tarefas de processamento de imagens e de geração de imagens também aumentaram significativamente.</p> <pre><code class="language-plotly">{"data": [{"customdata": [["colorization"], ["colorization"], ["colorization"], ["colorization"], ["colorization"], ["colorization"], ["colorization"], ["colorization"]], "hovertemplate": "tarefa=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "colorization", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "colorization", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "NOHPhD8T3j8dk/Uck/XMPy5/O8fHSts/AjGEv/Me0D+EcWIiEo23P/SPD4zsUMg/eQPQ5pu2tT+UJbBpP1nCPw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["denoising"], ["denoising"], ["denoising"], ["denoising"], ["denoising"], ["denoising"], ["denoising"], ["denoising"]], "hovertemplate": "tarefa=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "denoising", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "denoising", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "5+ibo2+OBkAdk/Uck/UMQB4lzGKfbA1A6il7hTWhDkAHXUtAUZkLQPSPD4zsUAhAPIRNAY52CkB1RirmVucVQA=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["editing"], ["editing"], ["editing"], ["editing"], ["editing"], ["editing"], ["editing"], ["editing"]], "hovertemplate": "tarefa=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "editing", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "editing", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "9oDZA2YP6D/bxovaxovqP3Dn+Fhpw+I/Qj1lr7Am9D/HDwNNB9/zP4vlEaGp9vs/zYNzhLq/B0CNVMytzkgQQA=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["image enhancement"], ["image enhancement"], ["image enhancement"], ["image enhancement"], ["image enhancement"], ["image enhancement"], ["image enhancement"], ["image enhancement"]], "hovertemplate": "tarefa=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "image enhancement", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "image enhancement", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "AAAAAAAAAACY+iGY+iHYPy5/O8fHSss/nYHTmB/L2T+1v65mtH7aP/SPD4zsUNg/CgP2acj/0j8Vc6szUjHfPw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["image filling"], ["image filling"], ["image filling"], ["image filling"], ["image filling"], ["image filling"], ["image filling"], ["image filling"]], "hovertemplate": "tarefa=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "image filling", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "image filling", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "JUmSJEmS/D/8rMD7rMD7P4BBaL2RoQBAnYHTmB/LCUD4XU+RqdAGQMDBz028nghAQgNjKDJbBEC1nyyBTfsLQA=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["image generation"], ["image generation"], ["image generation"], ["image generation"], ["image generation"], ["image generation"], ["image generation"], ["image generation"]], "hovertemplate": "tarefa=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "image generation", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "image generation", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "2FBeQ3kN5T8dk/Uck/X8Py5/O8fHSvs/HGkRuaW7AUALrqN3/DAAQIrXkzLHAP8/lYMGxlBkBkC+CwfR47sOQA=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["image retrieval"], ["image retrieval"], ["image retrieval"], ["image retrieval"], ["image retrieval"], ["image retrieval"], ["image retrieval"], ["image retrieval"]], "hovertemplate": "tarefa=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "image retrieval", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "image retrieval", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "uSDjgowL8j9fX19fX1/vP2OfbNUVePQ/XHXyqGLD9T+XwbYIZe3wP8TkCmJyBeE/IAQXItnI6T+UJbBpP1niPw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["image segmentation"], ["image segmentation"], ["image segmentation"], ["image segmentation"], ["image segmentation"], ["image segmentation"], ["image segmentation"], ["image segmentation"]], "hovertemplate": "tarefa=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "image segmentation", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "image segmentation", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "NOHPhD8T7j81SIM0SIP0P1x7phNsUvU/trlgktFn6z+cmIhE4wXpP/erC2mxPPI/CgP2acj/8j9VzK3OSMX4Pw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["image to image"], ["image to image"], ["image to image"], ["image to image"], ["image to image"], ["image to image"], ["image to image"], ["image to image"]], "hovertemplate": "tarefa=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "image to image", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "image to image", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "9oDZA2YP2D93FO12FO32P4RTS55mNABAT9krrAn19D8QhXWzekn4P4vlEaGp9us/CgP2acj/4j+n/WQJbNrnPw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["localization"], ["localization"], ["localization"], ["localization"], ["localization"], ["localization"], ["localization"], ["localization"]], "hovertemplate": "tarefa=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "localization", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "localization", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "2FBeQ3kNFUAD77MC77MSQChbdQUeJQxAaRG5pbuRBkBxIQ48vywOQL2l03D3sg5ASsTocGjNCkC3OiMVc6sMQA=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["matching"], ["matching"], ["matching"], ["matching"], ["matching"], ["matching"], ["matching"], ["matching"]], "hovertemplate": "tarefa=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "matching", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "matching", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "Cn8m/JnwGUCcm5ubm5sTQHJwasnTjBJASYvILd2NFEDbIpS1w/8WQBCM7FDYmhNAV+PLz3ndFEA/WQKb9pMSQA=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["odometry"], ["odometry"], ["odometry"], ["odometry"], ["odometry"], ["odometry"], ["odometry"], ["odometry"]], "hovertemplate": "tarefa=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "odometry", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "odometry", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "9oDZA2YPyD+Y+iGY+iHYP2OfbNUVeNQ/0PHti4ME3T9UIxbeb5vUP/SPD4zsUMg/WASE4EIkyz+UJbBpP1nCPw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["quality assessment"], ["quality assessment"], ["quality assessment"], ["quality assessment"], ["quality assessment"], ["quality assessment"], ["quality assessment"], ["quality assessment"]], "hovertemplate": "tarefa=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "quality assessment", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "quality assessment", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "9oDZA2YPyD8TYk4TYk7TPy5/O8fHSss/nYHTmB/LyT9UIxbeb5vUP44BDn5u4tU/mwIc7fRI0D84iB7fhYPgPw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["reconstruction"], ["reconstruction"], ["reconstruction"], ["reconstruction"], ["reconstruction"], ["reconstruction"], ["reconstruction"], ["reconstruction"]], "hovertemplate": "tarefa=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "reconstruction", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "reconstruction", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "vYbyGsprEkDv2p/u2p8WQGOfbNUVeBRAgKIUYNf6F0DN5tSIhfcbQI3zjw+M7BhA3XI9f4LlIUCUJbBpP9keQA=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["removal"], ["removal"], ["removal"], ["removal"], ["removal"], ["removal"], ["removal"], ["removal"]], "hovertemplate": "tarefa=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "removal", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "removal", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "uSDjgowL8j9WLrhVLrj1P0kPVM5u4fc/T9krrAn19D/HDwNNB9/zPyM7FLZmnO8/xwReXRbb7T/mVmekYm7xPw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["style transfer"], ["style transfer"], ["style transfer"], ["style transfer"], ["style transfer"], ["style transfer"], ["style transfer"], ["style transfer"]], "hovertemplate": "tarefa=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "style transfer", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "style transfer", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "uSDjgowL4j9fX19fX1/vPxTvIsAgtO4/HGkRuaW74T/HDwNNB9/zPylzDHDwc+M/xwReXRbb3T/mVmekYm7hPw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["super resolution"], ["super resolution"], ["super resolution"], ["super resolution"], ["super resolution"], ["super resolution"], ["super resolution"], ["super resolution"]], "hovertemplate": "tarefa=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "super resolution", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "super resolution", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "2FBeQ3kNBUDrOSbrOSYLQEX9cO2ZTghAsGv9E6UAC0C0/HHkSr4QQFkeEZpqvwpAQgNjKDJbBECwaT9ZApsKQA=="}, "yaxis": "y", "type": "scatter"}], "layout": {"xaxis": {"anchor": "y", "domain": [0.0, 1.0], "title": {"text": "ano"}}, "yaxis": {"anchor": "x", "domain": [0.0, 1.0], "title": {"text": "ocorrências (%)"}}, "legend": {"title": {"text": "tarefa"}, "tracegroupgap": 0}}}
</code></pre> <p style="--stagger: 15;" data-animate=""><strong>Tarefas de linguagem</strong> também têm observado uma variação no número de artigos publicados ao longo dos últimos anos, especialmente aqueles que se concentram em diálogo e conversação. Ao utilizar uma interface conversacional, os usuários podem interagir com sistemas de IA de forma mais natural e intuitiva, proporcionando melhores experiências e uma comunicação mais eficaz. Isso levou a um aumento nas pesquisas sobre sistemas de diálogo, incluindo chatbots, assistentes virtuais e outros agentes conversacionais. O desenvolvimento de modelos de linguagem em larga escala também desempenhou um papel significativo nessa tendência, pois esses modelos demonstraram capacidades impressionantes de gerar textos semelhantes aos humanos e de compreender o contexto.</p> <pre><code class="language-plotly">{"data": [{"customdata": [["dialog"], ["dialog"], ["dialog"], ["dialog"], ["dialog"], ["dialog"], ["dialog"], ["dialog"]], "hovertemplate": "tarefa=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "dialog", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "dialog", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "9oDZA2YP2D/RleTQleTgPxTvIsAgtN4/nYHTmB/L2T/lDfuqVnDNPylzDHDwc8M/mwIc7fRIwD+dkYq51RnlPw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["language translation"], ["language translation"], ["language translation"], ["language translation"], ["language translation"], ["language translation"], ["language translation"], ["language translation"]], "hovertemplate": "tarefa=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "language translation", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "language translation", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "AAAAAAAAAAATYk4TYk6zPwAAAAAAAAAAnYHTmB/LqT+EcWIiEo2nPylzDHDwc7M/eQPQ5pu2pT+61RmpmFudPw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["question answering"], ["question answering"], ["question answering"], ["question answering"], ["question answering"], ["question answering"], ["question answering"], ["question answering"]], "hovertemplate": "tarefa=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "question answering", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "question answering", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "FbFUxFIR6z8TYk4TYk7zP3Dn+Fhpw+I/nYHTmB/L2T+EcWIiEo3XP/SPD4zsUNg/CgP2acj/4j9C9PguHETjPw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["summarization"], ["summarization"], ["summarization"], ["summarization"], ["summarization"], ["summarization"], ["summarization"], ["summarization"]], "hovertemplate": "tarefa=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "summarization", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "summarization", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "9oDZA2YP6D+Y+iGY+iHYP2OfbNUVeMQ/nYHTmB/LuT+EcWIiEo23PylzDHDwc8M/eQPQ5pu2tT+UJbBpP1nCPw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["text generation"], ["text generation"], ["text generation"], ["text generation"], ["text generation"], ["text generation"], ["text generation"], ["text generation"]], "hovertemplate": "tarefa=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "text generation", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "text generation", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACEcWIiEo2nPwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA=="}, "yaxis": "y", "type": "scatter"}], "layout": {"legend": {"title": {"text": "tarefa"}, "tracegroupgap": 0}, "xaxis": {"anchor": "y", "domain": [0.0, 1.0], "title": {"text": "ano"}}, "yaxis": {"anchor": "x", "domain": [0.0, 1.0], "title": {"text": "ocorrências (%)"}}}}
</code></pre> <p style="--stagger: 16;" data-animate=""><strong>Tarefas multimodais</strong> são uma das tendências atuais na inteligência artificial. Essas tarefas envolvem a combinação de diferentes modalidades, como áudio, texto e imagens, para melhorar o desempenho dos modelos e resolver problemas que exigem uma compreensão mais profunda da intermodalidade do mundo. O número de artigos publicados sobre essas tarefas aumentou significativamente nos últimos anos, com um foco especial em tarefas como alinhamento de imagem-texto, síntese de imagens, síntese de vídeos e resposta a perguntas visuais. Essa tendência provavelmente continuará à medida que os pesquisadores exploram novas maneiras de combinar as diferentes modalidades de forma inovadora e aprimoram o desempenho dos modelos.</p> <pre><code class="language-plotly">{"data": [{"customdata": [["alignment"], ["alignment"], ["alignment"], ["alignment"], ["alignment"], ["alignment"], ["alignment"], ["alignment"]], "hovertemplate": "tarefa=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "alignment", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "alignment", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "5+ibo2+OBkAD77MC77MCQGCNifRA5QRAiZepHZqVCEAo6V5T4gEQQHYobM04/xJAV+PLz3ndFECuzkjF3OoZQA=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["audio synthesis"], ["audio synthesis"], ["audio synthesis"], ["audio synthesis"], ["audio synthesis"], ["audio synthesis"], ["audio synthesis"], ["audio synthesis"]], "hovertemplate": "tarefa=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "audio synthesis", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "audio synthesis", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAClzDHDwc7M/eQPQ5pu2tT+61RmpmFudPw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["captioning"], ["captioning"], ["captioning"], ["captioning"], ["captioning"], ["captioning"], ["captioning"], ["captioning"]], "hovertemplate": "tarefa=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "captioning", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "captioning", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "JUmSJEmS/D/yexnyexnyP3YLvxoT6fE/T9krrAn15D9UIxbeb5vkP76sEqjoLe0/mwIc7fRI8D/5LhxEj+/2Pw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["image synthesis"], ["image synthesis"], ["image synthesis"], ["image synthesis"], ["image synthesis"], ["image synthesis"], ["image synthesis"], ["image synthesis"]], "hovertemplate": "tarefa=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "image synthesis", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "image synthesis", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "2FBeQ3kN5T8dk/Uck/X8Py5/O8fHSvs/HGkRuaW7AUALrqN3/DAAQIrXkzLHAP8/lYMGxlBkBkC+CwfR47sOQA=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["referring expression comprehension"], ["referring expression comprehension"], ["referring expression comprehension"], ["referring expression comprehension"], ["referring expression comprehension"], ["referring expression comprehension"], ["referring expression comprehension"], ["referring expression comprehension"]], "hovertemplate": "tarefa=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "referring expression comprehension", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "referring expression comprehension", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "9oDZA2YPuD8TYk4TYk7DPy5/O8fHSrs/NqGesldYwz+EcWIiEo23PylzDHDwc6M/CgP2acj/0j+61RmpmFu9Pw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["video question answering"], ["video question answering"], ["video question answering"], ["video question answering"], ["video question answering"], ["video question answering"], ["video question answering"], ["video question answering"]], "hovertemplate": "tarefa=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "video question answering", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "video question answering", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "AAAAAAAAAAATYk4TYk6zPy5/O8fHSqs/NqGesldYwz+EcWIiEo3HP/SPD4zsUMg/mwIc7fRI4D9LYNN+sgTWPw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["video synthesis"], ["video synthesis"], ["video synthesis"], ["video synthesis"], ["video synthesis"], ["video synthesis"], ["video synthesis"], ["video synthesis"]], "hovertemplate": "tarefa=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "video synthesis", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "video synthesis", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "AAAAAAAAAAATYk4TYk7TP0kPVM5u4dc/NqGesldY0z8j1cmZzanRP/SPD4zsUNg/xwReXRbb3T+P78JB9PjwPw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["visual grounding"], ["visual grounding"], ["visual grounding"], ["visual grounding"], ["visual grounding"], ["visual grounding"], ["visual grounding"], ["visual grounding"]], "hovertemplate": "tarefa=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "visual grounding", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "visual grounding", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "9oDZA2YPuD8dk/Uck/XMPy5/O8fHSrs/nYHTmB/LuT8j1cmZzanRPylzDHDwc9M/CgP2acj/0j9eOIge34XbPw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["visual question answering"], ["visual question answering"], ["visual question answering"], ["visual question answering"], ["visual question answering"], ["visual question answering"], ["visual question answering"], ["visual question answering"]], "hovertemplate": "tarefa=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "visual question answering", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "visual question answering", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "yLgg44KM8z8+eSo+eSr+P1x7phNsUvU/D81KvEzt8D8LrqN3/DDwP76sEqjoLe0/CgP2acj/8j9QlsCm/WT3Pw=="}, "yaxis": "y", "type": "scatter"}], "layout": {"xaxis": {"anchor": "y", "domain": [0.0, 1.0], "title": {"text": "ano"}}, "yaxis": {"anchor": "x", "domain": [0.0, 1.0], "title": {"text": "ocorrências (%)"}}, "legend": {"title": {"text": "tarefa"}, "tracegroupgap": 0}}}
</code></pre> <p style="--stagger: 17;" data-animate="">Aqui nos concentramos em analisar o uso de algumas palavras-chave nos artigos sobre LLM. Mais especificamente:</p> <ul style="--stagger: 18;" data-animate=""> <li> <strong>Chain-of-Thought</strong>, <strong>Tree-of-Thought</strong> e quaisquer variações de <strong>of-Thought</strong> - técnicas de prompting que auxiliam o modelo a decompor tarefas complexas em etapas menores e mais gerenciáveis, permitindo que o raciocínio seja realizado de forma mais eficaz;</li> <li> <strong>Agent</strong> - refere-se ao uso de LLMs como agentes que podem executar tarefas de forma autônoma, frequentemente em conjunto com outras ferramentas ou sistemas;</li> <li> <strong>Distillation</strong> - uma técnica utilizada para comprimir modelos grandes em modelos menores e mais eficientes, mantendo seu desempenho;</li> <li> <strong>Few-shot prompting</strong> - uma técnica de prompting que fornece ao modelo alguns exemplos da tarefa em questão, permitindo que ele generalize e execute bem tarefas similares;</li> <li> <strong>Fine-tuning</strong> - o processo de treinar um modelo pré-treinado em uma tarefa ou conjunto de dados específico para melhorar seu desempenho;</li> <li> <strong>Reinforcement Learning (RL)</strong> - um tipo de aprendizagem de máquina onde um agente aprende a tomar decisões recebendo feedback do ambiente na forma de recompensas ou penalidades;</li> <li> <strong>Retrieval Augmented Generation (RAG)</strong> - uma técnica que combina métodos baseados em recuperação com modelos generativos para melhorar o desempenho dos modelos de linguagem em tarefas específicas;</li> <li> <strong>Self-Instruct</strong> - uma técnica que permite que os modelos aprendam com suas próprias saídas, melhorando seu desempenho ao longo do tempo;</li> <li> <strong>Tokenizer</strong> - um componente dos modelos de linguagem que converte o texto em um formato que o modelo possa compreender, frequentemente dividindo-o em unidades menores chamadas tokens;</li> <li> <strong>Tool</strong> - refere-se ao uso de ferramentas ou sistemas externos em conjunto com LLMs para executar tarefas de forma mais eficaz;</li> <li> <strong>Zero-shot prompting</strong> - uma técnica de prompting que permite ao modelo executar tarefas sem exemplos prévios ou treinamento específico para aquela tarefa.</li> </ul> <p style="--stagger: 19;" data-animate="">As técnicas de few-shot e zero-shot prompting perderam o interesse da comunidade acadêmica em favor do RAG, dos processos de pensamento (thought) e de novas técnicas de fine-tuning. O interesse em criar agentes LLM capazes de realizar tarefas mais desafiadoras e utilizar ferramentas é um dos tópicos mais quentes na área.</p> <pre><code class="language-plotly">{"data": [{"customdata": [["* of thought"], ["* of thought"], ["* of thought"], ["* of thought"], ["* of thought"], ["* of thought"], ["* of thought"]], "hovertemplate": "termo=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "* of thought", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "* of thought", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+QH5QfmB+cH6Ac="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAfDQhL2wVBEA="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["agent"], ["agent"], ["agent"], ["agent"], ["agent"], ["agent"], ["agent"]], "hovertemplate": "termo=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "agent", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "agent", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+QH5QfmB+cH6Ac="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABbUZmK5zHEA="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["distillation"], ["distillation"], ["distillation"], ["distillation"], ["distillation"], ["distillation"], ["distillation"]], "hovertemplate": "termo=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "distillation", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "distillation", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+QH5QfmB+cH6Ac="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABJQI7jOI7jOBZAus6xRiIgDkA="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["few shot"], ["few shot"], ["few shot"], ["few shot"], ["few shot"], ["few shot"], ["few shot"]], "hovertemplate": "termo=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "few shot", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "few shot", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+QH5QfmB+cH6Ac="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAKuqqqqqqjBAMU653d/BJUA="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["finetuning"], ["finetuning"], ["finetuning"], ["finetuning"], ["finetuning"], ["finetuning"], ["finetuning"]], "hovertemplate": "termo=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "finetuning", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "finetuning", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+QH5QfmB+cH6Ac="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAI7jOI7jOCZAus6xRiIgLkA="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["reinforcement learning"], ["reinforcement learning"], ["reinforcement learning"], ["reinforcement learning"], ["reinforcement learning"], ["reinforcement learning"], ["reinforcement learning"]], "hovertemplate": "termo=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "reinforcement learning", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "reinforcement learning", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+QH5QfmB+cH6Ac="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "AAAAAAAASUAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAKuqqqqqqjBA3P0dXPaGQEA="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["retrieval augmented generation"], ["retrieval augmented generation"], ["retrieval augmented generation"], ["retrieval augmented generation"], ["retrieval augmented generation"], ["retrieval augmented generation"], ["retrieval augmented generation"]], "hovertemplate": "termo=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "retrieval augmented generation", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "retrieval augmented generation", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+QH5QfmB+cH6Ac="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "AAAAAAAAOUAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABJQKuqqqqqqkBAayQC4qMJQ0A="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["tokenizer"], ["tokenizer"], ["tokenizer"], ["tokenizer"], ["tokenizer"], ["tokenizer"], ["tokenizer"]], "hovertemplate": "termo=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "tokenizer", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "tokenizer", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+QH5QfmB+cH6Ac="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAfDQhL2wV9D8="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["tool"], ["tool"], ["tool"], ["tool"], ["tool"], ["tool"], ["tool"]], "hovertemplate": "termo=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "tool", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "tool", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+QH5QfmB+cH6Ac="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "AAAAAAAAOUAAAAAAAAAAAAAAAAAAAElAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAMU653d/BFUA="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["zero shot"], ["zero shot"], ["zero shot"], ["zero shot"], ["zero shot"], ["zero shot"], ["zero shot"]], "hovertemplate": "termo=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "zero shot", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "zero shot", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+QH5QfmB+cH6Ac="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABJQBzHcRzHcUNAoifVVzI/M0A="}, "yaxis": "y", "type": "scatter"}], "layout": {"legend": {"title": {"text": "termo"}, "tracegroupgap": 0}, "xaxis": {"anchor": "y", "domain": [0.0, 1.0], "title": {"text": "ano"}}, "yaxis": {"anchor": "x", "domain": [0.0, 1.0], "title": {"text": "ocorrências (%)"}}}}
</code></pre> <h2 id="informações-sobre-os-autores" style="--stagger: 20;" data-animate="">Informações sobre os Autores</h2> <p style="--stagger: 21;" data-animate="">Agora, vamos analisar os autores dos artigos. Este primeiro gráfico mostra o número de artigos publicados por cada autor. Como podemos ver, a maioria dos autores publicou apenas um artigo na conferência. De um total de 33.861 autores, somente 1.308 possuem 10 ou mais artigos aceitos.</p> <pre><code class="language-plotly">{"data": [{"hovertemplate": "Número de artigos=%{text}&lt;br&gt;Autores=%{y}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "", "marker": {"pattern": {"shape": ""}}, "name": "", "orientation": "v", "showlegend": false, "text": {"dtype": "f8", "bdata": "AAAAAADAYEAAAAAAAMBdQAAAAAAAAFlAAAAAAACAVUAAAAAAAMBUQAAAAAAAAFRAAAAAAADAU0AAAAAAAMBSQAAAAAAAwFFAAAAAAACAUUAAAAAAAEBRQAAAAAAAgFBAAAAAAABAUEAAAAAAAABQQAAAAAAAgE9AAAAAAAAAT0AAAAAAAIBOQAAAAAAAgE1AAAAAAAAATUAAAAAAAABMQAAAAAAAgEtAAAAAAAAAS0AAAAAAAIBKQAAAAAAAAEpAAAAAAACASUAAAAAAAABJQAAAAAAAgEhAAAAAAAAASEAAAAAAAIBHQAAAAAAAAEdAAAAAAAAARkAAAAAAAIBFQAAAAAAAAEVAAAAAAACAREAAAAAAAABEQAAAAAAAgENAAAAAAAAAQ0AAAAAAAIBCQAAAAAAAAEJAAAAAAACAQUAAAAAAAABBQAAAAAAAgEBAAAAAAAAAQEAAAAAAAAA/QAAAAAAAAD5AAAAAAAAAPUAAAAAAAAA8QAAAAAAAADtAAAAAAAAAOkAAAAAAAAA5QAAAAAAAADhAAAAAAAAAN0AAAAAAAAA2QAAAAAAAADVAAAAAAAAANEAAAAAAAAAzQAAAAAAAADJAAAAAAAAAMUAAAAAAAAAwQAAAAAAAAC5AAAAAAAAALEAAAAAAAAAqQAAAAAAAAChAAAAAAAAAJkAAAAAAAAAkQAAAAAAAACJAAAAAAAAAIEAAAAAAAAAcQAAAAAAAABhAAAAAAAAAFEAAAAAAAAAQQAAAAAAAAAhAAAAAAAAAAEAAAAAAAADwPw=="}, "textposition": "auto", "x": {"dtype": "i2", "bdata": "hgB3AGQAVgBTAFAATwBLAEcARgBFAEIAQQBAAD8APgA9ADsAOgA4ADcANgA1ADQAMwAyADEAMAAvAC4ALAArACoAKQAoACcAJgAlACQAIwAiACEAIAAfAB4AHQAcABsAGgAZABgAFwAWABUAFAATABIAEQAQAA8ADgANAAwACwAKAAkACAAHAAYABQAEAAMAAgABAA=="}, "xaxis": "x", "y": {"dtype": "i2", "bdata": "AQABAAEAAQABAAEAAQABAAEAAQABAAEAAQABAAIAAQABAAEABQAEAAIAAQABAAMAAQACAAIABAABAAIABAAEAAQAAgAHAAgACwAGAAsABAAFAAYAFAALABIACwASABEAEgAVAB4AGAAWACQAIgAyACMALABAAFEAWABrAHsAlgCzAAIBKgGeAUMCSQOXBW8J3RU8UQ=="}, "yaxis": "y", "type": "bar"}], "layout": {"xaxis": {"anchor": "y", "domain": [0.0, 1.0], "title": {"text": "Número de artigos"}}, "yaxis": {"anchor": "x", "domain": [0.0, 1.0], "title": {"text": "Autores"}}, "legend": {"tracegroupgap": 0}, "barmode": "relative"}}
</code></pre> <p>Aqui estão os 10 autores com mais artigos:</p> <table> <thead> <tr> <th>Autor</th> <th>Artigos</th> </tr> </thead> <tbody> <tr> <td>Luc Van Gool</td> <td>134</td> </tr> <tr> <td>Radu Timofte</td> <td>119</td> </tr> <tr> <td>Lei Zhang</td> <td>100</td> </tr> <tr> <td>Yi Yang</td> <td>86</td> </tr> <tr> <td>Yu Qiao</td> <td>83</td> </tr> <tr> <td>Dacheng Tao</td> <td>80</td> </tr> <tr> <td>Ming-Hsuan Yang</td> <td>79</td> </tr> <tr> <td>Qi Tian</td> <td>75</td> </tr> <tr> <td>Marc Pollefeys</td> <td>71</td> </tr> <tr> <td>Xiaogang Wang</td> <td>70</td> </tr> </tbody> </table> <p>Agora vamos analisar o número de autores por artigo. A maioria dos artigos tem entre 2 e 7 autores, mas há alguns com um número elevado de autores, como <a href="https://openaccess.thecvf.com/content/CVPR2023/html/Eisenmann_Why_Is_the_Winner_the_Best_CVPR_2023_paper.html" rel="external nofollow noopener" target="_blank">Why Is the Winner the Best?</a>, que conta com 125 autores, e <a href="https://openaccess.thecvf.com/content/CVPR2024W/NTIRE/html/Ren_The_Ninth_NTIRE_2024_Efficient_Super-Resolution_Challenge_Report_CVPRW_2024_paper.html" rel="external nofollow noopener" target="_blank">The Ninth NTIRE 2024 Efficient Super-Resolution Challenge Report</a>, com impressionantes 134 autores. O primeiro é um estudo multicêntrico de todas as 80 competições realizadas no âmbito do IEEE ISBI 2021 e MICCAI 2021, enquanto o segundo é um relatório que resume os resultados do desafio NTIRE 2024, uma competição realizada na conferência CVPR.</p> <pre><code class="language-plotly">{"data": [{"hovertemplate": "Número de autores=%{x}&lt;br&gt;Número de artigos=%{text}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "", "marker": {"pattern": {"shape": ""}}, "name": "", "orientation": "v", "showlegend": false, "text": {"dtype": "f8", "bdata": "AAAAAAAgZ0AAAAAAAByWQAAAAAAAoqZAAAAAAAAArEAAAAAAAE6pQAAAAAAA2KJAAAAAAADklkAAAAAAALiIQAAAAAAA0HhAAAAAAADAaUAAAAAAAIBcQAAAAAAAAFBAAAAAAAAAOUAAAAAAAAA3QAAAAAAAACRAAAAAAAAAJkAAAAAAAAAQQAAAAAAAAABAAAAAAAAA8D8AAAAAAAAQQAAAAAAAAABAAAAAAAAAAEAAAAAAAAAQQAAAAAAAAAhAAAAAAAAA8D8AAAAAAAAIQAAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAAAEAAAAAAAAAAQAAAAAAAAPA/AAAAAAAA8D8AAAAAAAAAQAAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAACEAAAAAAAADwPwAAAAAAAPA/AAAAAAAACEAAAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8="}, "textposition": "auto", "x": {"dtype": "i2", "bdata": "AQACAAMABAAFAAYABwAIAAkACgALAAwADQAOAA8AEAARABIAEwAUABUAFgAXABgAGwAcAB8AIQAiACMAJAAlACcAKQAqACsALQAwADEANQA3ADgAOgBCAEMARABOAE8AVQBYAF0AZABlAGwAcQBzAH0AhgA="}, "xaxis": "x", "y": {"dtype": "i2", "bdata": "uQCHBVELAA6nDGwJuQUXA40BzgByAEAAGQAXAAoACwAEAAIAAQAEAAIAAgAEAAMAAQADAAEAAQABAAEAAgACAAEAAQACAAEAAQABAAEAAQABAAEAAwABAAEAAwABAAEAAQABAAEAAQABAAEAAQABAAEAAQA="}, "yaxis": "y", "type": "bar"}], "layout": {"xaxis": {"anchor": "y", "domain": [0.0, 1.0], "title": {"text": "Número de autores"}}, "yaxis": {"anchor": "x", "domain": [0.0, 1.0], "title": {"text": "Número de artigos"}}, "legend": {"tracegroupgap": 0}, "barmode": "relative"}}
</code></pre> <p>Como a maioria dos artigos tem múltiplos autores, é bastante comum ver alguns autores colaborando constantemente entre si. O par de autores mais comum é Jiwen Lu e Jie Zhou, que colaboraram em 57 artigos juntos. O segundo par mais comum é Luc Van Gool e Radu Timofte, com 43 artigos juntos, seguido por Tao Xiang e Yi-Zhe Song, com 38 artigos. Os 10 pares de autores mais frequentes são:</p> <table> <thead> <tr> <th>Autor 1</th> <th>Autor 2</th> <th>Artigos</th> </tr> </thead> <tbody> <tr> <td>Jiwen Lu</td> <td>Jie Zhou</td> <td>57</td> </tr> <tr> <td>Luc Van Gool</td> <td>Radu Timofte</td> <td>43</td> </tr> <tr> <td>Tao Xiang</td> <td>Yi-Zhe Song</td> <td>38</td> </tr> <tr> <td>Fahad Shahbaz Khan</td> <td>Salman Khan</td> <td>33</td> </tr> <tr> <td>Ting Yao</td> <td>Tao Mei</td> <td>32</td> </tr> <tr> <td>Xiaogang Wang</td> <td>Hongsheng Li</td> <td>28</td> </tr> <tr> <td>Shiguang Shan</td> <td>Xilin Chen</td> <td>27</td> </tr> <tr> <td>Richa Singh</td> <td>Mayank Vatsa</td> <td>26</td> </tr> <tr> <td>Dong Chen</td> <td>Fang Wen</td> <td>24</td> </tr> <tr> <td>Yi-Zhe Song</td> <td>Ayan Kumar Bhunia</td> <td>24</td> </tr> </tbody> </table> <p>Embora seja bastante raro que um artigo tenha um único autor, 185 trabalhos se enquadram nessa categoria. Algumas menções que merecem destaque incluem pesquisas que introduziram funções de perda inovadoras (<a href="https://openaccess.thecvf.com/content_CVPR_2019/html/Barron_A_General_and_Adaptive_Robust_Loss_Function_CVPR_2019_paper.html" rel="external nofollow noopener" target="_blank">Jonathan T. Barron</a>, <a href="https://openaccess.thecvf.com/content/CVPR2023/html/Kobayashi_Two-Way_Multi-Label_Loss_CVPR_2023_paper.html" rel="external nofollow noopener" target="_blank">Takumi Kobayashi</a>) e aprimoramentos em arquiteturas de transformers e técnicas de pós-treinamento (<a href="https://openaccess.thecvf.com/content/CVPR2024/html/Kobayashi_Mean-Shift_Feature_Transformer_CVPR_2024_paper.html" rel="external nofollow noopener" target="_blank">Takumi Kobayashi</a>, <a href="https://openaccess.thecvf.com/content/CVPR2024/html/Ma_Improved_Self-Training_for_Test-Time_Adaptation_CVPR_2024_paper.html" rel="external nofollow noopener" target="_blank">Jing Ma</a>). Nesta tabela, podemos ver os autores com o maior número de artigos nos quais são os únicos autores.</p> <table> <thead> <tr> <th>Autor</th> <th>Artigos</th> </tr> </thead> <tbody> <tr> <td>Takumi Kobayashi</td> <td>4</td> </tr> <tr> <td>Anant Khandelwal, Takuhiro Kaneko</td> <td>3</td> </tr> <tr> <td>Andrey V. Savchenko, Chong Yu, Dimitrios Kollias, Edgar A. Bernal, Jamie Hayes, Magnus Oskarsson, Ming Li, Oleksii Sidorov, Ren Yang, Rowel Atienza, Sanghwa Hong, Satoshi Ikehata, Shunta Maeda, Stamatios Lefkimmiatis, Ying Zhao</td> <td>2</td> </tr> </tbody> </table> <h2 id="identificando-tópicos">Identificando Tópicos</h2> <p>Para esta seção, utilizamos o <a href="https://arxiv.org/abs/2008.09470" rel="external nofollow noopener" target="_blank">Top2Vec</a>, um algoritmo de modelagem automática de tópicos, para identificar grupos de artigos que são semelhantes entre si com base em seus títulos e resumos. A solução identificou 172 tópicos, o que é um número um pouco alto para analisarmos individualmente. Em vez disso, focaremos nos tópicos mais quentes e mais frios, ou seja, aqueles com o maior e o menor número de artigos no último ano, respectivamente.</p> <p>Um problema com o algoritmo é que ele identifica tópicos com base nas palavras utilizadas nos artigos, mas não fornece uma explicação clara sobre o que esses tópicos abordam. Esse é um desafio comum em algoritmos de modelagem de tópicos, que frequentemente produzem resultados difíceis de interpretar. No entanto, podemos utilizar LLMs para nos ajudar a compreender o significado desses tópicos. Usaremos as palavras mais representativas de cada tópico (aquelas que aparecem com maior frequência nos artigos daquele tópico) para gerar um título e um parágrafo que o resuma.</p> <h3 id="-10-tópicos">🔥 10 tópicos</h3> <pre><code class="language-plotly">{"data": [{"customdata": {"dtype": "i1", "bdata": "AQEBAQEBAQE=", "shape": "8, 1"}, "hovertemplate": "tópico=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "1", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "1", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "AAAAAAAAAAATYk4TYk6zPy5/O8fHSrs/nYHTmB/LqT8AAAAAAAAAAClzDHDwc6M/WASE4EIk2z+NVMytzkgQQA=="}, "yaxis": "y", "type": "scatter"}, {"customdata": {"dtype": "i1", "bdata": "AgICAgICAgI=", "shape": "8, 1"}, "hovertemplate": "tópico=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "2", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "2", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "9oDZA2YP2D9WLrhVLrjlPy5/O8fHSus/T9krrAn19D/gNilv2Ff1P1keEZpqv/o/IAQXItnICUBXZ6RibnUZQA=="}, "yaxis": "y", "type": "scatter"}, {"customdata": {"dtype": "i1", "bdata": "AwMDAwMDAwM=", "shape": "8, 1"}, "hovertemplate": "tópico=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "3", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "3", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "yLgg44KMA0DhCH/hCH8BQHodovvnewFAHGkRuaW7AUD4XU+RqdD2PyhljgEOfvY/XoOZB+cI9T/+ZAls2k8IQA=="}, "yaxis": "y", "type": "scatter"}, {"customdata": {"dtype": "i1", "bdata": "BAQEBAQEBAQ=", "shape": "8, 1"}, "hovertemplate": "tópico=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "4", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "4", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "AAAAAAAAAAATYk4TYk6zPy5/O8fHSrs/nYHTmB/LqT8AAAAAAAAAAClzDHDwc7M/CgP2acj/0j+rM1Ixtzr5Pw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": {"dtype": "i1", "bdata": "BQUFBQUFBQU=", "shape": "8, 1"}, "hovertemplate": "tópico=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "5", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "5", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "qYilIpaKAEBWLrhVLrgFQFZX4FHCLAZAL1M7NCvxAkCcmIhE4wX5P43zjw+M7Pg/mwIc7fRI4D8MB9Hju3D8Pw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": {"dtype": "i1", "bdata": "BgYGBgYGBgY=", "shape": "8, 1"}, "hovertemplate": "tópico=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "6", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "6", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "NOHPhD8T3j9WLrhVLrjlP0kPVM5u4ec/0PHti4ME3T87/O+7niLjP1keEZpqv9o/eQPQ5pu2tT9VzK3OSMXoPw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": {"dtype": "i1", "bdata": "BwcHBwcHBwc=", "shape": "8, 1"}, "hovertemplate": "tópico=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "7", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "7", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "9oDZA2YPuD8AAAAAAAAAAAAAAAAAAAAAnYHTmB/LuT+EcWIiEo2nPylzDHDwc8M/0wKJq16k4T+ZW52RirnzPw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": {"dtype": "i1", "bdata": "CAgICAgICAg=", "shape": "8, 1"}, "hovertemplate": "tópico=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "8", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "8", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "uSDjgowL4j9WLrhVLrjlPy5/O8fHSqs/NqGesldYwz+EcWIiEo3HPyM7FLZmnN8/AAAAAAAAAADwwkH0+C7kPw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": {"dtype": "i1", "bdata": "CQkJCQkJCQk=", "shape": "8, 1"}, "hovertemplate": "tópico=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "9", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "9", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "NOHPhD8T/j9fX19fX1/vP3Dn+Fhpw+I/nYHTmB/L6T8j1cmZzanhP1keEZpqv9o/AAAAAAAAAACUJbBpP1niPw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": {"dtype": "i1", "bdata": "CgoKCgoKCgo=", "shape": "8, 1"}, "hovertemplate": "tópico=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "10", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "10", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "uSDjgowL4j/RleTQleTgP2nDMpe/nfM/0PHti4ME3T+1v65mtH7qP/erC2mxPOI/0wKJq16k4T/mVmekYm7xPw=="}, "yaxis": "y", "type": "scatter"}], "layout": {"legend": {"title": {"text": "tópico"}, "tracegroupgap": 0}, "xaxis": {"anchor": "y", "domain": [0.0, 1.0], "title": {"text": "ano"}}, "yaxis": {"anchor": "x", "domain": [0.0, 1.0], "title": {"text": "ocorrências (%)"}}}}
</code></pre> <p>Os tópicos abaixo estão listados na ordem em que tiveram mais artigos publicados no ano passado.</p> <details> <summary><b>Tópico 1 - Instruction-Tuned Multimodal LLMs for Vision-Language Understanding (157 documents)</b></summary> <figure> <picture> <img src="/assets/img/blog/2025-03-26-cvpr-history-analysis/topic_23.png" class="img-fluid rounded z-depth-1 mx-auto d-block" width="100%" height="auto" alt="Nuvem de palavras para o tópico 1" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> Recent advancements in large language models (LLMs) and multimodal large language models (MLLMs) have led to remarkable capabilities in integrating visual and textual information for tasks like question answering, dialogue, and reasoning. By leveraging instruction tuning and visual prompt techniques, these models — such as vision-language models (VLMs) like CLIP — have significantly improved in-context comprehension and instruction following across modalities. Despite these achievements, challenges like hallucinations and limited applicability in complex, real-world settings still remain. Research continues to focus on enhancing multimodal instruction learning to facilitate deeper, more reliable understanding between vision and language inputs. <br> <br> Principais autores: <ol> <li>Yu Qiao (7 artigos)</li> <li>Ying Shan (5 artigos)</li> <li>Yixiao Ge (5 artigos)</li> </ol> Exemplos de artigos: <ul> <li><a href="https://openaccess.thecvf.com/content/CVPR2024W/MMFM/html/Wang_LLM-Seg_Bridging_Image_Segmentation_and_Large_Language_Model_Reasoning_CVPRW_2024_paper.html" rel="external nofollow noopener" target="_blank">LLM-Seg: Bridging Image Segmentation and Large Language Model Reasoning (2024)</a></li> <li><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Guan_HallusionBench_An_Advanced_Diagnostic_Suite_for_Entangled_Language_Hallucination_and_CVPR_2024_paper.html" rel="external nofollow noopener" target="_blank">HallusionBench: An Advanced Diagnostic Suite for Entangled Language Hallucination and Visual Illusion in Large Vision-Language Models (2024)</a></li> <li><a href="https://openaccess.thecvf.com/content/CVPR2024W/CVsports/html/Nonaka_Rugby_Scene_Classification_Enhanced_by_Vision_Language_Model_CVPRW_2024_paper.html" rel="external nofollow noopener" target="_blank">Rugby Scene Classification Enhanced by Vision Language Model (2024)</a></li> </ul> <hr> </details> <details> <summary><b>Tópico 2 - Controllable Text-Guided Image Editing with Diffusion and GAN Inversion (426 documents)</b></summary> <figure> <picture> <img src="/assets/img/blog/2025-03-26-cvpr-history-analysis/topic_3.png" class="img-fluid rounded z-depth-1 mx-auto d-block" width="100%" height="auto" alt="Nuvem de palavras para o tópico 2" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> Advances in text-to-image diffusion models and GAN-based techniques have unlocked powerful, controllable image editing capabilities driven by natural language prompts. Methods like StyleGAN inversion, DDIM inversion, and textual inversion allow users to manipulate generated images or real inputs with high fidelity while preserving key features like identity. Text-guided editing leverages the latent space of pretrained diffusion and GAN models, enabling creative, precise, and personalized edits through simple prompts. Despite remarkable progress, achieving fine-grained control over complex edits and extending these capabilities to video editing remain active research challenges. <br> <br> Principais autores: <ol> <li>Chen Change Loy (9 artigos)</li> <li>Xintao Wang (8 artigos)</li> <li>Ying Shan (8 artigos)</li> </ol> Exemplos de artigos: <ul> <li><a href="https://openaccess.thecvf.com/content/CVPR2024W/GCV/html/Bodur_iEdit_Localised_Text-guided_Image_Editing_with_Weak_Supervision_CVPRW_2024_paper.html" rel="external nofollow noopener" target="_blank">iEdit: Localised Text-guided Image Editing with Weak Supervision (2024)</a></li> <li><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Guo_MoMask_Generative_Masked_Modeling_of_3D_Human_Motions_CVPR_2024_paper.html" rel="external nofollow noopener" target="_blank">MoMask: Generative Masked Modeling of 3D Human Motions (2024)</a></li> <li><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Nam_Contrastive_Denoising_Score_for_Text-guided_Latent_Diffusion_Image_Editing_CVPR_2024_paper.html" rel="external nofollow noopener" target="_blank">Contrastive Denoising Score for Text-guided Latent Diffusion Image Editing (2024)</a></li> </ul> <hr> </details> <details> <summary><b>Tópico 3 - AI-Driven Medical Imaging and Diagnosis in Clinical Practice (345 documents)</b></summary> <figure> <picture> <img src="/assets/img/blog/2025-03-26-cvpr-history-analysis/topic_5.png" class="img-fluid rounded z-depth-1 mx-auto d-block" width="100%" height="auto" alt="Nuvem de palavras para o tópico 3" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> The integration of computational methods into medical imaging and pathology has become increasingly important for improving diagnostic accuracy and early disease detection. Techniques like computer-aided diagnosis support clinicians in analyzing tissues, tumors, and organs across modalities such as histopathology, MRI, CT, and digital microscopy. Applications span cancer diagnosis (e.g., breast, brain, skin lesions), neurological diseases like Alzheimer's and Parkinson's, and blood analysis. By enhancing image analysis and tissue classification, these AI-driven tools aid in treatment planning and disease progression monitoring, making them vital in modern clinical practice and biomedical research. <br> <br> Principais autores: <ol> <li>Le Lu (9 artigos)</li> <li>Faisal Mahmood (5 artigos)</li> <li>Ke Yan (5 artigos)</li> </ol> Exemplos de artigos: <ul> <li><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Xiang_SQUID_Deep_Feature_In-Painting_for_Unsupervised_Anomaly_Detection_CVPR_2023_paper.html" rel="external nofollow noopener" target="_blank">SQUID: Deep Feature In-Painting for Unsupervised Anomaly Detection (2023)</a></li> <li><a href="https://openaccess.thecvf.com/content_cvpr_2017_workshops/w3/html/Mery_A_Logarithmic_X-Ray_CVPR_2017_paper.html" rel="external nofollow noopener" target="_blank">A Logarithmic X-Ray Imaging Model for Baggage Inspection: Simulation and Object Detection (2017)</a></li> <li><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Haghighi_DiRA_Discriminative_Restorative_and_Adversarial_Learning_for_Self-Supervised_Medical_Image_CVPR_2022_paper.html" rel="external nofollow noopener" target="_blank">DiRA: Discriminative, Restorative, and Adversarial Learning for Self-Supervised Medical Image Analysis (2022)</a></li> </ul> <hr> </details> <details> <summary><b>Tópico 4 - Challenges and Advances in 3D-Aware Text-to-Image and Text-to-Video Generation (68 documents)</b></summary> <figure> <picture> <img src="/assets/img/blog/2025-03-26-cvpr-history-analysis/topic_95.png" class="img-fluid rounded z-depth-1 mx-auto d-block" width="100%" height="auto" alt="Nuvem de palavras para o tópico 4" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> Text-to-image and text-to-video generation using diffusion models has made remarkable progress, enabling the synthesis of high-fidelity, photorealistic assets from simple prompts. However, existing methods still struggle with accurately handling 3D geometry, novel views, and maintaining global and multi-view consistency. Techniques like diffusion priors, 3D Gaussians, and NeRF-based approaches aim to improve subject-driven generation and diverse, globally consistent outputs. Despite advances in pretrained diffusion models and anisotropic diffusion strategies, achieving high-fidelity, geometry-aware synthesis remains a central challenge in the evolution of text-to-3D and motion generation. <br> <br> Principais autores: <ol> <li>Hsin-Ying Lee (4 artigos)</li> <li>Sergey Tulyakov (4 artigos)</li> <li>Ying Shan (4 artigos)</li> </ol> Exemplos de artigos: <ul> <li><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Kim_Enhancing_3D_Fidelity_of_Text-to-3D_using_Cross-View_Correspondences_CVPR_2024_paper.html" rel="external nofollow noopener" target="_blank">Enhancing 3D Fidelity of Text-to-3D using Cross-View Correspondences (2024)</a></li> <li><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Liu_DIRECT-3D_Learning_Direct_Text-to-3D_Generation_on_Massive_Noisy_3D_Data_CVPR_2024_paper.html" rel="external nofollow noopener" target="_blank">DIRECT-3D: Learning Direct Text-to-3D Generation on Massive Noisy 3D Data (2024)</a></li> <li><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Yi_Diffusion_Time-step_Curriculum_for_One_Image_to_3D_Generation_CVPR_2024_paper.html" rel="external nofollow noopener" target="_blank">Diffusion Time-step Curriculum for One Image to 3D Generation (2024)</a></li> </ul> <hr> </details> <details> <summary><b>Tópico 5 - Remote Sensing and Aerial Imagery for Environmental and Agricultural Monitoring (306 documents)</b></summary> <figure> <picture> <img src="/assets/img/blog/2025-03-26-cvpr-history-analysis/topic_10.png" class="img-fluid rounded z-depth-1 mx-auto d-block" width="100%" height="auto" alt="Nuvem de palavras para o tópico 5" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> The rapid development of satellite and unmanned aerial vehicle (UAV) technologies has fueled increased interest in using high-resolution imagery for environmental, agricultural, and urban management. Remote sensing enables the monitoring of plant species, crop types, water resources, land cover changes, and urban infrastructure such as roads and buildings, particularly aiding developing countries. Applications range from crop management and plant phenotyping to traffic management and tracking environmental factors and changes. Publicly accessible satellite and aerial datasets are becoming vital tools for tackling global challenges in resource management, environmental protection, and urban planning. <br> <br> Principais autores: <ol> <li>Sara Beery (5 artigos)</li> <li>David Lobell (4 artigos)</li> <li>Edward J. Delp (4 artigos)</li> </ol> Exemplos de artigos: <ul> <li><a href="https://openaccess.thecvf.com/content_cvpr_2018/html/Mahjourian_Unsupervised_Learning_of_CVPR_2018_paper.html" rel="external nofollow noopener" target="_blank">Unsupervised Learning of Depth and Ego-Motion From Monocular Video Using 3D Geometric Constraints (2018)</a></li> <li><a href="https://openaccess.thecvf.com/content_cvpr_2018_workshops/w4/html/Hamaguchi_Building_Detection_From_CVPR_2018_paper.html" rel="external nofollow noopener" target="_blank">Building Detection From Satellite Imagery Using Ensemble of Size-Specific Detectors (2018)</a></li> <li><a href="https://openaccess.thecvf.com/content/CVPR2021W/AI4Space/html/Oestreich_On-Orbit_Inspection_of_an_Unknown_Tumbling_Target_Using_NASAs_Astrobee_CVPRW_2021_paper.html" rel="external nofollow noopener" target="_blank">On-Orbit Inspection of an Unknown, Tumbling Target Using NASA's Astrobee Robotic Free-Flyers (2021)</a></li> </ul> <hr> </details> <details> <summary><b>Tópico 6 - Competitions and Challenges in Computer Vision: The Role of NTIRE and Beyond (90 documents)</b></summary> <figure> <picture> <img src="/assets/img/blog/2025-03-26-cvpr-history-analysis/topic_66.png" class="img-fluid rounded z-depth-1 mx-auto d-block" width="100%" height="auto" alt="Nuvem de palavras para o tópico 6" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> Large-scale competitions like the NTIRE Challenge, MegaFace Challenge, and ABAW Competition have become central to advancing computer vision research. Hosted at major conferences like CVPR, these challenges attract hundreds of registered participants and teams, competing across various tracks such as perceptual quality, AI-generated content, and traffic analysis. Through rigorous submissions and evaluations on standardized test sets, these challenges foster innovation, benchmark progress, and tackle formidable problems in the field. The NTIRE Workshop, in particular, has established itself as a premier platform for recognizing outstanding achievements and setting new frontiers in computer vision competitions. <br> <br> Principais autores: <ol> <li>Radu Timofte (44 artigos)</li> <li>Marcos V. Conde (8 artigos)</li> <li>Radu Timofte (7 artigos)</li> </ol> Exemplos de artigos: <ul> <li><a href="https://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Perez-Pellitero_NTIRE_2022_Challenge_on_High_Dynamic_Range_Imaging_Methods_and_CVPRW_2022_paper.html" rel="external nofollow noopener" target="_blank">NTIRE 2022 Challenge on High Dynamic Range Imaging: Methods and Results (2022)</a></li> <li><a href="https://openaccess.thecvf.com/content_cvpr_2017_workshops/w12/html/Bae_Beyond_Deep_Residual_CVPR_2017_paper.html" rel="external nofollow noopener" target="_blank">Beyond Deep Residual Learning for Image Restoration: Persistent Homology-Guided Manifold Simplification (2017)</a></li> <li><a href="https://openaccess.thecvf.com/content/CVPR2024W/NTIRE/html/Vasluianu_NTIRE_2024_Image_Shadow_Removal_Challenge_Report_CVPRW_2024_paper.html" rel="external nofollow noopener" target="_blank">NTIRE 2024 Image Shadow Removal Challenge Report (2024)</a></li> </ul> <hr> </details> <details> <summary><b>Tópico 7 - Enhancing Diffusion Models: Faster Inference and Higher Image Quality (64 documents)</b></summary> <figure> <picture> <img src="/assets/img/blog/2025-03-26-cvpr-history-analysis/topic_103.png" class="img-fluid rounded z-depth-1 mx-auto d-block" width="100%" height="auto" alt="Nuvem de palavras para o tópico 7" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> Diffusion models have emerged as powerful tools for generating high-quality images, particularly in text-to-image tasks, but they often suffer from slow inference speeds and inherent limitations tied to their timestep-based denoising process. Recent advances focus on accelerating inference and improving FID scores through innovations like post-training quality enhancement, tailored token mixing (e.g., super tokens, OCR tokens), and anisotropic diffusion strategies. These methods can be flexibly applied with negligible computational overhead, substantially improving image quality without retraining. By addressing inherent inefficiencies and showcasing superior performance, these techniques represent a major step forward in diffusion-based image generation. <br> <br> Principais autores: <ol> <li>Deli Zhao (3 artigos)</li> <li>Yujun Shen (3 artigos)</li> <li>Chengyue Gong (2 artigos)</li> </ol> Exemplos de artigos: <ul> <li><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Liu_FlowGrad_Controlling_the_Output_of_Generative_ODEs_With_Gradients_CVPR_2023_paper.html" rel="external nofollow noopener" target="_blank">FlowGrad: Controlling the Output of Generative ODEs With Gradients (2023)</a></li> <li><a href="https://openaccess.thecvf.com/content_CVPRW_2020/html/w1/B.S._Plug-and-Pipeline_Efficient_Regularization_for_Single-Step_Adversarial_Training_CVPRW_2020_paper.html" rel="external nofollow noopener" target="_blank">Plug-and-Pipeline: Efficient Regularization for Single-Step Adversarial Training (2020)</a></li> <li><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Karpikova_FIANCEE_Faster_Inference_of_Adversarial_Networks_via_Conditional_Early_Exits_CVPR_2023_paper.html" rel="external nofollow noopener" target="_blank">FIANCEE: Faster Inference of Adversarial Networks via Conditional Early Exits (2023)</a></li> </ul> <hr> </details> <details> <summary><b>Tópico 8 - Intelligent Traffic Monitoring and Driver Behavior Analysis for Road Safety (58 documents)</b></summary> <figure> <picture> <img src="/assets/img/blog/2025-03-26-cvpr-history-analysis/topic_107.png" class="img-fluid rounded z-depth-1 mx-auto d-block" width="100%" height="auto" alt="Nuvem de palavras para o tópico 8" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> Advances in intelligent transportation systems are increasingly focused on improving road safety through the analysis of driver behavior, traffic scenarios, and vehicle-pedestrian interactions. By leveraging traffic monitoring, naturalistic driving datasets, and leaderboard-driven challenges, researchers aim to develop better driver assistance systems and accident prevention technologies. Areas like distracted driver detection, traffic surveillance, and automated driving benefit from smart monitoring systems that enhance safe driving practices and reduce traffic accidents. As public leaderboards rank the best published methods, innovations in vehicle tracking, intelligent traffic analysis, and safe transportation continue to accelerate progress toward safer roads. <br> <br> Principais autores: <ol> <li>Armstrong Aboah (3 artigos)</li> <li>Fei Su (3 artigos)</li> <li>Zhe Cui (3 artigos)</li> </ol> Exemplos de artigos: <ul> <li><a href="https://openaccess.thecvf.com/content/CVPR2024W/AICity/html/Chen_An_Effective_Method_for_Detecting_Violation_of_Helmet_Rule_for_CVPRW_2024_paper.html" rel="external nofollow noopener" target="_blank">An Effective Method for Detecting Violation of Helmet Rule for Motorcyclists (2024)</a></li> <li><a href="https://openaccess.thecvf.com/content/CVPR2022W/AICity/html/Liang_Stargazer_A_Transformer-Based_Driver_Action_Detection_System_for_Intelligent_Transportation_CVPRW_2022_paper.html" rel="external nofollow noopener" target="_blank">Stargazer: A Transformer-Based Driver Action Detection System for Intelligent Transportation (2022)</a></li> <li><a href="https://openaccess.thecvf.com/content_cvpr_2017_workshops/w4/html/Reddy_Real-Time_Driver_Drowsiness_CVPR_2017_paper.html" rel="external nofollow noopener" target="_blank">Real-Time Driver Drowsiness Detection for Embedded System Using Model Compression of Deep Neural Networks (2017)</a></li> </ul> <hr> </details> <details> <summary><b>Tópico 9 - Soccer and Sports Video Analytics: Player Tracking and Game Understanding (103 documents)</b></summary> <figure> <picture> <img src="/assets/img/blog/2025-03-26-cvpr-history-analysis/topic_54.png" class="img-fluid rounded z-depth-1 mx-auto d-block" width="100%" height="auto" alt="Nuvem de palavras para o tópico 9" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> Advances in sports video analytics, particularly for soccer, focus on tracking players, analyzing game states, and generating highlights from broadcast footage and sport-specific datasets. Systems capable of detecting player positions, ball movements, and team dynamics have become fundamental tools for both game analysis and automated content production. Publicly released datasets like UCF and HMDB, along with open-source code, drive innovation in this field, enabling teams around the world to develop systems capable of capturing, processing, and understanding complex sport scenarios. These developments are reshaping sports analytics, enhancing performance evaluation, and enriching fan experiences. <br> <br> Principais autores: <ol> <li>Anthony Cioppa (11 artigos)</li> <li>Marc Van Droogenbroeck (10 artigos)</li> <li>Bernard Ghanem (8 artigos)</li> </ol> Exemplos de artigos: <ul> <li><a href="https://openaccess.thecvf.com/content_CVPRW_2020/html/w45/Ramaswamy_Spatio-Temporal_Action_Detection_and_Localization_Using_a_Hierarchical_LSTM_CVPRW_2020_paper.html" rel="external nofollow noopener" target="_blank">Spatio-Temporal Action Detection and Localization Using a Hierarchical LSTM (2020)</a></li> <li><a href="https://openaccess.thecvf.com/content_cvpr_2018_workshops/w34/html/Giancola_SoccerNet_A_Scalable_CVPR_2018_paper.html" rel="external nofollow noopener" target="_blank">SoccerNet: A Scalable Dataset for Action Spotting in Soccer Videos (2018)</a></li> <li><a href="https://openaccess.thecvf.com/content_cvpr_2017/html/Hu_Deep_360_Pilot_CVPR_2017_paper.html" rel="external nofollow noopener" target="_blank">Deep 360 Pilot: Learning a Deep Agent for Piloting Through 360deg Sports Videos (2017)</a></li> </ul> <hr> </details> <details> <summary><b>Tópico 10 - Event-Based Vision: High-Speed, Low-Latency Sensing with Neuromorphic Cameras (129 documents)</b></summary> <figure> <picture> <img src="/assets/img/blog/2025-03-26-cvpr-history-analysis/topic_34.png" class="img-fluid rounded z-depth-1 mx-auto d-block" width="100%" height="auto" alt="Nuvem de palavras para o tópico 10" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> Event-based vision, powered by bio-inspired neuromorphic cameras, represents a major shift from traditional frame-based imaging. Unlike conventional sensors, event cameras capture asynchronous changes in brightness with low latency, low power consumption, and exceptional dynamic range, making them ideal for high-speed motion scenarios and environments prone to motion blur. This technology, including event-based vision for video frame interpolation (VFI) and eye tracking, has progressed rapidly, offering advantages in bandwidth efficiency and noise reduction. Applications span robotics, autonomous driving, and high-speed tracking, where conventional frame-based approaches often fall short. <br> <br> Principais autores: <ol> <li>Davide Scaramuzza (11 artigos)</li> <li>Mathias Gehrig (6 artigos)</li> <li>Boxin Shi (5 artigos)</li> </ol> Exemplos de artigos: <ul> <li><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Deng_A_Voxel_Graph_CNN_for_Object_Classification_With_Event_Cameras_CVPR_2022_paper.html" rel="external nofollow noopener" target="_blank">A Voxel Graph CNN for Object Classification With Event Cameras (2022)</a></li> <li><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Sundar_Generalized_Event_Cameras_CVPR_2024_paper.html" rel="external nofollow noopener" target="_blank">Generalized Event Cameras (2024)</a></li> <li><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Yu_EventPS_Real-Time_Photometric_Stereo_Using_an_Event_Camera_CVPR_2024_paper.html" rel="external nofollow noopener" target="_blank">EventPS: Real-Time Photometric Stereo Using an Event Camera (2024)</a></li> </ul> <hr> </details> <h3 id="-10-topics">🧊 10 topics</h3> <pre><code class="language-plotly">{"data": [{"customdata": {"dtype": "i1", "bdata": "AQEBAQEBAQE=", "shape": "8, 1"}, "hovertemplate": "tópico=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "1", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "1", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "9oDZA2YPuD8TYk4TYk7DPy5/O8fHSss/aRG5pbuR1j/N5tSIhffrP1keEZpqv+o/jwTxnqx//D84iB7fhYPgPw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": {"dtype": "i1", "bdata": "AgICAgICAgI=", "shape": "8, 1"}, "hovertemplate": "tópico=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "2", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "2", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "sVTEUhFLAUDyexnyexkCQC5/O8fHSvs/T9krrAn19D+1v65mtH76P8HIDoWtGQdAviIgBBciEUBVzK3OSMUIQA=="}, "yaxis": "y", "type": "scatter"}, {"customdata": {"dtype": "i1", "bdata": "AwMDAwMDAwM=", "shape": "8, 1"}, "hovertemplate": "tópico=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "3", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "3", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "9oDZA2YPuD+Y+iGY+iHYPy5/O8fHSts/0PHti4ME7T+cmIhE4wX5P76sEqjoLf0/IAQXItnI+T9VzK3OSMXoPw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": {"dtype": "i1", "bdata": "BAQEBAQEBAQ=", "shape": "8, 1"}, "hovertemplate": "tópico=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "4", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "4", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "9oDZA2YP6D8+eSo+eSr+P23VFXiUMANAAjGEv/MeAEBiIhKNF2QJQCM7FLZmnP8/44SUPMuI/j89vgsH0ePxPw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": {"dtype": "i1", "bdata": "BQUFBQUFBQU=", "shape": "8, 1"}, "hovertemplate": "tópico=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "5", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "5", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "9oDZA2YPyD+Y+iGY+iHYP2OfbNUVeOQ/AjGEv/Me8D8LrqN3/DDwPyhljgEOfvY/JoMsSX2tA0C1nyyBTfv7Pw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": {"dtype": "i1", "bdata": "BgYGBgYGBgY=", "shape": "8, 1"}, "hovertemplate": "tópico=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "6", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "6", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "77S60+pOB0DhCH/hCH8BQELrjQzFu/g/HGkRuaW78T8j1cmZzanxP/SPD4zsUPg/BITgQiQb+T+61RmpmFvtPw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": {"dtype": "i1", "bdata": "BwcHBwcHBwc=", "shape": "8, 1"}, "hovertemplate": "tópico=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "7", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "7", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "qYilIpaK8D8TYk4TYk7zP2OfbNUVeOQ/0PHti4ME7T/N5tSIhffrP1w6DXcvq+Q/PIRNAY52+j/hIHp8Fw7wPw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": {"dtype": "i1", "bdata": "CAgICAgICAg=", "shape": "8, 1"}, "hovertemplate": "tópico=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "8", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "8", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "uSDjgowL4j/RleTQleTgP3Dn+Fhpw+I/aRG5pbuR5j+1v65mtH7qP/eySqCitwBAmwIc7fRIAED5LhxEj+/2Pw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": {"dtype": "i1", "bdata": "CQkJCQkJCQk=", "shape": "8, 1"}, "hovertemplate": "tópico=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "9", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "9", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "9oDZA2YPyD8TYk4TYk6zPy5/O8fHSrs/NqGesldY0z+1v65mtH7aP76sEqjoLd0/mwIc7fRI8D9eOIge34XbPw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": {"dtype": "i1", "bdata": "CgoKCgoKCgo=", "shape": "8, 1"}, "hovertemplate": "tópico=%{customdata[0]}&lt;br&gt;ano=%{x}&lt;br&gt;ocorrências (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "10", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "10", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "2FBeQ3kNFUAdk/Uck/UMQGnDMpe/nQNAdq1/ohRgB0CKSDRekKX/P7+6kBbLI/o/lYMGxlBk9j+waT9ZApvqPw=="}, "yaxis": "y", "type": "scatter"}], "layout": {"xaxis": {"anchor": "y", "domain": [0.0, 1.0], "title": {"text": "ano"}}, "yaxis": {"anchor": "x", "domain": [0.0, 1.0], "title": {"text": "ocorrências (%)"}}, "legend": {"title": {"text": "tópico"}, "tracegroupgap": 0}}}
</code></pre> <p>Os tópicos a seguir estão listados na ordem em que tiveram a maior diminuição de artigos no último ano.</p> <details> <summary><b>Tópico 1 - Self-Supervised Pretraining: Masked Models and Their Impact on Downstream Vision Tasks (115 documents)</b></summary> <figure> <picture> <img src="/assets/img/blog/2025-03-26-cvpr-history-analysis/topic_44.png" class="img-fluid rounded z-depth-1 mx-auto d-block" width="100%" height="auto" alt="Nuvem de palavras para o tópico 1" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> Self-supervised pretraining has revolutionized machine learning by enabling models to learn from unlabeled data, significantly improving performance on a wide range of downstream vision tasks. Techniques like masked autoencoding and contrastive pretraining have become central to this approach, where a model is trained to predict missing parts of the data, learning rich representations without the need for labeled examples. These methods, including masked token strategies and large-scale pretraining on unlabeled video or image datasets, have shown to outperform traditional supervised pretraining, achieving great success across various applications. The benefits of self-supervised learning, especially in terms of scalability and performance, are now being extensively explored in vision-language pretraining (VLP) and other vision tasks, often surpassing existing supervised methods. <br> <br> Principais autores: <ol> <li>Yu Qiao (9 artigos)</li> <li>Ishan Misra (4 artigos)</li> <li>Ross Girshick (4 artigos)</li> </ol> Exemplos de artigos: <ul> <li><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Liu_Solving_Masked_Jigsaw_Puzzles_with_Diffusion_Vision_Transformers_CVPR_2024_paper.html" rel="external nofollow noopener" target="_blank">Solving Masked Jigsaw Puzzles with Diffusion Vision Transformers (2024)</a></li> <li><a href="https://openaccess.thecvf.com/content_cvpr_2017/html/Pathak_Learning_Features_by_CVPR_2017_paper.html" rel="external nofollow noopener" target="_blank">Learning Features by Watching Objects Move (2017)</a></li> <li><a href="https://openaccess.thecvf.com/content/CVPR2024W/CV4MS/html/Kazimi_Self-Supervised_Learning_with_Generative_Adversarial_Networks_for_Electron_Microscopy_CVPRW_2024_paper.html" rel="external nofollow noopener" target="_blank">Self-Supervised Learning with Generative Adversarial Networks for Electron Microscopy (2024)</a></li> </ul> <hr> </details> <details> <summary><b>Tópico 2 - Vision-Language Models: Aligning Text and Image for Cross-Modal Understanding (432 documents)</b></summary> <figure> <picture> <img src="/assets/img/blog/2025-03-26-cvpr-history-analysis/topic_2.png" class="img-fluid rounded z-depth-1 mx-auto d-block" width="100%" height="auto" alt="Nuvem de palavras para o tópico 2" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> Vision-language models, such as CLIP, leverage large datasets of paired image-text data to learn the alignment between visual content and language. These models enable tasks like image captioning, where a description is generated for an image, and text-based image retrieval, where a sentence or phrase is used to find relevant visual content. Through pretraining on vast collections of images and their corresponding captions, these models achieve zero-shot capabilities, meaning they can generalize to tasks they were not explicitly trained on. Grounding text in visual concepts, such as matching sentences to images or videos, has become a central challenge in creating more sophisticated systems for understanding and generating visual and textual information across diverse domains, including untrimmed videos and spoken language. <br> <br> Principais autores: <ol> <li>Lijuan Wang (9 artigos)</li> <li>Mike Zheng Shou (7 artigos)</li> <li>Ying Shan (7 artigos)</li> </ol> Exemplos de artigos: <ul> <li><a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Chen_Image_Search_With_Text_Feedback_by_Visiolinguistic_Attention_Learning_CVPR_2020_paper.html" rel="external nofollow noopener" target="_blank">Image Search With Text Feedback by Visiolinguistic Attention Learning (2020)</a></li> <li><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Gao_ULIP_Learning_a_Unified_Representation_of_Language_Images_and_Point_CVPR_2023_paper.html" rel="external nofollow noopener" target="_blank">ULIP: Learning a Unified Representation of Language, Images, and Point Clouds for 3D Understanding (2023)</a></li> <li><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Vasu_MobileCLIP_Fast_Image-Text_Models_through_Multi-Modal_Reinforced_Training_CVPR_2024_paper.html" rel="external nofollow noopener" target="_blank">MobileCLIP: Fast Image-Text Models through Multi-Modal Reinforced Training (2024)</a></li> </ul> <hr> </details> <details> <summary><b>Tópico 3 - Semi-Supervised Learning: Leveraging Unlabeled Data for Improved Model Performance (179 documents)</b></summary> <figure> <picture> <img src="/assets/img/blog/2025-03-26-cvpr-history-analysis/topic_19.png" class="img-fluid rounded z-depth-1 mx-auto d-block" width="100%" height="auto" alt="Nuvem de palavras para o tópico 3" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> Semi-supervised learning (SSL) is a powerful technique that utilizes both labeled and unlabeled data to train models, especially when labeled data is scarce. In SSL, pseudo-labeling is commonly employed, where unlabeled examples are assigned pseudo-labels based on model predictions, and these pseudo-labeled data are incorporated into the training process. This approach allows models to learn from a large amount of unlabeled data, improving generalization without requiring extensive labeled datasets. Methods like pseudo label refinement and self-training help ensure the quality and reliability of the pseudo-labels, making SSL effective for tasks like medical image analysis, where labeled data is limited. By combining labeled data with confident pseudo-labels from unlabeled examples, semi-supervised learning can outperform traditional fully supervised methods, particularly in challenging settings with partially labeled data. <br> <br> Principais autores: <ol> <li>Jingdong Wang (4 artigos)</li> <li>Lei Qi (4 artigos)</li> <li>Yinghuan Shi (4 artigos)</li> </ol> Exemplos de artigos: <ul> <li><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Zhang_Decoupled_Pseudo-labeling_for_Semi-Supervised_Monocular_3D_Object_Detection_CVPR_2024_paper.html" rel="external nofollow noopener" target="_blank">Decoupled Pseudo-labeling for Semi-Supervised Monocular 3D Object Detection (2024)</a></li> <li><a href="https://openaccess.thecvf.com/content_CVPRW_2020/html/w45/Rebuffi_Semi-Supervised_Learning_With_Scarce_Annotations_CVPRW_2020_paper.html" rel="external nofollow noopener" target="_blank">Semi-Supervised Learning With Scarce Annotations (2020)</a></li> <li><a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Yang_Phase_Consistent_Ecological_Domain_Adaptation_CVPR_2020_paper.html" rel="external nofollow noopener" target="_blank">Phase Consistent Ecological Domain Adaptation (2020)</a></li> </ul> <hr> </details> <details> <summary><b>Tópico 4 - Domain Adaptation: Bridging the Gap Between Source and Target Domains (323 documents)</b></summary> <figure> <picture> <img src="/assets/img/blog/2025-03-26-cvpr-history-analysis/topic_6.png" class="img-fluid rounded z-depth-1 mx-auto d-block" width="100%" height="auto" alt="Nuvem de palavras para o tópico 4" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> Domain adaptation (DA) focuses on adapting models trained on a labeled source domain to perform well on an unseen target domain, addressing the challenges posed by domain shift or domain gap. This process is crucial when the source and target domains differ significantly, such as in cross-domain generalization tasks. Techniques like pseudo-labeling, self-training, and few-shot learning are employed to improve performance on target data, even when labeled data from the target domain is limited or unavailable. Domain adaptation methods aim to reduce the discrepancy between the source and target domains by minimizing the impact of domain shift and leveraging unlabeled target samples. These methods are vital for applications like visual domain adaptation, where new target domains with varying conditions or classes are frequently encountered. <br> <br> Principais autores: <ol> <li>Luc Van Gool (8 artigos)</li> <li>Dengxin Dai (7 artigos)</li> <li>Wen Li (7 artigos)</li> </ol> Exemplos de artigos: <ul> <li><a href="https://openaccess.thecvf.com/content/CVPR2021W/WiCV/html/Thota_Contrastive_Domain_Adaptation_CVPRW_2021_paper.html" rel="external nofollow noopener" target="_blank">Contrastive Domain Adaptation (2021)</a></li> <li><a href="https://openaccess.thecvf.com/content/CVPR2021W/LLID/html/Cai_DAMSL_Domain_Agnostic_Meta_Score-Based_Learning_CVPRW_2021_paper.html" rel="external nofollow noopener" target="_blank">DAMSL: Domain Agnostic Meta Score-Based Learning (2021)</a></li> <li><a href="https://openaccess.thecvf.com/content/CVPR2021/html/Song_Spatio-temporal_Contrastive_Domain_Adaptation_for_Action_Recognition_CVPR_2021_paper.html" rel="external nofollow noopener" target="_blank">Spatio-temporal Contrastive Domain Adaptation for Action Recognition (2021)</a></li> </ul> <hr> </details> <details> <summary><b>Tópico 5 - 3D Object Detection: Advancements in Lidar and Monocular Approaches for Autonomous Vehicles (217 documents)</b></summary> <figure> <picture> <img src="/assets/img/blog/2025-03-26-cvpr-history-analysis/topic_15.png" class="img-fluid rounded z-depth-1 mx-auto d-block" width="100%" height="auto" alt="Nuvem de palavras para o tópico 5" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> 3D object detection is a critical component of autonomous driving, enabling vehicles to perceive and understand their environment in three dimensions. Using technologies like lidar, monocular cameras, and radar, 3D detection systems create detailed representations of the surroundings, often represented in formats such as birds-eye view (BEV) or voxel grids. Datasets like KITTI, NuScenes, and Waymo provide benchmarks for evaluating 3D detection models, with lidar-based point clouds playing a central role in high-accuracy detection of objects, such as pedestrians, vehicles, and obstacles. These systems face challenges such as slow inference speeds and the complexity of predicting occupancy grids, but advancements in lidar sensors, occupancy prediction, and BEV detectors are helping improve autonomous vehicle perception and safety. As autonomous driving systems evolve, 3D detection continues to be crucial for precise navigation and decision-making. <br> <br> Principais autores: <ol> <li>Jie Zhou (7 artigos)</li> <li>Jiwen Lu (6 artigos)</li> <li>Yuexin Ma (6 artigos)</li> </ol> Exemplos de artigos: <ul> <li><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Li_GAFusion_Adaptive_Fusing_LiDAR_and_Camera_with_Multiple_Guidance_for_CVPR_2024_paper.html" rel="external nofollow noopener" target="_blank">GAFusion: Adaptive Fusing LiDAR and Camera with Multiple Guidance for 3D Object Detection (2024)</a></li> <li><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Tang_SparseOcc_Rethinking_Sparse_Latent_Representation_for_Vision-Based_Semantic_Occupancy_Prediction_CVPR_2024_paper.html" rel="external nofollow noopener" target="_blank">SparseOcc: Rethinking Sparse Latent Representation for Vision-Based Semantic Occupancy Prediction (2024)</a></li> <li><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Yin_IS-Fusion_Instance-Scene_Collaborative_Fusion_for_Multimodal_3D_Object_Detection_CVPR_2024_paper.html" rel="external nofollow noopener" target="_blank">IS-Fusion: Instance-Scene Collaborative Fusion for Multimodal 3D Object Detection (2024)</a></li> </ul> <hr> </details> <details> <summary><b>Tópico 6 - Weakly Supervised Object Segmentation: Balancing Annotations and Performance (244 documents)</b></summary> <figure> <picture> <img src="/assets/img/blog/2025-03-26-cvpr-history-analysis/topic_13.png" class="img-fluid rounded z-depth-1 mx-auto d-block" width="100%" height="auto" alt="Nuvem de palavras para o tópico 6" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> Weakly supervised object segmentation focuses on leveraging less detailed annotations, such as image-level labels or object proposals, to train segmentation models. Unlike fully supervised methods that require pixel-level annotations, weak supervision relies on class-level or bounding box labels to guide the segmentation process. Datasets like Pascal VOC and MS COCO provide benchmarks for evaluating segmentation models, with metrics such as mean Intersection over Union (mIoU) used to assess performance. Techniques like class-agnostic object masks, discriminative region mapping (CAM), and pseudo-masks are employed to generate pixel-level segmentations from weak annotations. This approach aims to reduce the cost and effort associated with obtaining high-quality, pixel-wise annotations, while still achieving competitive segmentation results, especially for complex tasks like instance-level segmentation and object part identification. <br> <br> Principais autores: <ol> <li>Junwei Han (5 artigos)</li> <li>Yunchao Wei (5 artigos)</li> <li>Bingfeng Zhang (4 artigos)</li> </ol> Exemplos de artigos: <ul> <li><a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Cheng_CascadePSP_Toward_Class-Agnostic_and_Very_High-Resolution_Segmentation_via_Global_and_CVPR_2020_paper.html" rel="external nofollow noopener" target="_blank">CascadePSP: Toward Class-Agnostic and Very High-Resolution Segmentation via Global and Local Refinement (2020)</a></li> <li><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Huynh_SimpSON_Simplifying_Photo_Cleanup_With_Single-Click_Distracting_Object_Segmentation_Network_CVPR_2023_paper.html" rel="external nofollow noopener" target="_blank">SimpSON: Simplifying Photo Cleanup With Single-Click Distracting Object Segmentation Network (2023)</a></li> <li><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Kang_Distilling_Self-Supervised_Vision_Transformers_for_Weakly-Supervised_Few-Shot_Classification__Segmentation_CVPR_2023_paper.html" rel="external nofollow noopener" target="_blank">Distilling Self-Supervised Vision Transformers for Weakly-Supervised Few-Shot Classification &amp; Segmentation (2023)</a></li> </ul> <hr> </details> <details> <summary><b>Tópico 7 - Image Relighting: Enhancing Lighting and Material Effects in Digital Rendering (167 documents)</b></summary> <figure> <picture> <img src="/assets/img/blog/2025-03-26-cvpr-history-analysis/topic_21.png" class="img-fluid rounded z-depth-1 mx-auto d-block" width="100%" height="auto" alt="Nuvem de palavras para o tópico 7" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> Image relighting is a technique used in computer graphics and computational photography to manipulate or simulate changes in lighting conditions on a given scene or object. This process involves adjusting various aspects of lighting, such as specular and diffuse reflections, albedo, and shadow effects, to create realistic or desired lighting outcomes. It takes into account material properties like reflectance, illumination, and surface normals, which determine how light interacts with the scene. Relighting is commonly applied in tasks such as portrait or face relighting, where the goal is to alter lighting without changing the geometry of the scene. By estimating and adjusting lighting effects like specular highlights, shadows, and ambient light, image relighting allows for enhanced visual realism and flexibility in various applications, from film production to virtual environments and interactive systems. <br> <br> Principais autores: <ol> <li>Boxin Shi (11 artigos)</li> <li>Kalyan Sunkavalli (7 artigos)</li> <li>Noah Snavely (6 artigos)</li> </ol> Exemplos de artigos: <ul> <li><a href="https://openaccess.thecvf.com/content_cvpr_2018/html/Mo_Uncalibrated_Photometric_Stereo_CVPR_2018_paper.html" rel="external nofollow noopener" target="_blank">Uncalibrated Photometric Stereo Under Natural Illumination (2018)</a></li> <li><a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Thapa_Dynamic_Fluid_Surface_Reconstruction_Using_Deep_Neural_Network_CVPR_2020_paper.html" rel="external nofollow noopener" target="_blank">Dynamic Fluid Surface Reconstruction Using Deep Neural Network (2020)</a></li> <li><a href="https://openaccess.thecvf.com/content/CVPR2021/html/R_Monocular_Reconstruction_of_Neural_Face_Reflectance_Fields_CVPR_2021_paper.html" rel="external nofollow noopener" target="_blank">Monocular Reconstruction of Neural Face Reflectance Fields (2021)</a></li> </ul> <hr> </details> <details> <summary><b>Tópico 8 - Large Kernel Convolutions and Self-Attention Mechanisms in Vision Transformers (209 documents)</b></summary> <figure> <picture> <img src="/assets/img/blog/2025-03-26-cvpr-history-analysis/topic_16.png" class="img-fluid rounded z-depth-1 mx-auto d-block" width="100%" height="auto" alt="Nuvem de palavras para o tópico 8" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> The integration of large kernel convolutions and self-attention mechanisms has become a powerful approach in modern computer vision tasks. Large kernel convolutions, such as atrous or depthwise convolutions, allow for an increased receptive field, enabling the model to capture long-range dependencies in an image without significantly increasing computational cost. This is essential for vision tasks like object detection and segmentation, where understanding the global context is crucial. On the other hand, self-attention mechanisms, particularly in Vision Transformers (ViTs), facilitate capturing relationships between distant image patches, enhancing the model's ability to focus on relevant parts of an image. By combining large kernel convolutions with self-attention layers, models like the Vision Transformer (ViT) can effectively balance local feature extraction and global context understanding, leading to improved performance on benchmarks like ADE, Cityscapes, and COCO, especially in tasks like segmentation and scene understanding. This combination provides an efficient and scalable solution for handling complex vision tasks while maintaining competitive performance. <br> <br> Principais autores: <ol> <li>Xiangyu Zhang (6 artigos)</li> <li>Chang Xu (5 artigos)</li> <li>Yu Qiao (5 artigos)</li> </ol> Exemplos de artigos: <ul> <li><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Zhang_PatchFormer_An_Efficient_Point_Transformer_With_Patch_Attention_CVPR_2022_paper.html" rel="external nofollow noopener" target="_blank">PatchFormer: An Efficient Point Transformer With Patch Attention (2022)</a></li> <li><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Ding_UniRepLKNet_A_Universal_Perception_Large-Kernel_ConvNet_for_Audio_Video_Point_CVPR_2024_paper.html" rel="external nofollow noopener" target="_blank">UniRepLKNet: A Universal Perception Large-Kernel ConvNet for Audio Video Point Cloud Time-Series and Image Recognition (2024)</a></li> <li><a href="https://openaccess.thecvf.com/content/CVPR2021/html/Ruan_Gaussian_Context_Transformer_CVPR_2021_paper.html" rel="external nofollow noopener" target="_blank">Gaussian Context Transformer (2021)</a></li> </ul> <hr> </details> <details> <summary><b>Tópico 9 - 3D-Aware Image Synthesis with GANs for High-Fidelity and Controllable Rendering (71 documents)</b></summary> <figure> <picture> <img src="/assets/img/blog/2025-03-26-cvpr-history-analysis/topic_91.png" class="img-fluid rounded z-depth-1 mx-auto d-block" width="100%" height="auto" alt="Nuvem de palavras para o tópico 9" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> 3D-aware image synthesis is an advanced technique that combines the power of Generative Adversarial Networks (GANs) with 3D geometry to create highly realistic and controllable images. By incorporating 3D-aware models like Neural Radiance Fields (NeRF) and leveraging latent spaces in GAN architectures such as StyleGAN, this method enables the generation of high-fidelity images from novel views or multi-view perspectives. This approach allows for fine-grained control over attributes like lighting, angles, and details, making it particularly useful for photorealistic rendering and editing. The synthesis process ensures that the generated images maintain consistency across different views and provide high-quality visual outputs, which can be applied in areas such as virtual reality, digital content creation, and computer graphics. With advancements in 3D-aware GANs, it is now possible to synthesize photo-realistic images with impressive fidelity, enabling novel applications in creative industries. <br> <br> Principais autores: <ol> <li>Gordon Wetzstein (4 artigos)</li> <li>Jiajun Wu (4 artigos)</li> <li>Sida Peng (4 artigos)</li> </ol> Exemplos de artigos: <ul> <li><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Xu_DisCoScene_Spatially_Disentangled_Generative_Radiance_Fields_for_Controllable_3D-Aware_Scene_CVPR_2023_paper.html" rel="external nofollow noopener" target="_blank">DisCoScene: Spatially Disentangled Generative Radiance Fields for Controllable 3D-Aware Scene Synthesis (2023)</a></li> <li><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Kim_NeuralField-LDM_Scene_Generation_With_Hierarchical_Latent_Diffusion_Models_CVPR_2023_paper.html" rel="external nofollow noopener" target="_blank">NeuralField-LDM: Scene Generation With Hierarchical Latent Diffusion Models (2023)</a></li> <li><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Li_Lift3D_Synthesize_3D_Training_Data_by_Lifting_2D_GAN_to_CVPR_2023_paper.html" rel="external nofollow noopener" target="_blank">Lift3D: Synthesize 3D Training Data by Lifting 2D GAN to 3D Generative Radiance Field (2023)</a></li> </ul> <hr> </details> <details> <summary><b>Tópico 10 - Efficient Solving of Non-Convex Problems with Outlier Rejection and Relaxation Techniques (356 documents)</b></summary> <figure> <picture> <img src="/assets/img/blog/2025-03-26-cvpr-history-analysis/topic_4.png" class="img-fluid rounded z-depth-1 mx-auto d-block" width="100%" height="auto" alt="Nuvem de palavras para o tópico 10" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> Solving non-convex problems, especially those involving outlier detection, correspondence, and registration, is a complex challenge in fields like computer vision and robotics. Methods like RANSAC (Random Sample Consensus) and polynomial solvers are commonly used to handle these issues, where outliers — incorrect data points — are filtered out to improve the accuracy of the solution. Convex relaxation techniques and non-convex optimization solvers are applied to iteratively refine the solution toward a globally optimal result. For example, pose estimation problems, such as relative rotation or translation, are solved efficiently by leveraging convex optimization and minimal solvers, ensuring that even with noisy or incomplete data, the solution converges to the correct answer. These techniques, including the use of least squares and graph matching, play a key role in ensuring robust performance in real-world applications, where noise and outliers are unavoidable. <br> <br> Principais autores: <ol> <li>Daniel Barath (13 artigos)</li> <li>Daniel Cremers (13 artigos)</li> <li>Viktor Larsson (10 artigos)</li> </ol> Exemplos de artigos: <ul> <li><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Bai_Sliced_Optimal_Partial_Transport_CVPR_2023_paper.html" rel="external nofollow noopener" target="_blank">Sliced Optimal Partial Transport (2023)</a></li> <li><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zhang_3D_Registration_With_Maximal_Cliques_CVPR_2023_paper.html" rel="external nofollow noopener" target="_blank">3D Registration With Maximal Cliques (2023)</a></li> <li><a href="https://openaccess.thecvf.com/content/CVPR2023/html/He_A_Rotation-Translation-Decoupled_Solution_for_Robust_and_Efficient_Visual-Inertial_Initialization_CVPR_2023_paper.html" rel="external nofollow noopener" target="_blank">A Rotation-Translation-Decoupled Solution for Robust and Efficient Visual-Inertial Initialization (2023)</a></li> </ul> <hr> </details> <h2 id="conclusão">Conclusão</h2> <p>Nesta análise, exploramos as tendências e mudanças nos tópicos de pesquisa da comunidade CVPR ao longo dos últimos anos. Os dados revelam um cenário dinâmico, com algumas áreas apresentando crescimento significativo, enquanto outras mostram uma queda de interesse. Isso reflete a natureza evolutiva da pesquisa em inteligência artificial e a busca contínua por inovação e aprimoramento em diversos domínios.</p> <p>Os tópicos mais quentes – que vão desde LLMs multimodais com fine-tuning por instrução para compreensão visão-linguagem até os avanços rápidos na edição orientada por texto, síntese com consciência do 3D e visão baseada em eventos – evidenciam um forte impulso na integração das modalidades, na criação de modelos generativos mais controláveis e na abordagem das crescentes demandas de aplicações reais. Pesquisadores estão cada vez mais empenhados em melhorar a integração entre linguagem e visão para possibilitar um raciocínio mais eficaz, lidar melhor com ambiguidades (como alucinações) e aprimorar o desempenho tanto em contextos criativos quanto em ambientes críticos para a segurança.</p> <p>Em contraste, os tópicos mais frios – tais como o pré-treinamento auto-supervisionado, o alinhamento tradicional visão-linguagem, o aprendizado semi-supervisionado, a adaptação de domínio e até mesmo a detecção clássica de objetos 3D – indicam áreas onde técnicas bem estabelecidas parecem ter atingido um platô. Enquanto esses métodos lançaram as bases para os avanços atuais, sua evolução desacelerou em favor de abordagens mais recentes. Técnicas que outrora foram inovadoras estão sendo revisitadas com o intuito de integrá-las a sistemas mais abrangentes, mas seu apelo isolado diminuiu à medida que a comunidade se volta para soluções end-to-end, multimodais e específicas para cada tarefa.</p> <p>Em conjunto, essas tendências sugerem que o campo está se direcionando para modelos mais holísticos e integrados, que não apenas expandem os limites do que os sistemas automatizados podem gerar ou analisar, mas também proporcionam maior confiabilidade e controle em aplicações do mundo real. À medida que a indústria continua a explorar a fusão de texto, imagem e até dados de sensores, a próxima onda de inovação provavelmente será impulsionada por sistemas que aprendem simultaneamente a partir de múltiplas modalidades, enquanto aproveitam métodos robustos já estabelecidos como alicerce.</p> <p>Essa evolução ressalta a natureza vibrante da pesquisa em inteligência artificial, onde métodos consagrados oferecem uma base sólida, ao mesmo tempo em que as técnicas emergentes prometem remodelar o futuro da inteligência artificial.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12"> Gostou de Ler este Artigo? </h2> <p class="mb-2">Aqui estão alguns artigos relacionados que você pode gostar de ler:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/pt-br/blog/2023/slidev_for_non_web_devs/">sli.dev para desenvolvedores não web</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/pt-br/blog/2023/simple-improvements-python-code/">Melhorando seu código Python com truques simples</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/pt-br/blog/2022/research-code-reproducibility/">O problema da reproducibilidade de códigos de pesquisa</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/pt-br/blog/2022/localized-blog/">Criando postagens de blog traduzidas</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/pt-br/blog/2022/localized-projects/">Criando páginas de projetos traduzidas</a> </li> <div id="giscus_thread" style="max-width: 1000px; margin: 0 auto;"> <script>
      let giscusTheme = determineComputedTheme();
      let giscusAttributes = {
        src: 'https://giscus.app/client.js',
        'data-repo': 'george-gca/george-gca.github.io',
        'data-repo-id': 'R_kgDOI66tbw',
        'data-category': 'Announcements',
        'data-category-id': 'DIC_kwDOI66tb84CUDq6',
        'data-mapping': 'title',
        'data-strict': '1',
        'data-reactions-enabled': '1',
        'data-emit-metadata': '0',
        'data-input-position': 'bottom',
        'data-theme': giscusTheme,
        'data-lang': 'pt',
        crossorigin: 'anonymous',
        async: '',
      };

      let giscusScript = document.createElement('script');
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById('giscus_thread').appendChild(giscusScript);
    </script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> ©Direitos autorais 2025 George C. de Araújo. Desenvolvido em <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> com o tema <a href="https://github.com/george-gca/multi-language-al-folio" rel="external nofollow noopener" target="_blank">multi-language-al-folio</a>. Hospedado por <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/plotly.js@3.0.1/dist/plotly.min.js" integrity="sha256-oy6Be7Eh6eiQFs5M7oXuPxxm9qbJXEtTpfSI93dW16Q=" crossorigin="anonymous"></script> <script defer src="/assets/js/plotly-setup.js?5e81fc889064852664784cb29c0d6970" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?ec82345cb41d6fd19bf9dfe5403e38e4"></script> <script defer src="/assets/js/common.js?dc2245338e496943bdc50ee60dcbf8f5"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-J7KHS011G7"></script> <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() {
      dataLayer.push(arguments);
    }
    gtag('js', new Date());

    gtag('config', 'G-J7KHS011G7');
  </script> <script defer src="/assets/js/google-analytics-setup.js?12374742c4b1801ba82226e617af7e2d"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Digite para iniciar a busca"> <div class="modal-footer" slot="footer"> <span class="help"> <svg version="1.0" class="ninja-examplekey" xmlns="http://www.w3.org/2000/svg" viewbox="0 0 1280 1280"> <path d="M1013 376c0 73.4-.4 113.3-1.1 120.2a159.9 159.9 0 0 1-90.2 127.3c-20 9.6-36.7 14-59.2 15.5-7.1.5-121.9.9-255 1h-242l95.5-95.5 95.5-95.5-38.3-38.2-38.2-38.3-160 160c-88 88-160 160.4-160 161 0 .6 72 73 160 161l160 160 38.2-38.3 38.3-38.2-95.5-95.5-95.5-95.5h251.1c252.9 0 259.8-.1 281.4-3.6 72.1-11.8 136.9-54.1 178.5-116.4 8.6-12.9 22.6-40.5 28-55.4 4.4-12 10.7-36.1 13.1-50.6 1.6-9.6 1.8-21 2.1-132.8l.4-122.2H1013v110z"></path> </svg> para selecionar </span> <span class="help"> <svg xmlns="http://www.w3.org/2000/svg" class="ninja-examplekey" viewbox="0 0 24 24"> <path d="M0 0h24v24H0V0z" fill="none"></path> <path d="M20 12l-1.41-1.41L13 16.17V4h-2v12.17l-5.58-5.59L4 12l8 8 8-8z"></path> </svg> <svg xmlns="http://www.w3.org/2000/svg" class="ninja-examplekey" viewbox="0 0 24 24"> <path d="M0 0h24v24H0V0z" fill="none"></path> <path d="M4 12l1.41 1.41L11 7.83V20h2V7.83l5.58 5.59L20 12l-8-8-8 8z"></path> </svg> para navegar </span> <span class="help"> <span class="ninja-examplekey esc">esc</span> para fechar </span> <span class="help"> <svg xmlns="http://www.w3.org/2000/svg" class="ninja-examplekey backspace" viewbox="0 0 20 20" fill="currentColor"> <path fill-rule="evenodd" d="M6.707 4.879A3 3 0 018.828 4H15a3 3 0 013 3v6a3 3 0 01-3 3H8.828a3 3 0 01-2.12-.879l-4.415-4.414a1 1 0 010-1.414l4.414-4.414zm4 2.414a1 1 0 00-1.414 1.414L10.586 10l-1.293 1.293a1 1 0 101.414 1.414L12 11.414l1.293 1.293a1 1 0 001.414-1.414L13.414 10l1.293-1.293a1 1 0 00-1.414-1.414L12 8.586l-1.293-1.293z" clip-rule="evenodd"></path> </svg> voltar um nível </span> </div> </ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/pt-br/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>