<!DOCTYPE html> <html lang="en-us"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="g-X-aUUud1gtIVUgKEAx9T7VKpcQw1Ymd8aQ-hlIIY8"> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Analyzing the history of CVPR | George C. de Araújo </title> <meta name="author" content="George C. de Araújo"> <meta name="description" content="A look at the history of CVPR and its workshops from 2017 to 2024."> <meta name="keywords" content="machine learning, artificial intelligence, deep learning, computer vision, natural language processing"> <meta property="og:site_name" content="George C. de Araújo"> <meta property="og:type" content="article"> <meta property="og:title" content="George C. de Araújo | Analyzing the history of CVPR"> <meta property="og:url" content="https://george-gca.github.io/blog/2025/cvpr-history-analysis/"> <meta property="og:description" content="A look at the history of CVPR and its workshops from 2017 to 2024."> <meta property="og:image" content="assets/img/profile_pic.png"> <meta property="og:image:alt" content="Profile picture"> <meta property="og:locale" content="en-us"> <meta name="twitter:card" content="summary"> <meta name="twitter:title" content="Analyzing the history of CVPR"> <meta name="twitter:description" content="A look at the history of CVPR and its workshops from 2017 to 2024."> <meta name="twitter:image" content="assets/img/profile_pic.png"> <script type="application/ld+json">
    {"author":{"@type":"Person","name":"George C. de Araújo"},"url":"https://george-gca.github.io/blog/2025/cvpr-history-analysis/","@type":"BlogPosting","description":"A look at the history of CVPR and its workshops from 2017 to 2024.","headline":"Analyzing the history of CVPR","name":"George C. de Araújo","@context":"https://schema.org"}
  </script> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%91%A8%F0%9F%8F%BD%E2%80%8D%F0%9F%92%BB&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://george-gca.github.io/blog/2025/cvpr-history-analysis/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">George</span> C. de Araújo </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap" style="--stagger: 1;" data-animate=""> <li class="nav-item "> <a class="nav-link" href="/">Bio </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item active"> <a class="nav-link" href="/pt-br/blog/2025/cvpr-history-analysis/"> PT-BR</a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link"><i class="ti ti-search"></i> ctrl k</span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main" style="--stagger: 0;" data-animate> <div class="post"> <header class="post-header"> <h1 class="post-title">Analyzing the history of CVPR</h1> <p class="post-meta" style="--stagger: 2;" data-animate=""> Created in April 25, 2025 </p> <p class="post-tags" style="--stagger: 3;" data-animate=""> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/ai"> <i class="fa-solid fa-hashtag fa-sm"></i> ai</a>   <a href="/blog/tag/conference"> <i class="fa-solid fa-hashtag fa-sm"></i> conference</a>   <a href="/blog/tag/analysis"> <i class="fa-solid fa-hashtag fa-sm"></i> analysis</a>   ·   <a href="/blog/category/data-science"> <i class="fa-solid fa-tag fa-sm"></i> data-science</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p style="--stagger: 4;" data-animate="">As the <a href="https://cvpr.thecvf.com/" rel="external nofollow noopener" target="_blank">IEEE/CVF Conference on Computer Vision and Pattern Recognition</a> (CVPR) 2025 approaches, let’s take a look at the history of the conference and its workshops from 2017 to 2024. The goal of this analysis is to provide insights into the evolution of topics and trends in artificial intelligence research over the years. Keep in mind that this information should be taken with a grain of salt, as some of the information that may be relevant to the analyses is discarded during the cleaning process. Some of the analysis is based on keywords, and we make some assumptions about how authors use keywords (e.g. it’s pretty unlikely that a paper about image data would have the keyword <code class="language-plaintext highlighter-rouge">audio</code> in its title or abstract), but this is not a perfect solution. The goal of this post is to give some insight into the history of the conference, not to be a definitive analysis.</p> <p style="--stagger: 5;" data-animate="">Note that some of the graphs use percentiles of the total number of papers published in each year. Since there are different numbers of papers published each year, you can’t really compare the numbers from one year to the next. The goal of these graphs is to show the distribution of papers published during the period and any changes in the focus of the academic community. You can also interact with the visualizations here. You can zoom in on specific parts, enable or disable lines by clicking on their names in the legend, and hover over the points to see more information.</p> <h2 id="overall-statistics" style="--stagger: 6;" data-animate="">Overall Statistics</h2> <p style="--stagger: 7;" data-animate="">Here you can see the number of published papers. Each year, there are more and more papers published compared to the previous year, except for 2023. There were more than three times as many papers published in 2024 as in 2017.</p> <pre><code class="language-plotly">{"data": [{"hovertemplate": "year=%{x}&lt;br&gt;papers=%{y}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "", "orientation": "v", "showlegend": false, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "i2", "bdata": "KAQuBVQHwQd+CEgKNgmgDQ=="}, "yaxis": "y", "type": "scatter"}], "layout": {"xaxis": {"anchor": "y", "domain": [0.0, 1.0], "title": {"text": "year"}}, "yaxis": {"anchor": "x", "domain": [0.0, 1.0], "title": {"text": "papers"}}, "legend": {"tracegroupgap": 0}}}
</code></pre> <p style="--stagger: 8;" data-animate="">Regarding the modalities used in the papers, we can see that image is still the most common, but the use of text and multiple modalities has increased significantly. The application of optical flow, graphs and depth information has decreased in the last years, while the use of particles has remained relatively stable.</p> <pre><code class="language-plotly">{"data": [{"customdata": [["audio"], ["audio"], ["audio"], ["audio"], ["audio"], ["audio"], ["audio"], ["audio"]], "hovertemplate": "modality=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "audio", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "audio", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "2FBeQ3kN5T8dk/Uck/XsP30vhdy8DuE/g0lGn20u6D/N5tSIhffrP/WdjfrORvU/WASE4EIk+z+P78JB9PgAQA=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["depth"], ["depth"], ["depth"], ["depth"], ["depth"], ["depth"], ["depth"], ["depth"]], "hovertemplate": "modality=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "depth", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "depth", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "zB4we8DsE0Ak1egj1egTQER0/3wvhRhAQj1lr7AmFEDHDwNNB98TQMTkCmJyBRFAzGK7c/F4EUDBpv1kCWwPQA=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["graph"], ["graph"], ["graph"], ["graph"], ["graph"], ["graph"], ["graph"], ["graph"]], "hovertemplate": "modality=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "graph", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "graph", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "HX1z9M3RC0A+eSo+eSoOQHumE2xSRRFAg0lGn20uGEDucuibFmAQQL+zUd/ZqAtALkSykbMfCkCWwKb9ZAkDQA=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["image"], ["image"], ["image"], ["image"], ["image"], ["image"], ["image"], ["image"]], "hovertemplate": "modality=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "image", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "image", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "eQ3lNZQXQkBUyixUyixCQAEGofVGhkBAMA4SdHz7PkB3eggk2Ro8QPeySqCiNz1AeiSI92T9O0B8Fw6ixydAQA=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["mesh"], ["mesh"], ["mesh"], ["mesh"], ["mesh"], ["mesh"], ["mesh"], ["mesh"]], "hovertemplate": "modality=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "mesh", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "mesh", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "9oDZA2YP6D/bxovaxovqP0kPVM5u4fc/kOUMnMb8+D9dS0BRmUsBQCZXEJMriPk/XoOZB+cIBUCUJbBpP1kCQA=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["multi modal"], ["multi modal"], ["multi modal"], ["multi modal"], ["multi modal"], ["multi modal"], ["multi modal"], ["multi modal"]], "hovertemplate": "modality=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "multi modal", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "multi modal", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "FbFUxFIRC0A1SIM0SIMEQFJF/XDtmQZAw1Unjyo2DEDlDfuqVnANQCdeT8ocAxhA1CNBvCfrF0DtJ0tg034jQA=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["optical flow"], ["optical flow"], ["optical flow"], ["optical flow"], ["optical flow"], ["optical flow"], ["optical flow"], ["optical flow"]], "hovertemplate": "modality=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "optical flow", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "optical flow", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "yLgg44KMA0A+eSo+eSr+Pw7LXP52jv8/98VBgo5v/z+EcWIiEo33P1osjwhNtfc/JoMsSX2t8z8Vc6szUjHvPw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["particle"], ["particle"], ["particle"], ["particle"], ["particle"], ["particle"], ["particle"], ["particle"]], "hovertemplate": "modality=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "particle", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "particle", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "9oDZA2YPyD8TYk4TYk6zPy5/O8fHSrs/AjGEv/Me0D8j1cmZzanRPylzDHDwc9M/mwIc7fRIwD84iB7fhYPQPw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["path"], ["path"], ["path"], ["path"], ["path"], ["path"], ["path"], ["path"]], "hovertemplate": "modality=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "path", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "path", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "NOHPhD8T7j+Y+iGY+iH4P4RTS55mNABAg0lGn20u+D+cmIhE4wX5P1osjwhNtfc/QgNjKDJb9D9GKuZWZ6T0Pw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["point cloud"], ["point cloud"], ["point cloud"], ["point cloud"], ["point cloud"], ["point cloud"], ["point cloud"], ["point cloud"]], "hovertemplate": "modality=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "point cloud", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "point cloud", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "FbFUxFIR6z8D77MC77MCQFlpwzKXvwVAw1Unjyo2DED9NCHNJ+kOQCVQ0Vs6DQtAA2MoMlvUEkCpmFudkYoIQA=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["text"], ["text"], ["text"], ["text"], ["text"], ["text"], ["text"], ["text"]], "hovertemplate": "modality=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "text", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "text", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "9oDZA2YPGEB3FO12FO0WQD/ZqivwKBlAU4Bd658oFUBUIxbeb5sUQChljgEOfhZAQgNjKDJbFEAJbNpPlsAbQA=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["video"], ["video"], ["video"], ["video"], ["video"], ["video"], ["video"], ["video"]], "hovertemplate": "modality=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "video", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "video", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "NpTXUF5DLkCE5g2E5g0sQMnTjAYkxidAjT7bXDDJKEDsq1rBdfQqQL+zUd/ZqCtANeR/ySBLKkDYtJ8sgc0pQA=="}, "yaxis": "y", "type": "scatter"}], "layout": {"legend": {"title": {"text": "modality"}, "tracegroupgap": 0}, "xaxis": {"anchor": "y", "domain": [0.0, 1.0], "title": {"text": "year"}}, "yaxis": {"anchor": "x", "domain": [0.0, 1.0], "title": {"text": "occurrences (%)"}}}}
</code></pre> <p style="--stagger: 9;" data-animate="">It is quite common for papers to introduce new concepts, be it a new method, a new dataset, or a new architecture. The following graph shows the most common concepts introduced in the papers. Not surprisingly, algorithms are the most common concept. Algorithms also involve new methods or approaches. Novel tasks have also been introduced over the years, which is highly correlated with the creation of novel datasets. The introduction of new architectures has also increased in the last year, including new models, modules, and networks. The creation of different losses and metrics has been quite stable over the years, with very few papers introducing new ones.</p> <pre><code class="language-plotly">{"data": [{"customdata": [["algorithms"], ["algorithms"], ["algorithms"], ["algorithms"], ["algorithms"], ["algorithms"], ["algorithms"], ["algorithms"]], "hovertemplate": "concept=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "algorithms", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "algorithms", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "qYilIpaKAEA+eSo+eSr+P1x7phNsUgVA3Y20iNzS/T9UIxbeb5v0P1osjwhNtfc/GoUB+zTk/z+P78JB9PgQQA=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["architectures"], ["architectures"], ["architectures"], ["architectures"], ["architectures"], ["architectures"], ["architectures"], ["architectures"]], "hovertemplate": "concept=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "architectures", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "architectures", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "2FBeQ3kN5T9fX19fX1/vP4RTS55mNPA/0PHti4ME7T+XwbYIZe3wPylzDHDwc/M//gTLG4A27z8H0eO7cBD7Pw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["datasets"], ["datasets"], ["datasets"], ["datasets"], ["datasets"], ["datasets"], ["datasets"], ["datasets"]], "hovertemplate": "concept=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "datasets", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "datasets", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "5+ibo2+O9j/8rMD7rMD7PyE3r0N0//w/g0lGn20u+D87/O+7niLzP4rXkzLHAP8/44SUPMuI/j9s2k+WwKb/Pw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["losses"], ["losses"], ["losses"], ["losses"], ["losses"], ["losses"], ["losses"], ["losses"]], "hovertemplate": "concept=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "losses", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "losses", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "9oDZA2YPuD8dk/Uck/XcP30vhdy8DtE/aRG5pbuR1j+1v65mtH7aP76sEqjoLc0/WASE4EIkyz9LYNN+sgTGPw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["metrics"], ["metrics"], ["metrics"], ["metrics"], ["metrics"], ["metrics"], ["metrics"], ["metrics"]], "hovertemplate": "concept=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "metrics", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "metrics", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "AAAAAAAAAAATYk4TYk6zPy5/O8fHSqs/nYHTmB/LuT/lDfuqVnDNPwAAAAAAAAAAeQPQ5pu2pT+61RmpmFu9Pw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["tasks"], ["tasks"], ["tasks"], ["tasks"], ["tasks"], ["tasks"], ["tasks"], ["tasks"]], "hovertemplate": "concept=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "tasks", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "tasks", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "9oDZA2YP2D8TYk4TYk7TPxTvIsAgtO4/0PHti4ME7T9sSjwAQRTmP1w6DXcvq/Q/XoOZB+cI9T9GKuZWZ6T0Pw=="}, "yaxis": "y", "type": "scatter"}], "layout": {"xaxis": {"anchor": "y", "domain": [0.0, 1.0], "title": {"text": "year"}}, "yaxis": {"anchor": "x", "domain": [0.0, 1.0], "title": {"text": "occurrences (%)"}}, "legend": {"title": {"text": "concept"}, "tracegroupgap": 0}}}
</code></pre> <p style="--stagger: 10;" data-animate="">Regarding the common tasks in the papers, we can see a steep increase in generation tasks, especially after 2022. This may be related to the advances in large language models such as <a href="https://openreview.net/forum?id=TG8KACxEON" rel="external nofollow noopener" target="_blank">InstructGPT</a> and <a href="https://openai.com/index/chatgpt/" rel="external nofollow noopener" target="_blank">ChatGPT</a> by the end of 2022, and the release of the first collections of foundational language models such as <a href="https://arxiv.org/abs/2302.13971" rel="external nofollow noopener" target="_blank">LLaMA</a> in early 2023. Classification, detection, estimation, and recognition have seen a decline in interest over the years, while prediction has only recently seen a decrease. Tasks such as segmentation have remained relatively stable. The use of reasoning tasks has also increased significantly in the last year, but is still a small percentage of the total number of published papers (about 3%).</p> <pre><code class="language-plotly">{"data": [{"customdata": [["captioning"], ["captioning"], ["captioning"], ["captioning"], ["captioning"], ["captioning"], ["captioning"], ["captioning"]], "hovertemplate": "task=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "captioning", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "captioning", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "JUmSJEmS/D/yexnyexnyP3YLvxoT6fE/T9krrAn15D9UIxbeb5vkP76sEqjoLe0/mwIc7fRI8D/5LhxEj+/2Pw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["classification"], ["classification"], ["classification"], ["classification"], ["classification"], ["classification"], ["classification"], ["classification"]], "hovertemplate": "task=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "classification", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "classification", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "zB4we8DsI0CmpaWlpaUhQPI0owGJcSJAlEcVG646IUCng+85dnYfQL+zUd/ZqBtAzYNzhLq/F0DMrc5IxVwWQA=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["clustering"], ["clustering"], ["clustering"], ["clustering"], ["clustering"], ["clustering"], ["clustering"], ["clustering"]], "hovertemplate": "task=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "clustering", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "clustering", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "qYilIpaKAED8rMD7rMD7P4RTS55mNABAIrd0N9IiAkDRNy3AMI8AQF1Ii+URoQFA/gTLG4A2/z9VzK3OSMX4Pw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["counting"], ["counting"], ["counting"], ["counting"], ["counting"], ["counting"], ["counting"], ["counting"]], "hovertemplate": "task=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "counting", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "counting", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "2FBeQ3kN5T9WLrhVLrjlP1x7phNsUvU/0PHti4ME7T9sSjwAQRTmP44BDn5u4uU/6QOqY29t2D8Vc6szUjHfPw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["detection"], ["detection"], ["detection"], ["detection"], ["detection"], ["detection"], ["detection"], ["detection"]], "hovertemplate": "task=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "detection", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "detection", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "DDIuyLggKkCIh4eHh4cnQMOvxkR6oChAFWDX+idKKUCJhffbJuUlQNuvLqTF8iZAA2MoMlvUIkB/sgQ27WciQA=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["estimation"], ["estimation"], ["estimation"], ["estimation"], ["estimation"], ["estimation"], ["estimation"], ["estimation"]], "hovertemplate": "task=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "estimation", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "estimation", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "vzn65uibIkA/Uo0+Uo0iQBycWvI0ox1AEnR8++IgIUAfhHFiIhIdQCZXEJMriBlAPIRNAY52GkD7yRLYtJ8XQA=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["forecasting"], ["forecasting"], ["forecasting"], ["forecasting"], ["forecasting"], ["forecasting"], ["forecasting"], ["forecasting"]], "hovertemplate": "task=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "forecasting", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "forecasting", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "9oDZA2YP2D8TYk4TYk7TP0kPVM5u4dc/g0lGn20u6D87/O+7niLjP/BzE68nZe4/0wKJq16k4T8Cm/aTJbDpPw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["generation"], ["generation"], ["generation"], ["generation"], ["generation"], ["generation"], ["generation"], ["generation"]], "hovertemplate": "task=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "generation", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "generation", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "+ubom6NvGECEv3CEv3AgQHumE2xSRSFAzgWTjD7bJEBNhbbHUBcnQPSPD4zsUChAtRpffs7rMECuzkjF3Mo2QA=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["identification"], ["identification"], ["identification"], ["identification"], ["identification"], ["identification"], ["identification"], ["identification"]], "hovertemplate": "task=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "identification", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "identification", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "wOwBswfMAkAMIFsMIFsMQDWjAYlxcApAQj1lr7AmBEA7/O+7niIDQPKBkR0KW/s/WASE4EIk6z9LYNN+sgT2Pw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["navigation"], ["navigation"], ["navigation"], ["navigation"], ["navigation"], ["navigation"], ["navigation"], ["navigation"]], "hovertemplate": "task=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "navigation", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "navigation", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "9oDZA2YP6D93FO12FO32P0kPVM5u4fc/trlgktFn6z/HDwNNB9/zPylzDHDwc/M/7oK/ihNS8j+ix3fhIHr2Pw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["prediction"], ["prediction"], ["prediction"], ["prediction"], ["prediction"], ["prediction"], ["prediction"], ["prediction"]], "hovertemplate": "task=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "prediction", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "prediction", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "tbrT6k6rIUAdbFgdbFghQOBRwiz2ySRA6+RRxYarJkA7/O+7niIjQEFMriAmVyZA76N3m9yYKEDDQfT4LpwjQA=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["reasoning"], ["reasoning"], ["reasoning"], ["reasoning"], ["reasoning"], ["reasoning"], ["reasoning"], ["reasoning"]], "hovertemplate": "task=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "reasoning", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "reasoning", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "2FBeQ3kNBUAk1egj1egDQFJF/XDtmQZAfPviIEHHB0DWDv/7rqcIQMPWjPOPDwRAJoMsSX2tA0AJbNpPlsALQA=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["recognition"], ["recognition"], ["recognition"], ["recognition"], ["recognition"], ["recognition"], ["recognition"], ["recognition"]], "hovertemplate": "task=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "recognition", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "recognition", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "2FBeQ3kNJUCmpaWlpaUhQCbSA5WzWxxAumCS0WebG0D4XU+RqdAWQPSPD4zsUBhAQgNjKDJbFEBlCWzaTxYRQA=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["regression"], ["regression"], ["regression"], ["regression"], ["regression"], ["regression"], ["regression"], ["regression"]], "hovertemplate": "task=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "regression", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "regression", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "LBWxVMRSDUAk1egj1egDQHDn+FhpwwJAKQXYtf6JAkAj1cmZzakBQPakzDHAwQNAQgNjKDJb9D+n/WQJbNr3Pw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["retrieval"], ["retrieval"], ["retrieval"], ["retrieval"], ["retrieval"], ["retrieval"], ["retrieval"], ["retrieval"]], "hovertemplate": "task=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "retrieval", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "retrieval", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "DeU1lNdQCkBWLrhVLrgFQFJF/XDtmQZAL1M7NCvxAkAQhXWzekkIQFszzj8+MAZASsTocGjNCkDyXTiIHt8EQA=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["segmentation"], ["segmentation"], ["segmentation"], ["segmentation"], ["segmentation"], ["segmentation"], ["segmentation"], ["segmentation"]], "hovertemplate": "task=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "segmentation", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "segmentation", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "MHvA7AGzHUDRleTQleQgQAOPEmaxTyBAgF3rnygFIECrl4Tzis4dQF5PyhwDHCBAmwIc7fRIIEDPSMXc6swgQA=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["tracking"], ["tracking"], ["tracking"], ["tracking"], ["tracking"], ["tracking"], ["tracking"], ["tracking"]], "hovertemplate": "task=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "tracking", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "tracking", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "0IQ/E/5MFEBJXJdIXJcQQChbdQUeJQxAtrlgktFnC0DucuibFmAQQCVQ0Vs6DQtAQgNjKDJbBEAFNu0nS2AKQA=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["translation"], ["translation"], ["translation"], ["translation"], ["translation"], ["translation"], ["translation"], ["translation"]], "hovertemplate": "task=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "translation", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "translation", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "FbFUxFIR6z8TYk4TYk4DQFJF/XDtmQZAPO8BMYS/A0BUIxbeb5sEQJEWyyNCUwFAlYMGxlBk9j+ZW52RirnzPw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["verification"], ["verification"], ["verification"], ["verification"], ["verification"], ["verification"], ["verification"], ["verification"]], "hovertemplate": "task=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "verification", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "verification", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "FbFUxFIR6z8dk/Uck/XsPxTvIsAgtO4/g0lGn20u6D+EcWIiEo3XP/SPD4zsUNg/eQPQ5pu21T84iB7fhYPQPw=="}, "yaxis": "y", "type": "scatter"}], "layout": {"xaxis": {"anchor": "y", "domain": [0.0, 1.0], "title": {"text": "year"}}, "yaxis": {"anchor": "x", "domain": [0.0, 1.0], "title": {"text": "occurrences (%)"}}, "legend": {"title": {"text": "task"}, "tracegroupgap": 0}}}
</code></pre> <p style="--stagger: 11;" data-animate="">Let’s dive a little deeper into the tasks.</p> <p style="--stagger: 12;" data-animate="">Algorithms focused on <strong>security and privacy</strong> have been around for a while, but the number of papers published on them has increased significantly in the last year. Spoofing detection is crucial for applications such as identity recognition, where attackers may try to use photos or videos to impersonate someone else, and has seemed to gain urgency since deepfake technologies have become more prevalent.</p> <pre><code class="language-plotly">{"data": [{"customdata": [["adversarial attack"], ["adversarial attack"], ["adversarial attack"], ["adversarial attack"], ["adversarial attack"], ["adversarial attack"], ["adversarial attack"], ["adversarial attack"]], "hovertemplate": "task=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "adversarial attack", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "adversarial attack", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "9oDZA2YPuD8dk/Uck/XcP1ZX4FHCLPY/D81KvEztAEBUIxbeb5v0P8HIDoWtGfc/BITgQiQb+T/5LhxEj+/2Pw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["anomaly detection"], ["anomaly detection"], ["anomaly detection"], ["anomaly detection"], ["anomaly detection"], ["anomaly detection"], ["anomaly detection"], ["anomaly detection"]], "hovertemplate": "task=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "anomaly detection", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "anomaly detection", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "9oDZA2YPyD8TYk4TYk7jP3Dn+Fhpw+I/aRG5pbuR5j8j1cmZzanhPylzDHDwc+M/mwIc7fRI4D8Vc6szUjHvPw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["disambiguation"], ["disambiguation"], ["disambiguation"], ["disambiguation"], ["disambiguation"], ["disambiguation"], ["disambiguation"], ["disambiguation"]], "hovertemplate": "task=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "disambiguation", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "disambiguation", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "9oDZA2YPuD8AAAAAAAAAAC5/O8fHSrs/AAAAAAAAAACEcWIiEo2nPylzDHDwc6M/mwIc7fRIwD+UJbBpP1nCPw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["face verification"], ["face verification"], ["face verification"], ["face verification"], ["face verification"], ["face verification"], ["face verification"], ["face verification"]], "hovertemplate": "task=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "face verification", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "face verification", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "9oDZA2YP2D9WLrhVLrjlPy5/O8fHSts/NqGesldYwz+EcWIiEo23P76sEqjoLb0/eQPQ5pu2pT+61RmpmFudPw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["fact checking"], ["fact checking"], ["fact checking"], ["fact checking"], ["fact checking"], ["fact checking"], ["fact checking"], ["fact checking"]], "hovertemplate": "task=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "fact checking", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "fact checking", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAClzDHDwc6M/AAAAAAAAAAAAAAAAAAAAAA=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["forensics"], ["forensics"], ["forensics"], ["forensics"], ["forensics"], ["forensics"], ["forensics"], ["forensics"]], "hovertemplate": "task=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "forensics", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "forensics", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "FbFUxFIR6z+Y+iGY+iHYP2OfbNUVeOQ/T9krrAn15D87/O+7niLjP/SPD4zsUNg/WASE4EIkyz8Cm/aTJbDZPw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["fraud detection"], ["fraud detection"], ["fraud detection"], ["fraud detection"], ["fraud detection"], ["fraud detection"], ["fraud detection"], ["fraud detection"]], "hovertemplate": "task=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "fraud detection", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "fraud detection", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAnYHTmB/LqT8AAAAAAAAAAClzDHDwc7M/eQPQ5pu2pT+61RmpmFu9Pw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["privacy"], ["privacy"], ["privacy"], ["privacy"], ["privacy"], ["privacy"], ["privacy"], ["privacy"]], "hovertemplate": "task=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "privacy", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "privacy", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "9oDZA2YP6D8dk/Uck/XcPxTvIsAgtO4/0PHti4ME7T+XwbYIZe3wP8HIDoWtGfc/IAQXItnI+T+waT9ZApv6Pw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["safety"], ["safety"], ["safety"], ["safety"], ["safety"], ["safety"], ["safety"], ["safety"]], "hovertemplate": "task=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "safety", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "safety", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "2FBeQ3kN9T8TYk4TYk7zP3Dn+FhpwwJAaRG5pbuR9j+cmIhE4wX5P11Ii+URoQFAjwTxnqx//D/yXTiIHt8EQA=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["spamming"], ["spamming"], ["spamming"], ["spamming"], ["spamming"], ["spamming"], ["spamming"], ["spamming"]], "hovertemplate": "task=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "spamming", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "spamming", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAClzDHDwc6M/AAAAAAAAAAC61RmpmFudPw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["spoofing"], ["spoofing"], ["spoofing"], ["spoofing"], ["spoofing"], ["spoofing"], ["spoofing"], ["spoofing"]], "hovertemplate": "task=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "spoofing", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "spoofing", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "9oDZA2YPuD8TYk4TYk7TP30vhdy8DuE/nYHTmB/L2T8j1cmZzanBP/SPD4zsUMg/eQPQ5pu2xT84iB7fhYPgPw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["spotting"], ["spotting"], ["spotting"], ["spotting"], ["spotting"], ["spotting"], ["spotting"], ["spotting"]], "hovertemplate": "task=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "spotting", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "spotting", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "9oDZA2YP2D8dk/Uck/XcP2OfbNUVeMQ/0PHti4ME3T8j1cmZzanRP/SPD4zsUMg/eQPQ5pu2xT+UJbBpP1nCPw=="}, "yaxis": "y", "type": "scatter"}], "layout": {"xaxis": {"anchor": "y", "domain": [0.0, 1.0], "title": {"text": "year"}}, "yaxis": {"anchor": "x", "domain": [0.0, 1.0], "title": {"text": "occurrences (%)"}}, "legend": {"title": {"text": "task"}, "tracegroupgap": 0}}}
</code></pre> <p style="--stagger: 13;" data-animate=""><strong>Explainability and interpretability</strong> has gained traction in the last few years, with a significant increase in the number of papers published on the topic around 2019, following a surge in some specific conferences and workshops on model transparency, interpretability, and fairness, such as <a href="https://facctconference.org/" rel="external nofollow noopener" target="_blank">ACM FaccT</a> and <a href="https://visxai.io/" rel="external nofollow noopener" target="_blank">VISxAI</a>. Explainability is crucial for building trust in AI systems and ensuring that they make decisions based on valid reasoning. One of the areas that has seen the most investment in recent years is model grounding, the process of tying the model’s predictions to specific features in the input data. This is particularly important in applications such as image classification and question answering, where it is essential to understand which parts of an input (text, image) are driving the model’s predictions.</p> <pre><code class="language-plotly">{"data": [{"customdata": [["explainability"], ["explainability"], ["explainability"], ["explainability"], ["explainability"], ["explainability"], ["explainability"], ["explainability"]], "hovertemplate": "word=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "explainability", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "explainability", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "9oDZA2YPuD8TYk4TYk6zPy5/O8fHSts/AjGEv/Me0D8LrqN3/DDgP76sEqjoLd0/CgP2acj/0j+61RmpmFvdPw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["grounding"], ["grounding"], ["grounding"], ["grounding"], ["grounding"], ["grounding"], ["grounding"], ["grounding"]], "hovertemplate": "word=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "grounding", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "grounding", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "uSDjgowL0j9WLrhVLrjlP0kPVM5u4ec/NqGesldY4z+1v65mtH7qP11Ii+URofE/lYMGxlBk9j9ZApv2kyX6Pw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["interpretability"], ["interpretability"], ["interpretability"], ["interpretability"], ["interpretability"], ["interpretability"], ["interpretability"], ["interpretability"]], "hovertemplate": "word=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "interpretability", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "interpretability", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "uSDjgowL4j+Y+iGY+iHYP3Dn+Fhpw/I/KQXYtf6J8j+XwbYIZe3wPyM7FLZmnO8/t4JSzKn28D9C9PguHETzPw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["traceability"], ["traceability"], ["traceability"], ["traceability"], ["traceability"], ["traceability"], ["traceability"], ["traceability"]], "hovertemplate": "word=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "traceability", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "traceability", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAC61RmpmFudPw=="}, "yaxis": "y", "type": "scatter"}], "layout": {"legend": {"title": {"text": "word"}, "tracegroupgap": 0}, "xaxis": {"anchor": "y", "domain": [0.0, 1.0], "title": {"text": "year"}}, "yaxis": {"anchor": "x", "domain": [0.0, 1.0], "title": {"text": "occurrences (%)"}}}}
</code></pre> <p style="--stagger: 14;" data-animate=""><strong>Visual tasks</strong> such as image denoising have received a lot of attention in recent years, with many papers published on the topic. This may be due to the increasing importance of image quality in computer vision applications, the development of new techniques to improve image quality, and the increased capacity of visual models to handle larger inputs. This category of tasks also includes deblurring, dehazing, demoireing, deraining, and others. Image processing and image generation tasks have also increased significantly.</p> <pre><code class="language-plotly">{"data": [{"customdata": [["colorization"], ["colorization"], ["colorization"], ["colorization"], ["colorization"], ["colorization"], ["colorization"], ["colorization"]], "hovertemplate": "task=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "colorization", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "colorization", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "NOHPhD8T3j8dk/Uck/XMPy5/O8fHSts/AjGEv/Me0D+EcWIiEo23P/SPD4zsUMg/eQPQ5pu2tT+UJbBpP1nCPw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["denoising"], ["denoising"], ["denoising"], ["denoising"], ["denoising"], ["denoising"], ["denoising"], ["denoising"]], "hovertemplate": "task=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "denoising", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "denoising", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "5+ibo2+OBkAdk/Uck/UMQB4lzGKfbA1A6il7hTWhDkAHXUtAUZkLQPSPD4zsUAhAPIRNAY52CkB1RirmVucVQA=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["editing"], ["editing"], ["editing"], ["editing"], ["editing"], ["editing"], ["editing"], ["editing"]], "hovertemplate": "task=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "editing", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "editing", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "9oDZA2YP6D/bxovaxovqP3Dn+Fhpw+I/Qj1lr7Am9D/HDwNNB9/zP4vlEaGp9vs/zYNzhLq/B0CNVMytzkgQQA=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["image enhancement"], ["image enhancement"], ["image enhancement"], ["image enhancement"], ["image enhancement"], ["image enhancement"], ["image enhancement"], ["image enhancement"]], "hovertemplate": "task=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "image enhancement", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "image enhancement", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "AAAAAAAAAACY+iGY+iHYPy5/O8fHSss/nYHTmB/L2T+1v65mtH7aP/SPD4zsUNg/CgP2acj/0j8Vc6szUjHfPw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["image filling"], ["image filling"], ["image filling"], ["image filling"], ["image filling"], ["image filling"], ["image filling"], ["image filling"]], "hovertemplate": "task=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "image filling", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "image filling", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "JUmSJEmS/D/8rMD7rMD7P4BBaL2RoQBAnYHTmB/LCUD4XU+RqdAGQMDBz028nghAQgNjKDJbBEC1nyyBTfsLQA=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["image generation"], ["image generation"], ["image generation"], ["image generation"], ["image generation"], ["image generation"], ["image generation"], ["image generation"]], "hovertemplate": "task=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "image generation", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "image generation", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "2FBeQ3kN5T8dk/Uck/X8Py5/O8fHSvs/HGkRuaW7AUALrqN3/DAAQIrXkzLHAP8/lYMGxlBkBkC+CwfR47sOQA=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["image retrieval"], ["image retrieval"], ["image retrieval"], ["image retrieval"], ["image retrieval"], ["image retrieval"], ["image retrieval"], ["image retrieval"]], "hovertemplate": "task=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "image retrieval", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "image retrieval", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "uSDjgowL8j9fX19fX1/vP2OfbNUVePQ/XHXyqGLD9T+XwbYIZe3wP8TkCmJyBeE/IAQXItnI6T+UJbBpP1niPw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["image segmentation"], ["image segmentation"], ["image segmentation"], ["image segmentation"], ["image segmentation"], ["image segmentation"], ["image segmentation"], ["image segmentation"]], "hovertemplate": "task=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "image segmentation", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "image segmentation", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "NOHPhD8T7j81SIM0SIP0P1x7phNsUvU/trlgktFn6z+cmIhE4wXpP/erC2mxPPI/CgP2acj/8j9VzK3OSMX4Pw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["image to image"], ["image to image"], ["image to image"], ["image to image"], ["image to image"], ["image to image"], ["image to image"], ["image to image"]], "hovertemplate": "task=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "image to image", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "image to image", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "9oDZA2YP2D93FO12FO32P4RTS55mNABAT9krrAn19D8QhXWzekn4P4vlEaGp9us/CgP2acj/4j+n/WQJbNrnPw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["localization"], ["localization"], ["localization"], ["localization"], ["localization"], ["localization"], ["localization"], ["localization"]], "hovertemplate": "task=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "localization", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "localization", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "2FBeQ3kNFUAD77MC77MSQChbdQUeJQxAaRG5pbuRBkBxIQ48vywOQL2l03D3sg5ASsTocGjNCkC3OiMVc6sMQA=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["matching"], ["matching"], ["matching"], ["matching"], ["matching"], ["matching"], ["matching"], ["matching"]], "hovertemplate": "task=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "matching", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "matching", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "Cn8m/JnwGUCcm5ubm5sTQHJwasnTjBJASYvILd2NFEDbIpS1w/8WQBCM7FDYmhNAV+PLz3ndFEA/WQKb9pMSQA=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["odometry"], ["odometry"], ["odometry"], ["odometry"], ["odometry"], ["odometry"], ["odometry"], ["odometry"]], "hovertemplate": "task=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "odometry", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "odometry", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "9oDZA2YPyD+Y+iGY+iHYP2OfbNUVeNQ/0PHti4ME3T9UIxbeb5vUP/SPD4zsUMg/WASE4EIkyz+UJbBpP1nCPw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["quality assessment"], ["quality assessment"], ["quality assessment"], ["quality assessment"], ["quality assessment"], ["quality assessment"], ["quality assessment"], ["quality assessment"]], "hovertemplate": "task=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "quality assessment", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "quality assessment", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "9oDZA2YPyD8TYk4TYk7TPy5/O8fHSss/nYHTmB/LyT9UIxbeb5vUP44BDn5u4tU/mwIc7fRI0D84iB7fhYPgPw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["reconstruction"], ["reconstruction"], ["reconstruction"], ["reconstruction"], ["reconstruction"], ["reconstruction"], ["reconstruction"], ["reconstruction"]], "hovertemplate": "task=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "reconstruction", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "reconstruction", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "vYbyGsprEkDv2p/u2p8WQGOfbNUVeBRAgKIUYNf6F0DN5tSIhfcbQI3zjw+M7BhA3XI9f4LlIUCUJbBpP9keQA=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["removal"], ["removal"], ["removal"], ["removal"], ["removal"], ["removal"], ["removal"], ["removal"]], "hovertemplate": "task=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "removal", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "removal", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "uSDjgowL8j9WLrhVLrj1P0kPVM5u4fc/T9krrAn19D/HDwNNB9/zPyM7FLZmnO8/xwReXRbb7T/mVmekYm7xPw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["style transfer"], ["style transfer"], ["style transfer"], ["style transfer"], ["style transfer"], ["style transfer"], ["style transfer"], ["style transfer"]], "hovertemplate": "task=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "style transfer", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "style transfer", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "uSDjgowL4j9fX19fX1/vPxTvIsAgtO4/HGkRuaW74T/HDwNNB9/zPylzDHDwc+M/xwReXRbb3T/mVmekYm7hPw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["super resolution"], ["super resolution"], ["super resolution"], ["super resolution"], ["super resolution"], ["super resolution"], ["super resolution"], ["super resolution"]], "hovertemplate": "task=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "super resolution", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "super resolution", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "2FBeQ3kNBUDrOSbrOSYLQEX9cO2ZTghAsGv9E6UAC0C0/HHkSr4QQFkeEZpqvwpAQgNjKDJbBECwaT9ZApsKQA=="}, "yaxis": "y", "type": "scatter"}], "layout": {"xaxis": {"anchor": "y", "domain": [0.0, 1.0], "title": {"text": "year"}}, "yaxis": {"anchor": "x", "domain": [0.0, 1.0], "title": {"text": "occurrences (%)"}}, "legend": {"title": {"text": "task"}, "tracegroupgap": 0}}}
</code></pre> <p style="--stagger: 15;" data-animate=""><strong>Language tasks</strong> have also seen a fluctuation in the number of papers published over the past few years, particularly those that focus on dialogue and conversation. By using a conversational interface, users can interact with AI systems in a more natural and intuitive way, leading to better user experiences and more effective communication. This has led to a surge in research on dialog systems, including chatbots, virtual assistants, and other conversational agents. The development of large-scale language models has also played a significant role in this trend, as these models have demonstrated impressive capabilities in generating human-like text and understanding context.</p> <pre><code class="language-plotly">{"data": [{"customdata": [["dialog"], ["dialog"], ["dialog"], ["dialog"], ["dialog"], ["dialog"], ["dialog"], ["dialog"]], "hovertemplate": "task=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "dialog", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "dialog", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "9oDZA2YP2D/RleTQleTgPxTvIsAgtN4/nYHTmB/L2T/lDfuqVnDNPylzDHDwc8M/mwIc7fRIwD+dkYq51RnlPw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["language translation"], ["language translation"], ["language translation"], ["language translation"], ["language translation"], ["language translation"], ["language translation"], ["language translation"]], "hovertemplate": "task=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "language translation", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "language translation", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "AAAAAAAAAAATYk4TYk6zPwAAAAAAAAAAnYHTmB/LqT+EcWIiEo2nPylzDHDwc7M/eQPQ5pu2pT+61RmpmFudPw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["question answering"], ["question answering"], ["question answering"], ["question answering"], ["question answering"], ["question answering"], ["question answering"], ["question answering"]], "hovertemplate": "task=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "question answering", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "question answering", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "FbFUxFIR6z8TYk4TYk7zP3Dn+Fhpw+I/nYHTmB/L2T+EcWIiEo3XP/SPD4zsUNg/CgP2acj/4j9C9PguHETjPw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["summarization"], ["summarization"], ["summarization"], ["summarization"], ["summarization"], ["summarization"], ["summarization"], ["summarization"]], "hovertemplate": "task=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "summarization", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "summarization", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "9oDZA2YP6D+Y+iGY+iHYP2OfbNUVeMQ/nYHTmB/LuT+EcWIiEo23PylzDHDwc8M/eQPQ5pu2tT+UJbBpP1nCPw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["text generation"], ["text generation"], ["text generation"], ["text generation"], ["text generation"], ["text generation"], ["text generation"], ["text generation"]], "hovertemplate": "task=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "text generation", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "text generation", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACEcWIiEo2nPwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA=="}, "yaxis": "y", "type": "scatter"}], "layout": {"legend": {"title": {"text": "task"}, "tracegroupgap": 0}, "xaxis": {"anchor": "y", "domain": [0.0, 1.0], "title": {"text": "year"}}, "yaxis": {"anchor": "x", "domain": [0.0, 1.0], "title": {"text": "occurrences (%)"}}}}
</code></pre> <p style="--stagger: 16;" data-animate=""><strong>Multimodal tasks</strong> are one of the current trends in artificial intelligence. These tasks involve the combination of different modalities, such as audio, text, and images, to improve the performance of models and to solve problems that require a deeper understanding of the intermodality of the world. The number of papers published on these tasks has increased significantly in recent years, with a particular focus on tasks such as image-text alignment, image synthesis, video synthesis, and visual question answering. This trend is likely to continue as researchers explore new ways to combine different modalities in novel ways and improve the performance of models.</p> <pre><code class="language-plotly">{"data": [{"customdata": [["alignment"], ["alignment"], ["alignment"], ["alignment"], ["alignment"], ["alignment"], ["alignment"], ["alignment"]], "hovertemplate": "task=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "alignment", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "alignment", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "5+ibo2+OBkAD77MC77MCQGCNifRA5QRAiZepHZqVCEAo6V5T4gEQQHYobM04/xJAV+PLz3ndFECuzkjF3OoZQA=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["audio synthesis"], ["audio synthesis"], ["audio synthesis"], ["audio synthesis"], ["audio synthesis"], ["audio synthesis"], ["audio synthesis"], ["audio synthesis"]], "hovertemplate": "task=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "audio synthesis", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "audio synthesis", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAClzDHDwc7M/eQPQ5pu2tT+61RmpmFudPw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["captioning"], ["captioning"], ["captioning"], ["captioning"], ["captioning"], ["captioning"], ["captioning"], ["captioning"]], "hovertemplate": "task=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "captioning", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "captioning", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "JUmSJEmS/D/yexnyexnyP3YLvxoT6fE/T9krrAn15D9UIxbeb5vkP76sEqjoLe0/mwIc7fRI8D/5LhxEj+/2Pw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["image synthesis"], ["image synthesis"], ["image synthesis"], ["image synthesis"], ["image synthesis"], ["image synthesis"], ["image synthesis"], ["image synthesis"]], "hovertemplate": "task=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "image synthesis", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "image synthesis", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "2FBeQ3kN5T8dk/Uck/X8Py5/O8fHSvs/HGkRuaW7AUALrqN3/DAAQIrXkzLHAP8/lYMGxlBkBkC+CwfR47sOQA=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["referring expression comprehension"], ["referring expression comprehension"], ["referring expression comprehension"], ["referring expression comprehension"], ["referring expression comprehension"], ["referring expression comprehension"], ["referring expression comprehension"], ["referring expression comprehension"]], "hovertemplate": "task=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "referring expression comprehension", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "referring expression comprehension", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "9oDZA2YPuD8TYk4TYk7DPy5/O8fHSrs/NqGesldYwz+EcWIiEo23PylzDHDwc6M/CgP2acj/0j+61RmpmFu9Pw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["video question answering"], ["video question answering"], ["video question answering"], ["video question answering"], ["video question answering"], ["video question answering"], ["video question answering"], ["video question answering"]], "hovertemplate": "task=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "video question answering", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "video question answering", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "AAAAAAAAAAATYk4TYk6zPy5/O8fHSqs/NqGesldYwz+EcWIiEo3HP/SPD4zsUMg/mwIc7fRI4D9LYNN+sgTWPw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["video synthesis"], ["video synthesis"], ["video synthesis"], ["video synthesis"], ["video synthesis"], ["video synthesis"], ["video synthesis"], ["video synthesis"]], "hovertemplate": "task=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "video synthesis", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "video synthesis", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "AAAAAAAAAAATYk4TYk7TP0kPVM5u4dc/NqGesldY0z8j1cmZzanRP/SPD4zsUNg/xwReXRbb3T+P78JB9PjwPw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["visual grounding"], ["visual grounding"], ["visual grounding"], ["visual grounding"], ["visual grounding"], ["visual grounding"], ["visual grounding"], ["visual grounding"]], "hovertemplate": "task=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "visual grounding", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "visual grounding", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "9oDZA2YPuD8dk/Uck/XMPy5/O8fHSrs/nYHTmB/LuT8j1cmZzanRPylzDHDwc9M/CgP2acj/0j9eOIge34XbPw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["visual question answering"], ["visual question answering"], ["visual question answering"], ["visual question answering"], ["visual question answering"], ["visual question answering"], ["visual question answering"], ["visual question answering"]], "hovertemplate": "task=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "visual question answering", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "visual question answering", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "yLgg44KM8z8+eSo+eSr+P1x7phNsUvU/D81KvEzt8D8LrqN3/DDwP76sEqjoLe0/CgP2acj/8j9QlsCm/WT3Pw=="}, "yaxis": "y", "type": "scatter"}], "layout": {"xaxis": {"anchor": "y", "domain": [0.0, 1.0], "title": {"text": "year"}}, "yaxis": {"anchor": "x", "domain": [0.0, 1.0], "title": {"text": "occurrences (%)"}}, "legend": {"title": {"text": "task"}, "tracegroupgap": 0}}}
</code></pre> <p style="--stagger: 17;" data-animate="">Here we focus on analyzing the use of some keywords in the LLM papers. More specifically:</p> <ul style="--stagger: 18;" data-animate=""> <li> <strong>Chain-of-Thought</strong>, <strong>Tree-of-Thought</strong>, and any <strong>of-Thought</strong> variations - these are prompting techniques that help the model to break down complex tasks into smaller, more manageable steps, allowing it to reason through the problem more effectively;</li> <li> <strong>Agent</strong> - refers to the use of LLMs as agents that can perform tasks autonomously, often in conjunction with other tools or systems;</li> <li> <strong>Distillation</strong> - a technique used to compress large models into smaller, more efficient ones while retaining their performance;</li> <li> <strong>Few-shot prompting</strong> - a prompting technique that provides the model with a few examples of the task at hand, allowing it to generalize and perform well on similar tasks;</li> <li> <strong>Fine-tuning</strong> - the process of training a pre-trained model on a specific task or dataset to improve its performance;</li> <li> <strong>Reinforcement Learning (RL)</strong> - a type of machine learning where an agent learns to make decisions by receiving feedback from its environment in the form of rewards or penalties;</li> <li> <strong>Retrieval Augmented Generation (RAG)</strong> - a technique that combines retrieval-based methods with generative models to improve the performance of language models on specific tasks;</li> <li> <strong>Self-Instruct</strong> - a technique that allows models to learn from their own outputs, improving their performance over time;</li> <li> <strong>Tokenizer</strong> - a component of language models that converts text into a format that the model can understand, often by breaking it down into smaller units called tokens;</li> <li> <strong>Tool</strong> - refers to the use of external tools or systems in conjunction with LLMs to perform tasks more effectively;</li> <li> <strong>Zero-shot prompting</strong> - a prompting technique that allows the model to perform tasks without any prior examples or training on that specific task.</li> </ul> <p style="--stagger: 19;" data-animate="">Few-shot and zero-shot prompting have lost the interest of the academic community in favor of RAG, thought processes, and novel fine-tuning techniques. Interest in creating LLM agents that can tackle harder tasks and use tools is one of the hottest topics in the field.</p> <pre><code class="language-plotly">{"data": [{"customdata": [["* of thought"], ["* of thought"], ["* of thought"], ["* of thought"], ["* of thought"], ["* of thought"], ["* of thought"]], "hovertemplate": "word=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "* of thought", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "* of thought", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+QH5QfmB+cH6Ac="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAfDQhL2wVBEA="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["agent"], ["agent"], ["agent"], ["agent"], ["agent"], ["agent"], ["agent"]], "hovertemplate": "word=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "agent", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "agent", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+QH5QfmB+cH6Ac="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABbUZmK5zHEA="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["distillation"], ["distillation"], ["distillation"], ["distillation"], ["distillation"], ["distillation"], ["distillation"]], "hovertemplate": "word=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "distillation", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "distillation", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+QH5QfmB+cH6Ac="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABJQI7jOI7jOBZAus6xRiIgDkA="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["few shot"], ["few shot"], ["few shot"], ["few shot"], ["few shot"], ["few shot"], ["few shot"]], "hovertemplate": "word=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "few shot", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "few shot", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+QH5QfmB+cH6Ac="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAKuqqqqqqjBAMU653d/BJUA="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["finetuning"], ["finetuning"], ["finetuning"], ["finetuning"], ["finetuning"], ["finetuning"], ["finetuning"]], "hovertemplate": "word=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "finetuning", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "finetuning", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+QH5QfmB+cH6Ac="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAI7jOI7jOCZAus6xRiIgLkA="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["reinforcement learning"], ["reinforcement learning"], ["reinforcement learning"], ["reinforcement learning"], ["reinforcement learning"], ["reinforcement learning"], ["reinforcement learning"]], "hovertemplate": "word=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "reinforcement learning", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "reinforcement learning", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+QH5QfmB+cH6Ac="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "AAAAAAAASUAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAKuqqqqqqjBA3P0dXPaGQEA="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["retrieval augmented generation"], ["retrieval augmented generation"], ["retrieval augmented generation"], ["retrieval augmented generation"], ["retrieval augmented generation"], ["retrieval augmented generation"], ["retrieval augmented generation"]], "hovertemplate": "word=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "retrieval augmented generation", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "retrieval augmented generation", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+QH5QfmB+cH6Ac="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "AAAAAAAAOUAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABJQKuqqqqqqkBAayQC4qMJQ0A="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["tokenizer"], ["tokenizer"], ["tokenizer"], ["tokenizer"], ["tokenizer"], ["tokenizer"], ["tokenizer"]], "hovertemplate": "word=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "tokenizer", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "tokenizer", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+QH5QfmB+cH6Ac="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAfDQhL2wV9D8="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["tool"], ["tool"], ["tool"], ["tool"], ["tool"], ["tool"], ["tool"]], "hovertemplate": "word=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "tool", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "tool", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+QH5QfmB+cH6Ac="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "AAAAAAAAOUAAAAAAAAAAAAAAAAAAAElAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAMU653d/BFUA="}, "yaxis": "y", "type": "scatter"}, {"customdata": [["zero shot"], ["zero shot"], ["zero shot"], ["zero shot"], ["zero shot"], ["zero shot"], ["zero shot"]], "hovertemplate": "word=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "zero shot", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "zero shot", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+QH5QfmB+cH6Ac="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABJQBzHcRzHcUNAoifVVzI/M0A="}, "yaxis": "y", "type": "scatter"}], "layout": {"legend": {"title": {"text": "word"}, "tracegroupgap": 0}, "xaxis": {"anchor": "y", "domain": [0.0, 1.0], "title": {"text": "year"}}, "yaxis": {"anchor": "x", "domain": [0.0, 1.0], "title": {"text": "occurrences (%)"}}}}
</code></pre> <h2 id="information-about-authors" style="--stagger: 20;" data-animate="">Information about Authors</h2> <p style="--stagger: 21;" data-animate="">Now let’s look at the authors of the papers. This first graph shows the number of papers published by each author. As we can see, most authors have published only one paper at the conference. Out of 33,861 authors, only 1,308 have 10 or more accepted papers.</p> <pre><code class="language-plotly">{"data": [{"hovertemplate": "Number of papers=%{text}&lt;br&gt;Authors=%{y}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "", "marker": {"pattern": {"shape": ""}}, "name": "", "orientation": "v", "showlegend": false, "text": {"dtype": "f8", "bdata": "AAAAAADAYEAAAAAAAMBdQAAAAAAAAFlAAAAAAACAVUAAAAAAAMBUQAAAAAAAAFRAAAAAAADAU0AAAAAAAMBSQAAAAAAAwFFAAAAAAACAUUAAAAAAAEBRQAAAAAAAgFBAAAAAAABAUEAAAAAAAABQQAAAAAAAgE9AAAAAAAAAT0AAAAAAAIBOQAAAAAAAgE1AAAAAAAAATUAAAAAAAABMQAAAAAAAgEtAAAAAAAAAS0AAAAAAAIBKQAAAAAAAAEpAAAAAAACASUAAAAAAAABJQAAAAAAAgEhAAAAAAAAASEAAAAAAAIBHQAAAAAAAAEdAAAAAAAAARkAAAAAAAIBFQAAAAAAAAEVAAAAAAACAREAAAAAAAABEQAAAAAAAgENAAAAAAAAAQ0AAAAAAAIBCQAAAAAAAAEJAAAAAAACAQUAAAAAAAABBQAAAAAAAgEBAAAAAAAAAQEAAAAAAAAA/QAAAAAAAAD5AAAAAAAAAPUAAAAAAAAA8QAAAAAAAADtAAAAAAAAAOkAAAAAAAAA5QAAAAAAAADhAAAAAAAAAN0AAAAAAAAA2QAAAAAAAADVAAAAAAAAANEAAAAAAAAAzQAAAAAAAADJAAAAAAAAAMUAAAAAAAAAwQAAAAAAAAC5AAAAAAAAALEAAAAAAAAAqQAAAAAAAAChAAAAAAAAAJkAAAAAAAAAkQAAAAAAAACJAAAAAAAAAIEAAAAAAAAAcQAAAAAAAABhAAAAAAAAAFEAAAAAAAAAQQAAAAAAAAAhAAAAAAAAAAEAAAAAAAADwPw=="}, "textposition": "auto", "x": {"dtype": "i2", "bdata": "hgB3AGQAVgBTAFAATwBLAEcARgBFAEIAQQBAAD8APgA9ADsAOgA4ADcANgA1ADQAMwAyADEAMAAvAC4ALAArACoAKQAoACcAJgAlACQAIwAiACEAIAAfAB4AHQAcABsAGgAZABgAFwAWABUAFAATABIAEQAQAA8ADgANAAwACwAKAAkACAAHAAYABQAEAAMAAgABAA=="}, "xaxis": "x", "y": {"dtype": "i2", "bdata": "AQABAAEAAQABAAEAAQABAAEAAQABAAEAAQABAAIAAQABAAEABQAEAAIAAQABAAMAAQACAAIABAABAAIABAAEAAQAAgAHAAgACwAGAAsABAAFAAYAFAALABIACwASABEAEgAVAB4AGAAWACQAIgAyACMALABAAFEAWABrAHsAlgCzAAIBKgGeAUMCSQOXBW8J3RU8UQ=="}, "yaxis": "y", "type": "bar"}], "layout": {"xaxis": {"anchor": "y", "domain": [0.0, 1.0], "title": {"text": "Number of papers"}}, "yaxis": {"anchor": "x", "domain": [0.0, 1.0], "title": {"text": "Authors"}}, "legend": {"tracegroupgap": 0}, "barmode": "relative"}}
</code></pre> <p>Here are the top 10 authors with the most papers:</p> <table> <thead> <tr> <th>Author</th> <th>Papers</th> </tr> </thead> <tbody> <tr> <td>Luc Van Gool</td> <td>134</td> </tr> <tr> <td>Radu Timofte</td> <td>119</td> </tr> <tr> <td>Lei Zhang</td> <td>100</td> </tr> <tr> <td>Yi Yang</td> <td>86</td> </tr> <tr> <td>Yu Qiao</td> <td>83</td> </tr> <tr> <td>Dacheng Tao</td> <td>80</td> </tr> <tr> <td>Ming-Hsuan Yang</td> <td>79</td> </tr> <tr> <td>Qi Tian</td> <td>75</td> </tr> <tr> <td>Marc Pollefeys</td> <td>71</td> </tr> <tr> <td>Xiaogang Wang</td> <td>70</td> </tr> </tbody> </table> <p>Now let’s look at the number of authors per paper. Most of the papers have between 2 and 7 authors, but there are a few with a large number of authors, such as <a href="https://openaccess.thecvf.com/content/CVPR2023/html/Eisenmann_Why_Is_the_Winner_the_Best_CVPR_2023_paper.html" rel="external nofollow noopener" target="_blank">Why Is the Winner the Best?</a>, which has 125 authors, and <a href="https://openaccess.thecvf.com/content/CVPR2024W/NTIRE/html/Ren_The_Ninth_NTIRE_2024_Efficient_Super-Resolution_Challenge_Report_CVPRW_2024_paper.html" rel="external nofollow noopener" target="_blank">The Ninth NTIRE 2024 Efficient Super-Resolution Challenge Report</a>, with a staggering 134 authors. The former is a multi-center study of all 80 competitions held as part of IEEE ISBI 2021 and MICCAI 2021, while the latter is a report summarizing the results of the NTIRE 2024 challenge, a competition held at the CVPR conference.</p> <pre><code class="language-plotly">{"data": [{"hovertemplate": "Number of authors=%{x}&lt;br&gt;Number of papers=%{text}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "", "marker": {"pattern": {"shape": ""}}, "name": "", "orientation": "v", "showlegend": false, "text": {"dtype": "f8", "bdata": "AAAAAAAgZ0AAAAAAAByWQAAAAAAAoqZAAAAAAAAArEAAAAAAAE6pQAAAAAAA2KJAAAAAAADklkAAAAAAALiIQAAAAAAA0HhAAAAAAADAaUAAAAAAAIBcQAAAAAAAAFBAAAAAAAAAOUAAAAAAAAA3QAAAAAAAACRAAAAAAAAAJkAAAAAAAAAQQAAAAAAAAABAAAAAAAAA8D8AAAAAAAAQQAAAAAAAAABAAAAAAAAAAEAAAAAAAAAQQAAAAAAAAAhAAAAAAAAA8D8AAAAAAAAIQAAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAAAEAAAAAAAAAAQAAAAAAAAPA/AAAAAAAA8D8AAAAAAAAAQAAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAACEAAAAAAAADwPwAAAAAAAPA/AAAAAAAACEAAAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8="}, "textposition": "auto", "x": {"dtype": "i2", "bdata": "AQACAAMABAAFAAYABwAIAAkACgALAAwADQAOAA8AEAARABIAEwAUABUAFgAXABgAGwAcAB8AIQAiACMAJAAlACcAKQAqACsALQAwADEANQA3ADgAOgBCAEMARABOAE8AVQBYAF0AZABlAGwAcQBzAH0AhgA="}, "xaxis": "x", "y": {"dtype": "i2", "bdata": "uQCHBVELAA6nDGwJuQUXA40BzgByAEAAGQAXAAoACwAEAAIAAQAEAAIAAgAEAAMAAQADAAEAAQABAAEAAgACAAEAAQACAAEAAQABAAEAAQABAAEAAwABAAEAAwABAAEAAQABAAEAAQABAAEAAQABAAEAAQA="}, "yaxis": "y", "type": "bar"}], "layout": {"xaxis": {"anchor": "y", "domain": [0.0, 1.0], "title": {"text": "Number of authors"}}, "yaxis": {"anchor": "x", "domain": [0.0, 1.0], "title": {"text": "Number of papers"}}, "legend": {"tracegroupgap": 0}, "barmode": "relative"}}
</code></pre> <p>Since most papers have multiple authors, it is quite common to see some authors constantly collaborating with each other. The most common pair of authors is Jiwen Lu and Jie Zhou, who have collaborated on 57 papers together. The second most common pair is Luc Van Gool and Radu Timofte with 43 papers together, followed by Tao Xiang and Yi-Zhe Song with 38 papers. The top 10 most frequent pairs of authors are:</p> <table> <thead> <tr> <th>Author 1</th> <th>Author 2</th> <th>Papers</th> </tr> </thead> <tbody> <tr> <td>Jiwen Lu</td> <td>Jie Zhou</td> <td>57</td> </tr> <tr> <td>Luc Van Gool</td> <td>Radu Timofte</td> <td>43</td> </tr> <tr> <td>Tao Xiang</td> <td>Yi-Zhe Song</td> <td>38</td> </tr> <tr> <td>Fahad Shahbaz Khan</td> <td>Salman Khan</td> <td>33</td> </tr> <tr> <td>Ting Yao</td> <td>Tao Mei</td> <td>32</td> </tr> <tr> <td>Xiaogang Wang</td> <td>Hongsheng Li</td> <td>28</td> </tr> <tr> <td>Shiguang Shan</td> <td>Xilin Chen</td> <td>27</td> </tr> <tr> <td>Richa Singh</td> <td>Mayank Vatsa</td> <td>26</td> </tr> <tr> <td>Dong Chen</td> <td>Fang Wen</td> <td>24</td> </tr> <tr> <td>Yi-Zhe Song</td> <td>Ayan Kumar Bhunia</td> <td>24</td> </tr> </tbody> </table> <p>Although it is quite rare for a paper to have a single author, 185 papers fall into this category. A few worthy mentions are research that introduced novel loss functions (<a href="https://openaccess.thecvf.com/content_CVPR_2019/html/Barron_A_General_and_Adaptive_Robust_Loss_Function_CVPR_2019_paper.html" rel="external nofollow noopener" target="_blank">Jonathan T. Barron</a>, <a href="https://openaccess.thecvf.com/content/CVPR2023/html/Kobayashi_Two-Way_Multi-Label_Loss_CVPR_2023_paper.html" rel="external nofollow noopener" target="_blank">Takumi Kobayashi</a>) and improved transformer architectures and post-training techniques (<a href="https://openaccess.thecvf.com/content/CVPR2024/html/Kobayashi_Mean-Shift_Feature_Transformer_CVPR_2024_paper.html" rel="external nofollow noopener" target="_blank">Takumi Kobayashi</a>, <a href="https://openaccess.thecvf.com/content/CVPR2024/html/Ma_Improved_Self-Training_for_Test-Time_Adaptation_CVPR_2024_paper.html" rel="external nofollow noopener" target="_blank">Jing Ma</a>). In this table we can see the authors with the most papers where they are the only author:</p> <table> <thead> <tr> <th>Author</th> <th>Papers</th> </tr> </thead> <tbody> <tr> <td>Takumi Kobayashi</td> <td>4</td> </tr> <tr> <td>Anant Khandelwal, Takuhiro Kaneko</td> <td>3</td> </tr> <tr> <td>Andrey V. Savchenko, Chong Yu, Dimitrios Kollias, Edgar A. Bernal, Jamie Hayes, Magnus Oskarsson, Ming Li, Oleksii Sidorov, Ren Yang, Rowel Atienza, Sanghwa Hong, Satoshi Ikehata, Shunta Maeda, Stamatios Lefkimmiatis, Ying Zhao</td> <td>2</td> </tr> </tbody> </table> <h2 id="identifying-topics">Identifying Topics</h2> <p>For this section, we used <a href="https://arxiv.org/abs/2008.09470" rel="external nofollow noopener" target="_blank">Top2Vec</a>, an automatic topic modeling algorithm, to identify groups of papers that are similar to each other based on their titles and abstracts. The solution found 172 topics, which is a bit too many for us to analyze individually. Instead, we will focus on the hottest and coldest topics, which are those with the most and least papers in the last year, respectively.</p> <p>One problem with the algorithm is that it identifies topics based on the words used in the papers, but it doesn’t provide a clear explanation of what those topics are about. This is a common problem with topic modeling algorithms, as they often produce results that are difficult to interpret. However, we can use LLMs to help us understand the meaning of these topics. We will use the most representative words of each topic (the words that appear most often in the papers of that topic) to generate a title and a paragraph summarizing it.</p> <h3 id="-10-topics">🔥 10 topics</h3> <pre><code class="language-plotly">{"data": [{"customdata": {"dtype": "i1", "bdata": "AQEBAQEBAQE=", "shape": "8, 1"}, "hovertemplate": "topic=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "1", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "1", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "AAAAAAAAAAATYk4TYk6zPy5/O8fHSrs/nYHTmB/LqT8AAAAAAAAAAClzDHDwc6M/WASE4EIk2z+NVMytzkgQQA=="}, "yaxis": "y", "type": "scatter"}, {"customdata": {"dtype": "i1", "bdata": "AgICAgICAgI=", "shape": "8, 1"}, "hovertemplate": "topic=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "2", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "2", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "9oDZA2YP2D9WLrhVLrjlPy5/O8fHSus/T9krrAn19D/gNilv2Ff1P1keEZpqv/o/IAQXItnICUBXZ6RibnUZQA=="}, "yaxis": "y", "type": "scatter"}, {"customdata": {"dtype": "i1", "bdata": "AwMDAwMDAwM=", "shape": "8, 1"}, "hovertemplate": "topic=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "3", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "3", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "yLgg44KMA0DhCH/hCH8BQHodovvnewFAHGkRuaW7AUD4XU+RqdD2PyhljgEOfvY/XoOZB+cI9T/+ZAls2k8IQA=="}, "yaxis": "y", "type": "scatter"}, {"customdata": {"dtype": "i1", "bdata": "BAQEBAQEBAQ=", "shape": "8, 1"}, "hovertemplate": "topic=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "4", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "4", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "AAAAAAAAAAATYk4TYk6zPy5/O8fHSrs/nYHTmB/LqT8AAAAAAAAAAClzDHDwc7M/CgP2acj/0j+rM1Ixtzr5Pw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": {"dtype": "i1", "bdata": "BQUFBQUFBQU=", "shape": "8, 1"}, "hovertemplate": "topic=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "5", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "5", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "qYilIpaKAEBWLrhVLrgFQFZX4FHCLAZAL1M7NCvxAkCcmIhE4wX5P43zjw+M7Pg/mwIc7fRI4D8MB9Hju3D8Pw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": {"dtype": "i1", "bdata": "BgYGBgYGBgY=", "shape": "8, 1"}, "hovertemplate": "topic=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "6", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "6", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "NOHPhD8T3j9WLrhVLrjlP0kPVM5u4ec/0PHti4ME3T87/O+7niLjP1keEZpqv9o/eQPQ5pu2tT9VzK3OSMXoPw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": {"dtype": "i1", "bdata": "BwcHBwcHBwc=", "shape": "8, 1"}, "hovertemplate": "topic=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "7", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "7", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "9oDZA2YPuD8AAAAAAAAAAAAAAAAAAAAAnYHTmB/LuT+EcWIiEo2nPylzDHDwc8M/0wKJq16k4T+ZW52RirnzPw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": {"dtype": "i1", "bdata": "CAgICAgICAg=", "shape": "8, 1"}, "hovertemplate": "topic=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "8", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "8", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "uSDjgowL4j9WLrhVLrjlPy5/O8fHSqs/NqGesldYwz+EcWIiEo3HPyM7FLZmnN8/AAAAAAAAAADwwkH0+C7kPw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": {"dtype": "i1", "bdata": "CQkJCQkJCQk=", "shape": "8, 1"}, "hovertemplate": "topic=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "9", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "9", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "NOHPhD8T/j9fX19fX1/vP3Dn+Fhpw+I/nYHTmB/L6T8j1cmZzanhP1keEZpqv9o/AAAAAAAAAACUJbBpP1niPw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": {"dtype": "i1", "bdata": "CgoKCgoKCgo=", "shape": "8, 1"}, "hovertemplate": "topic=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "10", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "10", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "uSDjgowL4j/RleTQleTgP2nDMpe/nfM/0PHti4ME3T+1v65mtH7qP/erC2mxPOI/0wKJq16k4T/mVmekYm7xPw=="}, "yaxis": "y", "type": "scatter"}], "layout": {"legend": {"title": {"text": "topic"}, "tracegroupgap": 0}, "xaxis": {"anchor": "y", "domain": [0.0, 1.0], "title": {"text": "year"}}, "yaxis": {"anchor": "x", "domain": [0.0, 1.0], "title": {"text": "occurrences (%)"}}}}
</code></pre> <p>The topics below are listed in the order in which they had the most published articles last year.</p> <details> <summary><b>Topic 1 - Instruction-Tuned Multimodal LLMs for Vision-Language Understanding (157 documents)</b></summary> <figure> <picture> <img src="/assets/img/blog/2025-03-26-cvpr-history-analysis/topic_23.png" class="img-fluid rounded z-depth-1 mx-auto d-block" width="100%" height="auto" alt="Wordcloud for topic 1" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> Recent advancements in large language models (LLMs) and multimodal large language models (MLLMs) have led to remarkable capabilities in integrating visual and textual information for tasks like question answering, dialogue, and reasoning. By leveraging instruction tuning and visual prompt techniques, these models — such as vision-language models (VLMs) like CLIP — have significantly improved in-context comprehension and instruction following across modalities. Despite these achievements, challenges like hallucinations and limited applicability in complex, real-world settings still remain. Research continues to focus on enhancing multimodal instruction learning to facilitate deeper, more reliable understanding between vision and language inputs. <br> <br> Top authors: <ol> <li>Yu Qiao (7 papers)</li> <li>Ying Shan (5 papers)</li> <li>Yixiao Ge (5 papers)</li> </ol> Example papers: <ul> <li><a href="https://openaccess.thecvf.com/content/CVPR2024W/MMFM/html/Wang_LLM-Seg_Bridging_Image_Segmentation_and_Large_Language_Model_Reasoning_CVPRW_2024_paper.html" rel="external nofollow noopener" target="_blank">LLM-Seg: Bridging Image Segmentation and Large Language Model Reasoning (2024)</a></li> <li><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Guan_HallusionBench_An_Advanced_Diagnostic_Suite_for_Entangled_Language_Hallucination_and_CVPR_2024_paper.html" rel="external nofollow noopener" target="_blank">HallusionBench: An Advanced Diagnostic Suite for Entangled Language Hallucination and Visual Illusion in Large Vision-Language Models (2024)</a></li> <li><a href="https://openaccess.thecvf.com/content/CVPR2024W/CVsports/html/Nonaka_Rugby_Scene_Classification_Enhanced_by_Vision_Language_Model_CVPRW_2024_paper.html" rel="external nofollow noopener" target="_blank">Rugby Scene Classification Enhanced by Vision Language Model (2024)</a></li> </ul> <hr> </details> <details> <summary><b>Topic 2 - Controllable Text-Guided Image Editing with Diffusion and GAN Inversion (426 documents)</b></summary> <figure> <picture> <img src="/assets/img/blog/2025-03-26-cvpr-history-analysis/topic_3.png" class="img-fluid rounded z-depth-1 mx-auto d-block" width="100%" height="auto" alt="Wordcloud for topic 2" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> Advances in text-to-image diffusion models and GAN-based techniques have unlocked powerful, controllable image editing capabilities driven by natural language prompts. Methods like StyleGAN inversion, DDIM inversion, and textual inversion allow users to manipulate generated images or real inputs with high fidelity while preserving key features like identity. Text-guided editing leverages the latent space of pretrained diffusion and GAN models, enabling creative, precise, and personalized edits through simple prompts. Despite remarkable progress, achieving fine-grained control over complex edits and extending these capabilities to video editing remain active research challenges. <br> <br> Top authors: <ol> <li>Chen Change Loy (9 papers)</li> <li>Xintao Wang (8 papers)</li> <li>Ying Shan (8 papers)</li> </ol> Example papers: <ul> <li><a href="https://openaccess.thecvf.com/content/CVPR2024W/GCV/html/Bodur_iEdit_Localised_Text-guided_Image_Editing_with_Weak_Supervision_CVPRW_2024_paper.html" rel="external nofollow noopener" target="_blank">iEdit: Localised Text-guided Image Editing with Weak Supervision (2024)</a></li> <li><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Guo_MoMask_Generative_Masked_Modeling_of_3D_Human_Motions_CVPR_2024_paper.html" rel="external nofollow noopener" target="_blank">MoMask: Generative Masked Modeling of 3D Human Motions (2024)</a></li> <li><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Nam_Contrastive_Denoising_Score_for_Text-guided_Latent_Diffusion_Image_Editing_CVPR_2024_paper.html" rel="external nofollow noopener" target="_blank">Contrastive Denoising Score for Text-guided Latent Diffusion Image Editing (2024)</a></li> </ul> <hr> </details> <details> <summary><b>Topic 3 - AI-Driven Medical Imaging and Diagnosis in Clinical Practice (345 documents)</b></summary> <figure> <picture> <img src="/assets/img/blog/2025-03-26-cvpr-history-analysis/topic_5.png" class="img-fluid rounded z-depth-1 mx-auto d-block" width="100%" height="auto" alt="Wordcloud for topic 3" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> The integration of computational methods into medical imaging and pathology has become increasingly important for improving diagnostic accuracy and early disease detection. Techniques like computer-aided diagnosis support clinicians in analyzing tissues, tumors, and organs across modalities such as histopathology, MRI, CT, and digital microscopy. Applications span cancer diagnosis (e.g., breast, brain, skin lesions), neurological diseases like Alzheimer's and Parkinson's, and blood analysis. By enhancing image analysis and tissue classification, these AI-driven tools aid in treatment planning and disease progression monitoring, making them vital in modern clinical practice and biomedical research. <br> <br> Top authors: <ol> <li>Le Lu (9 papers)</li> <li>Faisal Mahmood (5 papers)</li> <li>Ke Yan (5 papers)</li> </ol> Example papers: <ul> <li><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Xiang_SQUID_Deep_Feature_In-Painting_for_Unsupervised_Anomaly_Detection_CVPR_2023_paper.html" rel="external nofollow noopener" target="_blank">SQUID: Deep Feature In-Painting for Unsupervised Anomaly Detection (2023)</a></li> <li><a href="https://openaccess.thecvf.com/content_cvpr_2017_workshops/w3/html/Mery_A_Logarithmic_X-Ray_CVPR_2017_paper.html" rel="external nofollow noopener" target="_blank">A Logarithmic X-Ray Imaging Model for Baggage Inspection: Simulation and Object Detection (2017)</a></li> <li><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Haghighi_DiRA_Discriminative_Restorative_and_Adversarial_Learning_for_Self-Supervised_Medical_Image_CVPR_2022_paper.html" rel="external nofollow noopener" target="_blank">DiRA: Discriminative, Restorative, and Adversarial Learning for Self-Supervised Medical Image Analysis (2022)</a></li> </ul> <hr> </details> <details> <summary><b>Topic 4 - Challenges and Advances in 3D-Aware Text-to-Image and Text-to-Video Generation (68 documents)</b></summary> <figure> <picture> <img src="/assets/img/blog/2025-03-26-cvpr-history-analysis/topic_95.png" class="img-fluid rounded z-depth-1 mx-auto d-block" width="100%" height="auto" alt="Wordcloud for topic 4" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> Text-to-image and text-to-video generation using diffusion models has made remarkable progress, enabling the synthesis of high-fidelity, photorealistic assets from simple prompts. However, existing methods still struggle with accurately handling 3D geometry, novel views, and maintaining global and multi-view consistency. Techniques like diffusion priors, 3D Gaussians, and NeRF-based approaches aim to improve subject-driven generation and diverse, globally consistent outputs. Despite advances in pretrained diffusion models and anisotropic diffusion strategies, achieving high-fidelity, geometry-aware synthesis remains a central challenge in the evolution of text-to-3D and motion generation. <br> <br> Top authors: <ol> <li>Hsin-Ying Lee (4 papers)</li> <li>Sergey Tulyakov (4 papers)</li> <li>Ying Shan (4 papers)</li> </ol> Example papers: <ul> <li><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Kim_Enhancing_3D_Fidelity_of_Text-to-3D_using_Cross-View_Correspondences_CVPR_2024_paper.html" rel="external nofollow noopener" target="_blank">Enhancing 3D Fidelity of Text-to-3D using Cross-View Correspondences (2024)</a></li> <li><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Liu_DIRECT-3D_Learning_Direct_Text-to-3D_Generation_on_Massive_Noisy_3D_Data_CVPR_2024_paper.html" rel="external nofollow noopener" target="_blank">DIRECT-3D: Learning Direct Text-to-3D Generation on Massive Noisy 3D Data (2024)</a></li> <li><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Yi_Diffusion_Time-step_Curriculum_for_One_Image_to_3D_Generation_CVPR_2024_paper.html" rel="external nofollow noopener" target="_blank">Diffusion Time-step Curriculum for One Image to 3D Generation (2024)</a></li> </ul> <hr> </details> <details> <summary><b>Topic 5 - Remote Sensing and Aerial Imagery for Environmental and Agricultural Monitoring (306 documents)</b></summary> <figure> <picture> <img src="/assets/img/blog/2025-03-26-cvpr-history-analysis/topic_10.png" class="img-fluid rounded z-depth-1 mx-auto d-block" width="100%" height="auto" alt="Wordcloud for topic 5" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> The rapid development of satellite and unmanned aerial vehicle (UAV) technologies has fueled increased interest in using high-resolution imagery for environmental, agricultural, and urban management. Remote sensing enables the monitoring of plant species, crop types, water resources, land cover changes, and urban infrastructure such as roads and buildings, particularly aiding developing countries. Applications range from crop management and plant phenotyping to traffic management and tracking environmental factors and changes. Publicly accessible satellite and aerial datasets are becoming vital tools for tackling global challenges in resource management, environmental protection, and urban planning. <br> <br> Top authors: <ol> <li>Sara Beery (5 papers)</li> <li>David Lobell (4 papers)</li> <li>Edward J. Delp (4 papers)</li> </ol> Example papers: <ul> <li><a href="https://openaccess.thecvf.com/content_cvpr_2018/html/Mahjourian_Unsupervised_Learning_of_CVPR_2018_paper.html" rel="external nofollow noopener" target="_blank">Unsupervised Learning of Depth and Ego-Motion From Monocular Video Using 3D Geometric Constraints (2018)</a></li> <li><a href="https://openaccess.thecvf.com/content_cvpr_2018_workshops/w4/html/Hamaguchi_Building_Detection_From_CVPR_2018_paper.html" rel="external nofollow noopener" target="_blank">Building Detection From Satellite Imagery Using Ensemble of Size-Specific Detectors (2018)</a></li> <li><a href="https://openaccess.thecvf.com/content/CVPR2021W/AI4Space/html/Oestreich_On-Orbit_Inspection_of_an_Unknown_Tumbling_Target_Using_NASAs_Astrobee_CVPRW_2021_paper.html" rel="external nofollow noopener" target="_blank">On-Orbit Inspection of an Unknown, Tumbling Target Using NASA's Astrobee Robotic Free-Flyers (2021)</a></li> </ul> <hr> </details> <details> <summary><b>Topic 6 - Competitions and Challenges in Computer Vision: The Role of NTIRE and Beyond (90 documents)</b></summary> <figure> <picture> <img src="/assets/img/blog/2025-03-26-cvpr-history-analysis/topic_66.png" class="img-fluid rounded z-depth-1 mx-auto d-block" width="100%" height="auto" alt="Wordcloud for topic 6" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> Large-scale competitions like the NTIRE Challenge, MegaFace Challenge, and ABAW Competition have become central to advancing computer vision research. Hosted at major conferences like CVPR, these challenges attract hundreds of registered participants and teams, competing across various tracks such as perceptual quality, AI-generated content, and traffic analysis. Through rigorous submissions and evaluations on standardized test sets, these challenges foster innovation, benchmark progress, and tackle formidable problems in the field. The NTIRE Workshop, in particular, has established itself as a premier platform for recognizing outstanding achievements and setting new frontiers in computer vision competitions. <br> <br> Top authors: <ol> <li>Radu Timofte (44 papers)</li> <li>Marcos V. Conde (8 papers)</li> <li>Radu Timofte (7 papers)</li> </ol> Example papers: <ul> <li><a href="https://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Perez-Pellitero_NTIRE_2022_Challenge_on_High_Dynamic_Range_Imaging_Methods_and_CVPRW_2022_paper.html" rel="external nofollow noopener" target="_blank">NTIRE 2022 Challenge on High Dynamic Range Imaging: Methods and Results (2022)</a></li> <li><a href="https://openaccess.thecvf.com/content_cvpr_2017_workshops/w12/html/Bae_Beyond_Deep_Residual_CVPR_2017_paper.html" rel="external nofollow noopener" target="_blank">Beyond Deep Residual Learning for Image Restoration: Persistent Homology-Guided Manifold Simplification (2017)</a></li> <li><a href="https://openaccess.thecvf.com/content/CVPR2024W/NTIRE/html/Vasluianu_NTIRE_2024_Image_Shadow_Removal_Challenge_Report_CVPRW_2024_paper.html" rel="external nofollow noopener" target="_blank">NTIRE 2024 Image Shadow Removal Challenge Report (2024)</a></li> </ul> <hr> </details> <details> <summary><b>Topic 7 - Enhancing Diffusion Models: Faster Inference and Higher Image Quality (64 documents)</b></summary> <figure> <picture> <img src="/assets/img/blog/2025-03-26-cvpr-history-analysis/topic_103.png" class="img-fluid rounded z-depth-1 mx-auto d-block" width="100%" height="auto" alt="Wordcloud for topic 7" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> Diffusion models have emerged as powerful tools for generating high-quality images, particularly in text-to-image tasks, but they often suffer from slow inference speeds and inherent limitations tied to their timestep-based denoising process. Recent advances focus on accelerating inference and improving FID scores through innovations like post-training quality enhancement, tailored token mixing (e.g., super tokens, OCR tokens), and anisotropic diffusion strategies. These methods can be flexibly applied with negligible computational overhead, substantially improving image quality without retraining. By addressing inherent inefficiencies and showcasing superior performance, these techniques represent a major step forward in diffusion-based image generation. <br> <br> Top authors: <ol> <li>Deli Zhao (3 papers)</li> <li>Yujun Shen (3 papers)</li> <li>Chengyue Gong (2 papers)</li> </ol> Example papers: <ul> <li><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Liu_FlowGrad_Controlling_the_Output_of_Generative_ODEs_With_Gradients_CVPR_2023_paper.html" rel="external nofollow noopener" target="_blank">FlowGrad: Controlling the Output of Generative ODEs With Gradients (2023)</a></li> <li><a href="https://openaccess.thecvf.com/content_CVPRW_2020/html/w1/B.S._Plug-and-Pipeline_Efficient_Regularization_for_Single-Step_Adversarial_Training_CVPRW_2020_paper.html" rel="external nofollow noopener" target="_blank">Plug-and-Pipeline: Efficient Regularization for Single-Step Adversarial Training (2020)</a></li> <li><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Karpikova_FIANCEE_Faster_Inference_of_Adversarial_Networks_via_Conditional_Early_Exits_CVPR_2023_paper.html" rel="external nofollow noopener" target="_blank">FIANCEE: Faster Inference of Adversarial Networks via Conditional Early Exits (2023)</a></li> </ul> <hr> </details> <details> <summary><b>Topic 8 - Intelligent Traffic Monitoring and Driver Behavior Analysis for Road Safety (58 documents)</b></summary> <figure> <picture> <img src="/assets/img/blog/2025-03-26-cvpr-history-analysis/topic_107.png" class="img-fluid rounded z-depth-1 mx-auto d-block" width="100%" height="auto" alt="Wordcloud for topic 8" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> Advances in intelligent transportation systems are increasingly focused on improving road safety through the analysis of driver behavior, traffic scenarios, and vehicle-pedestrian interactions. By leveraging traffic monitoring, naturalistic driving datasets, and leaderboard-driven challenges, researchers aim to develop better driver assistance systems and accident prevention technologies. Areas like distracted driver detection, traffic surveillance, and automated driving benefit from smart monitoring systems that enhance safe driving practices and reduce traffic accidents. As public leaderboards rank the best published methods, innovations in vehicle tracking, intelligent traffic analysis, and safe transportation continue to accelerate progress toward safer roads. <br> <br> Top authors: <ol> <li>Armstrong Aboah (3 papers)</li> <li>Fei Su (3 papers)</li> <li>Zhe Cui (3 papers)</li> </ol> Example papers: <ul> <li><a href="https://openaccess.thecvf.com/content/CVPR2024W/AICity/html/Chen_An_Effective_Method_for_Detecting_Violation_of_Helmet_Rule_for_CVPRW_2024_paper.html" rel="external nofollow noopener" target="_blank">An Effective Method for Detecting Violation of Helmet Rule for Motorcyclists (2024)</a></li> <li><a href="https://openaccess.thecvf.com/content/CVPR2022W/AICity/html/Liang_Stargazer_A_Transformer-Based_Driver_Action_Detection_System_for_Intelligent_Transportation_CVPRW_2022_paper.html" rel="external nofollow noopener" target="_blank">Stargazer: A Transformer-Based Driver Action Detection System for Intelligent Transportation (2022)</a></li> <li><a href="https://openaccess.thecvf.com/content_cvpr_2017_workshops/w4/html/Reddy_Real-Time_Driver_Drowsiness_CVPR_2017_paper.html" rel="external nofollow noopener" target="_blank">Real-Time Driver Drowsiness Detection for Embedded System Using Model Compression of Deep Neural Networks (2017)</a></li> </ul> <hr> </details> <details> <summary><b>Topic 9 - Soccer and Sports Video Analytics: Player Tracking and Game Understanding (103 documents)</b></summary> <figure> <picture> <img src="/assets/img/blog/2025-03-26-cvpr-history-analysis/topic_54.png" class="img-fluid rounded z-depth-1 mx-auto d-block" width="100%" height="auto" alt="Wordcloud for topic 9" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> Advances in sports video analytics, particularly for soccer, focus on tracking players, analyzing game states, and generating highlights from broadcast footage and sport-specific datasets. Systems capable of detecting player positions, ball movements, and team dynamics have become fundamental tools for both game analysis and automated content production. Publicly released datasets like UCF and HMDB, along with open-source code, drive innovation in this field, enabling teams around the world to develop systems capable of capturing, processing, and understanding complex sport scenarios. These developments are reshaping sports analytics, enhancing performance evaluation, and enriching fan experiences. <br> <br> Top authors: <ol> <li>Anthony Cioppa (11 papers)</li> <li>Marc Van Droogenbroeck (10 papers)</li> <li>Bernard Ghanem (8 papers)</li> </ol> Example papers: <ul> <li><a href="https://openaccess.thecvf.com/content_CVPRW_2020/html/w45/Ramaswamy_Spatio-Temporal_Action_Detection_and_Localization_Using_a_Hierarchical_LSTM_CVPRW_2020_paper.html" rel="external nofollow noopener" target="_blank">Spatio-Temporal Action Detection and Localization Using a Hierarchical LSTM (2020)</a></li> <li><a href="https://openaccess.thecvf.com/content_cvpr_2018_workshops/w34/html/Giancola_SoccerNet_A_Scalable_CVPR_2018_paper.html" rel="external nofollow noopener" target="_blank">SoccerNet: A Scalable Dataset for Action Spotting in Soccer Videos (2018)</a></li> <li><a href="https://openaccess.thecvf.com/content_cvpr_2017/html/Hu_Deep_360_Pilot_CVPR_2017_paper.html" rel="external nofollow noopener" target="_blank">Deep 360 Pilot: Learning a Deep Agent for Piloting Through 360deg Sports Videos (2017)</a></li> </ul> <hr> </details> <details> <summary><b>Topic 10 - Event-Based Vision: High-Speed, Low-Latency Sensing with Neuromorphic Cameras (129 documents)</b></summary> <figure> <picture> <img src="/assets/img/blog/2025-03-26-cvpr-history-analysis/topic_34.png" class="img-fluid rounded z-depth-1 mx-auto d-block" width="100%" height="auto" alt="Wordcloud for topic 10" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> Event-based vision, powered by bio-inspired neuromorphic cameras, represents a major shift from traditional frame-based imaging. Unlike conventional sensors, event cameras capture asynchronous changes in brightness with low latency, low power consumption, and exceptional dynamic range, making them ideal for high-speed motion scenarios and environments prone to motion blur. This technology, including event-based vision for video frame interpolation (VFI) and eye tracking, has progressed rapidly, offering advantages in bandwidth efficiency and noise reduction. Applications span robotics, autonomous driving, and high-speed tracking, where conventional frame-based approaches often fall short. <br> <br> Top authors: <ol> <li>Davide Scaramuzza (11 papers)</li> <li>Mathias Gehrig (6 papers)</li> <li>Boxin Shi (5 papers)</li> </ol> Example papers: <ul> <li><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Deng_A_Voxel_Graph_CNN_for_Object_Classification_With_Event_Cameras_CVPR_2022_paper.html" rel="external nofollow noopener" target="_blank">A Voxel Graph CNN for Object Classification With Event Cameras (2022)</a></li> <li><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Sundar_Generalized_Event_Cameras_CVPR_2024_paper.html" rel="external nofollow noopener" target="_blank">Generalized Event Cameras (2024)</a></li> <li><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Yu_EventPS_Real-Time_Photometric_Stereo_Using_an_Event_Camera_CVPR_2024_paper.html" rel="external nofollow noopener" target="_blank">EventPS: Real-Time Photometric Stereo Using an Event Camera (2024)</a></li> </ul> <hr> </details> <h3 id="-10-topics-1">🧊 10 topics</h3> <pre><code class="language-plotly">{"data": [{"customdata": {"dtype": "i1", "bdata": "AQEBAQEBAQE=", "shape": "8, 1"}, "hovertemplate": "topic=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "1", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "1", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "9oDZA2YPuD8TYk4TYk7DPy5/O8fHSss/aRG5pbuR1j/N5tSIhffrP1keEZpqv+o/jwTxnqx//D84iB7fhYPgPw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": {"dtype": "i1", "bdata": "AgICAgICAgI=", "shape": "8, 1"}, "hovertemplate": "topic=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "2", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "2", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "sVTEUhFLAUDyexnyexkCQC5/O8fHSvs/T9krrAn19D+1v65mtH76P8HIDoWtGQdAviIgBBciEUBVzK3OSMUIQA=="}, "yaxis": "y", "type": "scatter"}, {"customdata": {"dtype": "i1", "bdata": "AwMDAwMDAwM=", "shape": "8, 1"}, "hovertemplate": "topic=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "3", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "3", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "9oDZA2YPuD+Y+iGY+iHYPy5/O8fHSts/0PHti4ME7T+cmIhE4wX5P76sEqjoLf0/IAQXItnI+T9VzK3OSMXoPw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": {"dtype": "i1", "bdata": "BAQEBAQEBAQ=", "shape": "8, 1"}, "hovertemplate": "topic=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "4", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "4", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "9oDZA2YP6D8+eSo+eSr+P23VFXiUMANAAjGEv/MeAEBiIhKNF2QJQCM7FLZmnP8/44SUPMuI/j89vgsH0ePxPw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": {"dtype": "i1", "bdata": "BQUFBQUFBQU=", "shape": "8, 1"}, "hovertemplate": "topic=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "5", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "5", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "9oDZA2YPyD+Y+iGY+iHYP2OfbNUVeOQ/AjGEv/Me8D8LrqN3/DDwPyhljgEOfvY/JoMsSX2tA0C1nyyBTfv7Pw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": {"dtype": "i1", "bdata": "BgYGBgYGBgY=", "shape": "8, 1"}, "hovertemplate": "topic=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "6", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "6", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "77S60+pOB0DhCH/hCH8BQELrjQzFu/g/HGkRuaW78T8j1cmZzanxP/SPD4zsUPg/BITgQiQb+T+61RmpmFvtPw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": {"dtype": "i1", "bdata": "BwcHBwcHBwc=", "shape": "8, 1"}, "hovertemplate": "topic=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "7", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "7", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "qYilIpaK8D8TYk4TYk7zP2OfbNUVeOQ/0PHti4ME7T/N5tSIhffrP1w6DXcvq+Q/PIRNAY52+j/hIHp8Fw7wPw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": {"dtype": "i1", "bdata": "CAgICAgICAg=", "shape": "8, 1"}, "hovertemplate": "topic=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "8", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "8", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "uSDjgowL4j/RleTQleTgP3Dn+Fhpw+I/aRG5pbuR5j+1v65mtH7qP/eySqCitwBAmwIc7fRIAED5LhxEj+/2Pw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": {"dtype": "i1", "bdata": "CQkJCQkJCQk=", "shape": "8, 1"}, "hovertemplate": "topic=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "9", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "9", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "9oDZA2YPyD8TYk4TYk6zPy5/O8fHSrs/NqGesldY0z+1v65mtH7aP76sEqjoLd0/mwIc7fRI8D9eOIge34XbPw=="}, "yaxis": "y", "type": "scatter"}, {"customdata": {"dtype": "i1", "bdata": "CgoKCgoKCgo=", "shape": "8, 1"}, "hovertemplate": "topic=%{customdata[0]}&lt;br&gt;year=%{x}&lt;br&gt;occurrences (%)=%{y:.3f}&lt;extra&gt;&lt;/extra&gt;", "legendgroup": "10", "line": {"dash": "solid"}, "marker": {"symbol": "circle"}, "mode": "lines+markers", "name": "10", "orientation": "v", "showlegend": true, "x": {"dtype": "i2", "bdata": "4QfiB+MH5AflB+YH5wfoBw=="}, "xaxis": "x", "y": {"dtype": "f8", "bdata": "2FBeQ3kNFUAdk/Uck/UMQGnDMpe/nQNAdq1/ohRgB0CKSDRekKX/P7+6kBbLI/o/lYMGxlBk9j+waT9ZApvqPw=="}, "yaxis": "y", "type": "scatter"}], "layout": {"xaxis": {"anchor": "y", "domain": [0.0, 1.0], "title": {"text": "year"}}, "yaxis": {"anchor": "x", "domain": [0.0, 1.0], "title": {"text": "occurrences (%)"}}, "legend": {"title": {"text": "topic"}, "tracegroupgap": 0}}}
</code></pre> <p>The topics below are listed in the order in which they had the largest decrease in papers over the last year.</p> <details> <summary><b>Topic 1 - Self-Supervised Pretraining: Masked Models and Their Impact on Downstream Vision Tasks (115 documents)</b></summary> <figure> <picture> <img src="/assets/img/blog/2025-03-26-cvpr-history-analysis/topic_44.png" class="img-fluid rounded z-depth-1 mx-auto d-block" width="100%" height="auto" alt="Wordcloud for topic 1" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> Self-supervised pretraining has revolutionized machine learning by enabling models to learn from unlabeled data, significantly improving performance on a wide range of downstream vision tasks. Techniques like masked autoencoding and contrastive pretraining have become central to this approach, where a model is trained to predict missing parts of the data, learning rich representations without the need for labeled examples. These methods, including masked token strategies and large-scale pretraining on unlabeled video or image datasets, have shown to outperform traditional supervised pretraining, achieving great success across various applications. The benefits of self-supervised learning, especially in terms of scalability and performance, are now being extensively explored in vision-language pretraining (VLP) and other vision tasks, often surpassing existing supervised methods. <br> <br> Top authors: <ol> <li>Yu Qiao (9 papers)</li> <li>Ishan Misra (4 papers)</li> <li>Ross Girshick (4 papers)</li> </ol> Example papers: <ul> <li><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Liu_Solving_Masked_Jigsaw_Puzzles_with_Diffusion_Vision_Transformers_CVPR_2024_paper.html" rel="external nofollow noopener" target="_blank">Solving Masked Jigsaw Puzzles with Diffusion Vision Transformers (2024)</a></li> <li><a href="https://openaccess.thecvf.com/content_cvpr_2017/html/Pathak_Learning_Features_by_CVPR_2017_paper.html" rel="external nofollow noopener" target="_blank">Learning Features by Watching Objects Move (2017)</a></li> <li><a href="https://openaccess.thecvf.com/content/CVPR2024W/CV4MS/html/Kazimi_Self-Supervised_Learning_with_Generative_Adversarial_Networks_for_Electron_Microscopy_CVPRW_2024_paper.html" rel="external nofollow noopener" target="_blank">Self-Supervised Learning with Generative Adversarial Networks for Electron Microscopy (2024)</a></li> </ul> <hr> </details> <details> <summary><b>Topic 2 - Vision-Language Models: Aligning Text and Image for Cross-Modal Understanding (432 documents)</b></summary> <figure> <picture> <img src="/assets/img/blog/2025-03-26-cvpr-history-analysis/topic_2.png" class="img-fluid rounded z-depth-1 mx-auto d-block" width="100%" height="auto" alt="Wordcloud for topic 2" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> Vision-language models, such as CLIP, leverage large datasets of paired image-text data to learn the alignment between visual content and language. These models enable tasks like image captioning, where a description is generated for an image, and text-based image retrieval, where a sentence or phrase is used to find relevant visual content. Through pretraining on vast collections of images and their corresponding captions, these models achieve zero-shot capabilities, meaning they can generalize to tasks they were not explicitly trained on. Grounding text in visual concepts, such as matching sentences to images or videos, has become a central challenge in creating more sophisticated systems for understanding and generating visual and textual information across diverse domains, including untrimmed videos and spoken language. <br> <br> Top authors: <ol> <li>Lijuan Wang (9 papers)</li> <li>Mike Zheng Shou (7 papers)</li> <li>Ying Shan (7 papers)</li> </ol> Example papers: <ul> <li><a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Chen_Image_Search_With_Text_Feedback_by_Visiolinguistic_Attention_Learning_CVPR_2020_paper.html" rel="external nofollow noopener" target="_blank">Image Search With Text Feedback by Visiolinguistic Attention Learning (2020)</a></li> <li><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Gao_ULIP_Learning_a_Unified_Representation_of_Language_Images_and_Point_CVPR_2023_paper.html" rel="external nofollow noopener" target="_blank">ULIP: Learning a Unified Representation of Language, Images, and Point Clouds for 3D Understanding (2023)</a></li> <li><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Vasu_MobileCLIP_Fast_Image-Text_Models_through_Multi-Modal_Reinforced_Training_CVPR_2024_paper.html" rel="external nofollow noopener" target="_blank">MobileCLIP: Fast Image-Text Models through Multi-Modal Reinforced Training (2024)</a></li> </ul> <hr> </details> <details> <summary><b>Topic 3 - Semi-Supervised Learning: Leveraging Unlabeled Data for Improved Model Performance (179 documents)</b></summary> <figure> <picture> <img src="/assets/img/blog/2025-03-26-cvpr-history-analysis/topic_19.png" class="img-fluid rounded z-depth-1 mx-auto d-block" width="100%" height="auto" alt="Wordcloud for topic 3" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> Semi-supervised learning (SSL) is a powerful technique that utilizes both labeled and unlabeled data to train models, especially when labeled data is scarce. In SSL, pseudo-labeling is commonly employed, where unlabeled examples are assigned pseudo-labels based on model predictions, and these pseudo-labeled data are incorporated into the training process. This approach allows models to learn from a large amount of unlabeled data, improving generalization without requiring extensive labeled datasets. Methods like pseudo label refinement and self-training help ensure the quality and reliability of the pseudo-labels, making SSL effective for tasks like medical image analysis, where labeled data is limited. By combining labeled data with confident pseudo-labels from unlabeled examples, semi-supervised learning can outperform traditional fully supervised methods, particularly in challenging settings with partially labeled data. <br> <br> Top authors: <ol> <li>Jingdong Wang (4 papers)</li> <li>Lei Qi (4 papers)</li> <li>Yinghuan Shi (4 papers)</li> </ol> Example papers: <ul> <li><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Zhang_Decoupled_Pseudo-labeling_for_Semi-Supervised_Monocular_3D_Object_Detection_CVPR_2024_paper.html" rel="external nofollow noopener" target="_blank">Decoupled Pseudo-labeling for Semi-Supervised Monocular 3D Object Detection (2024)</a></li> <li><a href="https://openaccess.thecvf.com/content_CVPRW_2020/html/w45/Rebuffi_Semi-Supervised_Learning_With_Scarce_Annotations_CVPRW_2020_paper.html" rel="external nofollow noopener" target="_blank">Semi-Supervised Learning With Scarce Annotations (2020)</a></li> <li><a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Yang_Phase_Consistent_Ecological_Domain_Adaptation_CVPR_2020_paper.html" rel="external nofollow noopener" target="_blank">Phase Consistent Ecological Domain Adaptation (2020)</a></li> </ul> <hr> </details> <details> <summary><b>Topic 4 - Domain Adaptation: Bridging the Gap Between Source and Target Domains (323 documents)</b></summary> <figure> <picture> <img src="/assets/img/blog/2025-03-26-cvpr-history-analysis/topic_6.png" class="img-fluid rounded z-depth-1 mx-auto d-block" width="100%" height="auto" alt="Wordcloud for topic 4" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> Domain adaptation (DA) focuses on adapting models trained on a labeled source domain to perform well on an unseen target domain, addressing the challenges posed by domain shift or domain gap. This process is crucial when the source and target domains differ significantly, such as in cross-domain generalization tasks. Techniques like pseudo-labeling, self-training, and few-shot learning are employed to improve performance on target data, even when labeled data from the target domain is limited or unavailable. Domain adaptation methods aim to reduce the discrepancy between the source and target domains by minimizing the impact of domain shift and leveraging unlabeled target samples. These methods are vital for applications like visual domain adaptation, where new target domains with varying conditions or classes are frequently encountered. <br> <br> Top authors: <ol> <li>Luc Van Gool (8 papers)</li> <li>Dengxin Dai (7 papers)</li> <li>Wen Li (7 papers)</li> </ol> Example papers: <ul> <li><a href="https://openaccess.thecvf.com/content/CVPR2021W/WiCV/html/Thota_Contrastive_Domain_Adaptation_CVPRW_2021_paper.html" rel="external nofollow noopener" target="_blank">Contrastive Domain Adaptation (2021)</a></li> <li><a href="https://openaccess.thecvf.com/content/CVPR2021W/LLID/html/Cai_DAMSL_Domain_Agnostic_Meta_Score-Based_Learning_CVPRW_2021_paper.html" rel="external nofollow noopener" target="_blank">DAMSL: Domain Agnostic Meta Score-Based Learning (2021)</a></li> <li><a href="https://openaccess.thecvf.com/content/CVPR2021/html/Song_Spatio-temporal_Contrastive_Domain_Adaptation_for_Action_Recognition_CVPR_2021_paper.html" rel="external nofollow noopener" target="_blank">Spatio-temporal Contrastive Domain Adaptation for Action Recognition (2021)</a></li> </ul> <hr> </details> <details> <summary><b>Topic 5 - 3D Object Detection: Advancements in Lidar and Monocular Approaches for Autonomous Vehicles (217 documents)</b></summary> <figure> <picture> <img src="/assets/img/blog/2025-03-26-cvpr-history-analysis/topic_15.png" class="img-fluid rounded z-depth-1 mx-auto d-block" width="100%" height="auto" alt="Wordcloud for topic 5" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> 3D object detection is a critical component of autonomous driving, enabling vehicles to perceive and understand their environment in three dimensions. Using technologies like lidar, monocular cameras, and radar, 3D detection systems create detailed representations of the surroundings, often represented in formats such as birds-eye view (BEV) or voxel grids. Datasets like KITTI, NuScenes, and Waymo provide benchmarks for evaluating 3D detection models, with lidar-based point clouds playing a central role in high-accuracy detection of objects, such as pedestrians, vehicles, and obstacles. These systems face challenges such as slow inference speeds and the complexity of predicting occupancy grids, but advancements in lidar sensors, occupancy prediction, and BEV detectors are helping improve autonomous vehicle perception and safety. As autonomous driving systems evolve, 3D detection continues to be crucial for precise navigation and decision-making. <br> <br> Top authors: <ol> <li>Jie Zhou (7 papers)</li> <li>Jiwen Lu (6 papers)</li> <li>Yuexin Ma (6 papers)</li> </ol> Example papers: <ul> <li><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Li_GAFusion_Adaptive_Fusing_LiDAR_and_Camera_with_Multiple_Guidance_for_CVPR_2024_paper.html" rel="external nofollow noopener" target="_blank">GAFusion: Adaptive Fusing LiDAR and Camera with Multiple Guidance for 3D Object Detection (2024)</a></li> <li><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Tang_SparseOcc_Rethinking_Sparse_Latent_Representation_for_Vision-Based_Semantic_Occupancy_Prediction_CVPR_2024_paper.html" rel="external nofollow noopener" target="_blank">SparseOcc: Rethinking Sparse Latent Representation for Vision-Based Semantic Occupancy Prediction (2024)</a></li> <li><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Yin_IS-Fusion_Instance-Scene_Collaborative_Fusion_for_Multimodal_3D_Object_Detection_CVPR_2024_paper.html" rel="external nofollow noopener" target="_blank">IS-Fusion: Instance-Scene Collaborative Fusion for Multimodal 3D Object Detection (2024)</a></li> </ul> <hr> </details> <details> <summary><b>Topic 6 - Weakly Supervised Object Segmentation: Balancing Annotations and Performance (244 documents)</b></summary> <figure> <picture> <img src="/assets/img/blog/2025-03-26-cvpr-history-analysis/topic_13.png" class="img-fluid rounded z-depth-1 mx-auto d-block" width="100%" height="auto" alt="Wordcloud for topic 6" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> Weakly supervised object segmentation focuses on leveraging less detailed annotations, such as image-level labels or object proposals, to train segmentation models. Unlike fully supervised methods that require pixel-level annotations, weak supervision relies on class-level or bounding box labels to guide the segmentation process. Datasets like Pascal VOC and MS COCO provide benchmarks for evaluating segmentation models, with metrics such as mean Intersection over Union (mIoU) used to assess performance. Techniques like class-agnostic object masks, discriminative region mapping (CAM), and pseudo-masks are employed to generate pixel-level segmentations from weak annotations. This approach aims to reduce the cost and effort associated with obtaining high-quality, pixel-wise annotations, while still achieving competitive segmentation results, especially for complex tasks like instance-level segmentation and object part identification. <br> <br> Top authors: <ol> <li>Junwei Han (5 papers)</li> <li>Yunchao Wei (5 papers)</li> <li>Bingfeng Zhang (4 papers)</li> </ol> Example papers: <ul> <li><a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Cheng_CascadePSP_Toward_Class-Agnostic_and_Very_High-Resolution_Segmentation_via_Global_and_CVPR_2020_paper.html" rel="external nofollow noopener" target="_blank">CascadePSP: Toward Class-Agnostic and Very High-Resolution Segmentation via Global and Local Refinement (2020)</a></li> <li><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Huynh_SimpSON_Simplifying_Photo_Cleanup_With_Single-Click_Distracting_Object_Segmentation_Network_CVPR_2023_paper.html" rel="external nofollow noopener" target="_blank">SimpSON: Simplifying Photo Cleanup With Single-Click Distracting Object Segmentation Network (2023)</a></li> <li><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Kang_Distilling_Self-Supervised_Vision_Transformers_for_Weakly-Supervised_Few-Shot_Classification__Segmentation_CVPR_2023_paper.html" rel="external nofollow noopener" target="_blank">Distilling Self-Supervised Vision Transformers for Weakly-Supervised Few-Shot Classification &amp; Segmentation (2023)</a></li> </ul> <hr> </details> <details> <summary><b>Topic 7 - Image Relighting: Enhancing Lighting and Material Effects in Digital Rendering (167 documents)</b></summary> <figure> <picture> <img src="/assets/img/blog/2025-03-26-cvpr-history-analysis/topic_21.png" class="img-fluid rounded z-depth-1 mx-auto d-block" width="100%" height="auto" alt="Wordcloud for topic 7" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> Image relighting is a technique used in computer graphics and computational photography to manipulate or simulate changes in lighting conditions on a given scene or object. This process involves adjusting various aspects of lighting, such as specular and diffuse reflections, albedo, and shadow effects, to create realistic or desired lighting outcomes. It takes into account material properties like reflectance, illumination, and surface normals, which determine how light interacts with the scene. Relighting is commonly applied in tasks such as portrait or face relighting, where the goal is to alter lighting without changing the geometry of the scene. By estimating and adjusting lighting effects like specular highlights, shadows, and ambient light, image relighting allows for enhanced visual realism and flexibility in various applications, from film production to virtual environments and interactive systems. <br> <br> Top authors: <ol> <li>Boxin Shi (11 papers)</li> <li>Kalyan Sunkavalli (7 papers)</li> <li>Noah Snavely (6 papers)</li> </ol> Example papers: <ul> <li><a href="https://openaccess.thecvf.com/content_cvpr_2018/html/Mo_Uncalibrated_Photometric_Stereo_CVPR_2018_paper.html" rel="external nofollow noopener" target="_blank">Uncalibrated Photometric Stereo Under Natural Illumination (2018)</a></li> <li><a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Thapa_Dynamic_Fluid_Surface_Reconstruction_Using_Deep_Neural_Network_CVPR_2020_paper.html" rel="external nofollow noopener" target="_blank">Dynamic Fluid Surface Reconstruction Using Deep Neural Network (2020)</a></li> <li><a href="https://openaccess.thecvf.com/content/CVPR2021/html/R_Monocular_Reconstruction_of_Neural_Face_Reflectance_Fields_CVPR_2021_paper.html" rel="external nofollow noopener" target="_blank">Monocular Reconstruction of Neural Face Reflectance Fields (2021)</a></li> </ul> <hr> </details> <details> <summary><b>Topic 8 - Large Kernel Convolutions and Self-Attention Mechanisms in Vision Transformers (209 documents)</b></summary> <figure> <picture> <img src="/assets/img/blog/2025-03-26-cvpr-history-analysis/topic_16.png" class="img-fluid rounded z-depth-1 mx-auto d-block" width="100%" height="auto" alt="Wordcloud for topic 8" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> The integration of large kernel convolutions and self-attention mechanisms has become a powerful approach in modern computer vision tasks. Large kernel convolutions, such as atrous or depthwise convolutions, allow for an increased receptive field, enabling the model to capture long-range dependencies in an image without significantly increasing computational cost. This is essential for vision tasks like object detection and segmentation, where understanding the global context is crucial. On the other hand, self-attention mechanisms, particularly in Vision Transformers (ViTs), facilitate capturing relationships between distant image patches, enhancing the model's ability to focus on relevant parts of an image. By combining large kernel convolutions with self-attention layers, models like the Vision Transformer (ViT) can effectively balance local feature extraction and global context understanding, leading to improved performance on benchmarks like ADE, Cityscapes, and COCO, especially in tasks like segmentation and scene understanding. This combination provides an efficient and scalable solution for handling complex vision tasks while maintaining competitive performance. <br> <br> Top authors: <ol> <li>Xiangyu Zhang (6 papers)</li> <li>Chang Xu (5 papers)</li> <li>Yu Qiao (5 papers)</li> </ol> Example papers: <ul> <li><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Zhang_PatchFormer_An_Efficient_Point_Transformer_With_Patch_Attention_CVPR_2022_paper.html" rel="external nofollow noopener" target="_blank">PatchFormer: An Efficient Point Transformer With Patch Attention (2022)</a></li> <li><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Ding_UniRepLKNet_A_Universal_Perception_Large-Kernel_ConvNet_for_Audio_Video_Point_CVPR_2024_paper.html" rel="external nofollow noopener" target="_blank">UniRepLKNet: A Universal Perception Large-Kernel ConvNet for Audio Video Point Cloud Time-Series and Image Recognition (2024)</a></li> <li><a href="https://openaccess.thecvf.com/content/CVPR2021/html/Ruan_Gaussian_Context_Transformer_CVPR_2021_paper.html" rel="external nofollow noopener" target="_blank">Gaussian Context Transformer (2021)</a></li> </ul> <hr> </details> <details> <summary><b>Topic 9 - 3D-Aware Image Synthesis with GANs for High-Fidelity and Controllable Rendering (71 documents)</b></summary> <figure> <picture> <img src="/assets/img/blog/2025-03-26-cvpr-history-analysis/topic_91.png" class="img-fluid rounded z-depth-1 mx-auto d-block" width="100%" height="auto" alt="Wordcloud for topic 9" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> 3D-aware image synthesis is an advanced technique that combines the power of Generative Adversarial Networks (GANs) with 3D geometry to create highly realistic and controllable images. By incorporating 3D-aware models like Neural Radiance Fields (NeRF) and leveraging latent spaces in GAN architectures such as StyleGAN, this method enables the generation of high-fidelity images from novel views or multi-view perspectives. This approach allows for fine-grained control over attributes like lighting, angles, and details, making it particularly useful for photorealistic rendering and editing. The synthesis process ensures that the generated images maintain consistency across different views and provide high-quality visual outputs, which can be applied in areas such as virtual reality, digital content creation, and computer graphics. With advancements in 3D-aware GANs, it is now possible to synthesize photo-realistic images with impressive fidelity, enabling novel applications in creative industries. <br> <br> Top authors: <ol> <li>Gordon Wetzstein (4 papers)</li> <li>Jiajun Wu (4 papers)</li> <li>Sida Peng (4 papers)</li> </ol> Example papers: <ul> <li><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Xu_DisCoScene_Spatially_Disentangled_Generative_Radiance_Fields_for_Controllable_3D-Aware_Scene_CVPR_2023_paper.html" rel="external nofollow noopener" target="_blank">DisCoScene: Spatially Disentangled Generative Radiance Fields for Controllable 3D-Aware Scene Synthesis (2023)</a></li> <li><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Kim_NeuralField-LDM_Scene_Generation_With_Hierarchical_Latent_Diffusion_Models_CVPR_2023_paper.html" rel="external nofollow noopener" target="_blank">NeuralField-LDM: Scene Generation With Hierarchical Latent Diffusion Models (2023)</a></li> <li><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Li_Lift3D_Synthesize_3D_Training_Data_by_Lifting_2D_GAN_to_CVPR_2023_paper.html" rel="external nofollow noopener" target="_blank">Lift3D: Synthesize 3D Training Data by Lifting 2D GAN to 3D Generative Radiance Field (2023)</a></li> </ul> <hr> </details> <details> <summary><b>Topic 10 - Efficient Solving of Non-Convex Problems with Outlier Rejection and Relaxation Techniques (356 documents)</b></summary> <figure> <picture> <img src="/assets/img/blog/2025-03-26-cvpr-history-analysis/topic_4.png" class="img-fluid rounded z-depth-1 mx-auto d-block" width="100%" height="auto" alt="Wordcloud for topic 10" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> Solving non-convex problems, especially those involving outlier detection, correspondence, and registration, is a complex challenge in fields like computer vision and robotics. Methods like RANSAC (Random Sample Consensus) and polynomial solvers are commonly used to handle these issues, where outliers — incorrect data points — are filtered out to improve the accuracy of the solution. Convex relaxation techniques and non-convex optimization solvers are applied to iteratively refine the solution toward a globally optimal result. For example, pose estimation problems, such as relative rotation or translation, are solved efficiently by leveraging convex optimization and minimal solvers, ensuring that even with noisy or incomplete data, the solution converges to the correct answer. These techniques, including the use of least squares and graph matching, play a key role in ensuring robust performance in real-world applications, where noise and outliers are unavoidable. <br> <br> Top authors: <ol> <li>Daniel Barath (13 papers)</li> <li>Daniel Cremers (13 papers)</li> <li>Viktor Larsson (10 papers)</li> </ol> Example papers: <ul> <li><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Bai_Sliced_Optimal_Partial_Transport_CVPR_2023_paper.html" rel="external nofollow noopener" target="_blank">Sliced Optimal Partial Transport (2023)</a></li> <li><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zhang_3D_Registration_With_Maximal_Cliques_CVPR_2023_paper.html" rel="external nofollow noopener" target="_blank">3D Registration With Maximal Cliques (2023)</a></li> <li><a href="https://openaccess.thecvf.com/content/CVPR2023/html/He_A_Rotation-Translation-Decoupled_Solution_for_Robust_and_Efficient_Visual-Inertial_Initialization_CVPR_2023_paper.html" rel="external nofollow noopener" target="_blank">A Rotation-Translation-Decoupled Solution for Robust and Efficient Visual-Inertial Initialization (2023)</a></li> </ul> <hr> </details> <h2 id="conclusion">Conclusion</h2> <p>In this analysis, we have explored the trends and shifts in research topics within the CVPR community over the past years. The data reveals a dynamic landscape, with certain areas experiencing significant growth while others have seen a decline in interest. This reflects the evolving nature of artificial intelligence research and the continuous pursuit of innovation and improvement in various domains.</p> <p>The hottest topics — ranging from instruction-tuned multimodal LLMs for vision-language understanding to the rapid advancements in text-guided editing, 3D-aware synthesis, and event-based vision — highlight a strong drive toward bridging modalities, creating more controllable generative models, and addressing the growing needs of real-world applications. Researchers are increasingly focusing on improving the integration of language and vision to enable more effective reasoning, better handling of ambiguities (such as hallucinations), and enhanced performance in both creative and safety-critical environments.</p> <p>In contrast, the coldest topics — such as self-supervised pretraining, traditional vision-language alignment, semi-supervised learning, domain adaptation, and even classical 3D object detection — indicate areas where mature techniques have plateaued. While these methods laid the foundations for current advances, their rate of improvement appears to have slowed in favor of newer approaches. Techniques once at the cutting edge are now being revisited with an eye toward integrating them into more comprehensive systems, but their standalone appeal is declining as the community shifts toward end-to-end, multimodal, and task-specific solutions.</p> <p>Taken together, the trends suggest that the field is steering toward more holistic, integrated models that not only push the boundaries of what automated systems can generate or analyze but also provide greater reliability and control in real-world applications. As the industry continues to explore the fusion of text, image, and even sensor data, the next wave of innovation will likely be driven by systems that learn from multiple modalities concurrently — while leveraging long-standing, robust principles as a stepping stone.</p> <p>This evolution underscores the vibrant nature of artificial intelligence research, where established methods provide a stable base while emerging techniques hold the promise of reshaping the future of artificial intelligence.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12"> Enjoy Reading This Article? </h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/slidev_for_non_web_devs/">sli.dev for non-web developers</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/simple-improvements-python-code/">Improving your python code with simple tricks</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2022/research-code-reproducibility/">The problem of research code reproducibility</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2022/localized-blog/">Creating localized blog posts</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2022/localized-projects/">Creating localized Projects pages</a> </li> <div id="giscus_thread" style="max-width: 1000px; margin: 0 auto;"> <script>
      let giscusTheme = determineComputedTheme();
      let giscusAttributes = {
        src: 'https://giscus.app/client.js',
        'data-repo': 'george-gca/george-gca.github.io',
        'data-repo-id': 'R_kgDOI66tbw',
        'data-category': 'Announcements',
        'data-category-id': 'DIC_kwDOI66tb84CUDq6',
        'data-mapping': 'title',
        'data-strict': '1',
        'data-reactions-enabled': '1',
        'data-emit-metadata': '0',
        'data-input-position': 'bottom',
        'data-theme': giscusTheme,
        'data-lang': 'en',
        crossorigin: 'anonymous',
        async: '',
      };

      let giscusScript = document.createElement('script');
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById('giscus_thread').appendChild(giscusScript);
    </script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> ©Copyright 2025 George C. de Araújo. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/george-gca/multi-language-al-folio" rel="external nofollow noopener" target="_blank">multi-language-al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/plotly.js@3.0.1/dist/plotly.min.js" integrity="sha256-oy6Be7Eh6eiQFs5M7oXuPxxm9qbJXEtTpfSI93dW16Q=" crossorigin="anonymous"></script> <script defer src="/assets/js/plotly-setup.js?5e81fc889064852664784cb29c0d6970" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?ec82345cb41d6fd19bf9dfe5403e38e4"></script> <script defer src="/assets/js/common.js?dc2245338e496943bdc50ee60dcbf8f5"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-J7KHS011G7"></script> <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() {
      dataLayer.push(arguments);
    }
    gtag('js', new Date());

    gtag('config', 'G-J7KHS011G7');
  </script> <script defer src="/assets/js/google-analytics-setup.js?12374742c4b1801ba82226e617af7e2d"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"> <div class="modal-footer" slot="footer"> <span class="help"> <svg version="1.0" class="ninja-examplekey" xmlns="http://www.w3.org/2000/svg" viewbox="0 0 1280 1280"> <path d="M1013 376c0 73.4-.4 113.3-1.1 120.2a159.9 159.9 0 0 1-90.2 127.3c-20 9.6-36.7 14-59.2 15.5-7.1.5-121.9.9-255 1h-242l95.5-95.5 95.5-95.5-38.3-38.2-38.2-38.3-160 160c-88 88-160 160.4-160 161 0 .6 72 73 160 161l160 160 38.2-38.3 38.3-38.2-95.5-95.5-95.5-95.5h251.1c252.9 0 259.8-.1 281.4-3.6 72.1-11.8 136.9-54.1 178.5-116.4 8.6-12.9 22.6-40.5 28-55.4 4.4-12 10.7-36.1 13.1-50.6 1.6-9.6 1.8-21 2.1-132.8l.4-122.2H1013v110z"></path> </svg> to select </span> <span class="help"> <svg xmlns="http://www.w3.org/2000/svg" class="ninja-examplekey" viewbox="0 0 24 24"> <path d="M0 0h24v24H0V0z" fill="none"></path> <path d="M20 12l-1.41-1.41L13 16.17V4h-2v12.17l-5.58-5.59L4 12l8 8 8-8z"></path> </svg> <svg xmlns="http://www.w3.org/2000/svg" class="ninja-examplekey" viewbox="0 0 24 24"> <path d="M0 0h24v24H0V0z" fill="none"></path> <path d="M4 12l1.41 1.41L11 7.83V20h2V7.83l5.58 5.59L20 12l-8-8-8 8z"></path> </svg> to navigate </span> <span class="help"> <span class="ninja-examplekey esc">esc</span> to close </span> <span class="help"> <svg xmlns="http://www.w3.org/2000/svg" class="ninja-examplekey backspace" viewbox="0 0 20 20" fill="currentColor"> <path fill-rule="evenodd" d="M6.707 4.879A3 3 0 018.828 4H15a3 3 0 013 3v6a3 3 0 01-3 3H8.828a3 3 0 01-2.12-.879l-4.415-4.414a1 1 0 010-1.414l4.414-4.414zm4 2.414a1 1 0 00-1.414 1.414L10.586 10l-1.293 1.293a1 1 0 101.414 1.414L12 11.414l1.293 1.293a1 1 0 001.414-1.414L13.414 10l1.293-1.293a1 1 0 00-1.414-1.414L12 8.586l-1.293-1.293z" clip-rule="evenodd"></path> </svg> move to parent </span> </div> </ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>